diff --git a/models/carl_encoder.ipynb b/models/carl_encoder.ipynb
index bad0ed71..2edd5a79 100644
--- a/models/carl_encoder.ipynb
+++ b/models/carl_encoder.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -34,27 +34,21 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 19,
    "metadata": {},
    "outputs": [],
    "source": [
-    "file_path = '../data/name_smiles_embedding_file.csv'\n",
-    "name_smiles_embedding_df = pd.read_csv(file_path)\n",
-    "\n",
-    "file_path = '/mnt/usb/cmdunham/MoNA_embeddings_big_df.csv'\n",
-    "mass_spec_embeddings = pd.read_csv(file_path)\n",
-    "mass_spec_embeddings = mass_spec_embeddings.rename(columns={\n",
-    "    'METHYL PROPIONATE': 'Methyl Propionate', 'DIETHYL MALEATE':'Diethyl Maleate'\n",
-    "    })\n",
-    "\n",
-    "file_path = '../data/mass_spec_encoder_generated_embeddings.csv'\n",
-    "mass_spec_encoder_generated_embeddings = pd.read_csv(file_path)\n",
-    "mass_spec_encoder_generated_embeddings = mass_spec_encoder_generated_embeddings.drop('Unnamed: 0', axis=1)"
+    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_carls.csv'\n",
+    "train_carls = pd.read_csv(file_path)\n",
+    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls.csv'\n",
+    "val_carls = pd.read_csv(file_path)\n",
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls.csv'\n",
+    "test_carls = pd.read_csv(file_path)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -78,75 +72,205 @@
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
-       "      <th>Name</th>\n",
-       "      <th>SMILES</th>\n",
-       "      <th>embedding</th>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>Unnamed: 0</th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
-       "      <th></th>\n",
+       "      <th>index</th>\n",
+       "      <th>p_184</th>\n",
+       "      <th>p_185</th>\n",
+       "      <th>p_186</th>\n",
+       "      <th>p_187</th>\n",
+       "      <th>p_188</th>\n",
+       "      <th>p_189</th>\n",
+       "      <th>p_190</th>\n",
+       "      <th>p_191</th>\n",
+       "      <th>p_192</th>\n",
+       "      <th>...</th>\n",
+       "      <th>n_1021</th>\n",
+       "      <th>Label</th>\n",
+       "      <th>DEB</th>\n",
+       "      <th>DEM</th>\n",
+       "      <th>DMMP</th>\n",
+       "      <th>DPM</th>\n",
+       "      <th>DtBP</th>\n",
+       "      <th>JP8</th>\n",
+       "      <th>MES</th>\n",
+       "      <th>TEPO</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
-       "      <th>BKG</th>\n",
-       "      <td>background</td>\n",
-       "      <td>NaN</td>\n",
-       "      <td>NaN</td>\n",
+       "      <th>0</th>\n",
+       "      <td>1297539</td>\n",
+       "      <td>-12.0</td>\n",
+       "      <td>-14.0</td>\n",
+       "      <td>-16.0</td>\n",
+       "      <td>-20.0</td>\n",
+       "      <td>-21.0</td>\n",
+       "      <td>-22.0</td>\n",
+       "      <td>-22.0</td>\n",
+       "      <td>-24.0</td>\n",
+       "      <td>-25.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-8.0</td>\n",
+       "      <td>DtBP</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>DEM</th>\n",
-       "      <td>Diethyl Malonate</td>\n",
-       "      <td>CCOC(=O)CC(=O)OCC</td>\n",
-       "      <td>[0.3809721, 0.0005454041, 0.25539744, -0.24272...</td>\n",
+       "      <th>1</th>\n",
+       "      <td>1297539</td>\n",
+       "      <td>-12.0</td>\n",
+       "      <td>-10.0</td>\n",
+       "      <td>-7.0</td>\n",
+       "      <td>-4.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>7.0</td>\n",
+       "      <td>6.0</td>\n",
+       "      <td>8.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>15.0</td>\n",
+       "      <td>DtBP</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>DEB</th>\n",
-       "      <td>1,2,3,4-Diepoxybutane</td>\n",
-       "      <td>C1C(O1)C2CO2</td>\n",
-       "      <td>[0.06318794, 0.009022224, 0.42160064, 0.195722...</td>\n",
+       "      <th>2</th>\n",
+       "      <td>1344903</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>-2.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-7.0</td>\n",
+       "      <td>DMMP</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>MES</th>\n",
-       "      <td>2-(N-morpholino)ethanesulfonic acid</td>\n",
-       "      <td>C1COCCN1CCS(=O)(=O)O</td>\n",
-       "      <td>[-0.32520828, 0.009838344, -0.15108332, 0.2845...</td>\n",
+       "      <th>3</th>\n",
+       "      <td>1344903</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>6.0</td>\n",
+       "      <td>11.0</td>\n",
+       "      <td>17.0</td>\n",
+       "      <td>24.0</td>\n",
+       "      <td>27.0</td>\n",
+       "      <td>30.0</td>\n",
+       "      <td>31.0</td>\n",
+       "      <td>31.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>16.0</td>\n",
+       "      <td>DMMP</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>DMMP</th>\n",
-       "      <td>Dimethyl methylphosphonate</td>\n",
-       "      <td>COP(=O)(C)OC</td>\n",
-       "      <td>[0.12106811, 0.00294244, -0.14450458, 0.072665...</td>\n",
+       "      <th>4</th>\n",
+       "      <td>1221071</td>\n",
+       "      <td>10.0</td>\n",
+       "      <td>12.0</td>\n",
+       "      <td>15.0</td>\n",
+       "      <td>14.0</td>\n",
+       "      <td>17.0</td>\n",
+       "      <td>19.0</td>\n",
+       "      <td>22.0</td>\n",
+       "      <td>23.0</td>\n",
+       "      <td>26.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>TEPO</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
+       "<p>5 rows × 1686 columns</p>\n",
        "</div>"
       ],
       "text/plain": [
-       "                                           Name                SMILES  \\\n",
-       "Unnamed: 0                                                              \n",
-       "BKG                                  background                   NaN   \n",
-       "DEM                            Diethyl Malonate     CCOC(=O)CC(=O)OCC   \n",
-       "DEB                       1,2,3,4-Diepoxybutane          C1C(O1)C2CO2   \n",
-       "MES         2-(N-morpholino)ethanesulfonic acid  C1COCCN1CCS(=O)(=O)O   \n",
-       "DMMP                 Dimethyl methylphosphonate          COP(=O)(C)OC   \n",
+       "     index  p_184  p_185  p_186  p_187  p_188  p_189  p_190  p_191  p_192  \\\n",
+       "0  1297539  -12.0  -14.0  -16.0  -20.0  -21.0  -22.0  -22.0  -24.0  -25.0   \n",
+       "1  1297539  -12.0  -10.0   -7.0   -4.0    1.0    3.0    7.0    6.0    8.0   \n",
+       "2  1344903    2.0    2.0    2.0    1.0    2.0    2.0    1.0    1.0   -2.0   \n",
+       "3  1344903    2.0    6.0   11.0   17.0   24.0   27.0   30.0   31.0   31.0   \n",
+       "4  1221071   10.0   12.0   15.0   14.0   17.0   19.0   22.0   23.0   26.0   \n",
+       "\n",
+       "   ...  n_1021  Label  DEB  DEM  DMMP  DPM  DtBP  JP8  MES  TEPO  \n",
+       "0  ...    -8.0   DtBP  0.0  0.0   0.0  0.0   1.0  0.0  0.0   0.0  \n",
+       "1  ...    15.0   DtBP  0.0  0.0   0.0  0.0   1.0  0.0  0.0   0.0  \n",
+       "2  ...    -7.0   DMMP  0.0  0.0   1.0  0.0   0.0  0.0  0.0   0.0  \n",
+       "3  ...    16.0   DMMP  0.0  0.0   1.0  0.0   0.0  0.0  0.0   0.0  \n",
+       "4  ...     0.0   TEPO  0.0  0.0   0.0  0.0   0.0  0.0  0.0   1.0  \n",
        "\n",
-       "                                                    embedding  \n",
-       "Unnamed: 0                                                     \n",
-       "BKG                                                       NaN  \n",
-       "DEM         [0.3809721, 0.0005454041, 0.25539744, -0.24272...  \n",
-       "DEB         [0.06318794, 0.009022224, 0.42160064, 0.195722...  \n",
-       "MES         [-0.32520828, 0.009838344, -0.15108332, 0.2845...  \n",
-       "DMMP        [0.12106811, 0.00294244, -0.14450458, 0.072665...  "
+       "[5 rows x 1686 columns]"
       ]
      },
-     "execution_count": 3,
      "metadata": {},
-     "output_type": "execute_result"
+     "output_type": "display_data"
     }
    ],
+   "source": [
+    "train_carls.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path = '../data/name_smiles_embedding_file.csv'\n",
+    "name_smiles_embedding_df = pd.read_csv(file_path)\n",
+    "\n",
+    "file_path = '../data/MoNA_embeddings_multiple_instrument_types.csv'\n",
+    "mass_spec_embeddings = pd.read_csv(file_path)\n",
+    "# mass_spec_embeddings = mass_spec_embeddings.rename(columns={\n",
+    "#     'METHYL PROPIONATE': 'Methyl Propionate', 'DIETHYL MALEATE':'Diethyl Maleate'\n",
+    "#     })\n",
+    "\n",
+    "file_path = '../data/mass_spec_encoder_output.csv'\n",
+    "mass_spec_encoder_generated_embeddings = pd.read_csv(file_path)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
    "source": [
     "# set the df index to be the chemical abbreviations in col 'Unnamed: 0'\n",
     "name_smiles_embedding_df.set_index('Unnamed: 0', inplace=True)\n",
@@ -155,7 +279,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 36,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -172,81 +296,308 @@
     "name_smiles_embedding_df['Embedding Floats'] = embedding_floats"
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Setting up GPU:\n",
-    "---"
-   ]
-  },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 55,
    "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Selected GPU ID: 0\n",
-      "  Name: NVIDIA GeForce RTX 4090\n",
-      "  Memory Free: 21682.0 MB\n",
-      "  Memory Used: 2534.0 MB\n",
-      "  GPU Load: 98.00%\n",
-      "Current device ID:  cuda:0\n",
-      "PyTorch current device ID: 0\n",
-      "PyTorch current device name: NVIDIA GeForce RTX 4090\n"
-     ]
-    }
-   ],
-   "source": [
-    "if torch.cuda.is_available():\n",
-    "    # Get the list of GPUs\n",
-    "    gpus = GPUtil.getGPUs()\n",
-    "\n",
-    "    # Find the GPU with the most free memory\n",
-    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
-    "\n",
-    "    # Print details about the selected GPU\n",
-    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
-    "    print(f\"  Name: {best_gpu.name}\")\n",
-    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
-    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
-    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
-    "\n",
-    "    # Set the device for later use\n",
-    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
-    "    print('Current device ID: ', device)\n",
-    "\n",
-    "    # Set the current device in PyTorch\n",
-    "    torch.cuda.set_device(best_gpu.id)\n",
-    "else:\n",
-    "    device = torch.device('cpu')\n",
-    "    print('Using CPU')\n",
-    "\n",
-    "# Confirm the currently selected device in PyTorch\n",
-    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
-    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Definitions:\n",
-    "---"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def flatten_and_bin(predicted_embeddings_batches):\n",
-    "    # Function to flatten the list of prediction batches and make each prediction binary\n",
-    "    binary_preds_list = []\n",
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>DEM</th>\n",
+       "      <th>DEB</th>\n",
+       "      <th>MES</th>\n",
+       "      <th>DMMP</th>\n",
+       "      <th>DPM</th>\n",
+       "      <th>JP8</th>\n",
+       "      <th>TEPO</th>\n",
+       "      <th>DtBP</th>\n",
+       "      <th>Glycine</th>\n",
+       "      <th>Tryptophan</th>\n",
+       "      <th>...</th>\n",
+       "      <th>Succinic Acid</th>\n",
+       "      <th>Malonic Acid</th>\n",
+       "      <th>L-Lysine</th>\n",
+       "      <th>Undecane</th>\n",
+       "      <th>L-Asparagine</th>\n",
+       "      <th>Testosterone</th>\n",
+       "      <th>L-Alanine</th>\n",
+       "      <th>Fluorene</th>\n",
+       "      <th>2-Octanol</th>\n",
+       "      <th>Gamma-Amino-N-Butyric Acid</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>0</th>\n",
+       "      <td>0.380972</td>\n",
+       "      <td>0.063188</td>\n",
+       "      <td>-0.325208</td>\n",
+       "      <td>0.121068</td>\n",
+       "      <td>-0.023968</td>\n",
+       "      <td>0.025142</td>\n",
+       "      <td>0.193039</td>\n",
+       "      <td>0.226090</td>\n",
+       "      <td>0.012962</td>\n",
+       "      <td>0.094619</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.064590</td>\n",
+       "      <td>0.104017</td>\n",
+       "      <td>-0.008052</td>\n",
+       "      <td>0.110134</td>\n",
+       "      <td>0.191296</td>\n",
+       "      <td>-0.028562</td>\n",
+       "      <td>0.121412</td>\n",
+       "      <td>0.499569</td>\n",
+       "      <td>-0.243301</td>\n",
+       "      <td>0.012944</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>1</th>\n",
+       "      <td>0.000545</td>\n",
+       "      <td>0.009022</td>\n",
+       "      <td>0.009838</td>\n",
+       "      <td>0.002942</td>\n",
+       "      <td>0.002720</td>\n",
+       "      <td>0.011977</td>\n",
+       "      <td>0.000974</td>\n",
+       "      <td>0.000808</td>\n",
+       "      <td>0.003898</td>\n",
+       "      <td>0.001532</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.035480</td>\n",
+       "      <td>0.060536</td>\n",
+       "      <td>0.012390</td>\n",
+       "      <td>0.000952</td>\n",
+       "      <td>0.005387</td>\n",
+       "      <td>0.063791</td>\n",
+       "      <td>0.005075</td>\n",
+       "      <td>-0.003935</td>\n",
+       "      <td>0.004697</td>\n",
+       "      <td>0.008486</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>0.255397</td>\n",
+       "      <td>0.421601</td>\n",
+       "      <td>-0.151083</td>\n",
+       "      <td>-0.144505</td>\n",
+       "      <td>0.158301</td>\n",
+       "      <td>0.542705</td>\n",
+       "      <td>0.066288</td>\n",
+       "      <td>-0.062160</td>\n",
+       "      <td>-0.179823</td>\n",
+       "      <td>0.048508</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.251732</td>\n",
+       "      <td>-0.260941</td>\n",
+       "      <td>0.077747</td>\n",
+       "      <td>-0.037645</td>\n",
+       "      <td>-0.178066</td>\n",
+       "      <td>-0.061395</td>\n",
+       "      <td>-0.331054</td>\n",
+       "      <td>0.206176</td>\n",
+       "      <td>-0.108382</td>\n",
+       "      <td>-0.003481</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>-0.242728</td>\n",
+       "      <td>0.195723</td>\n",
+       "      <td>0.284503</td>\n",
+       "      <td>0.072665</td>\n",
+       "      <td>-0.010084</td>\n",
+       "      <td>0.364973</td>\n",
+       "      <td>-0.195223</td>\n",
+       "      <td>-0.045343</td>\n",
+       "      <td>0.618586</td>\n",
+       "      <td>-0.228242</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.044983</td>\n",
+       "      <td>0.027383</td>\n",
+       "      <td>0.442515</td>\n",
+       "      <td>-0.432724</td>\n",
+       "      <td>0.447277</td>\n",
+       "      <td>-0.363461</td>\n",
+       "      <td>0.078999</td>\n",
+       "      <td>0.408699</td>\n",
+       "      <td>-0.401711</td>\n",
+       "      <td>0.426480</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>-0.305106</td>\n",
+       "      <td>-0.167326</td>\n",
+       "      <td>-0.099838</td>\n",
+       "      <td>-0.107286</td>\n",
+       "      <td>-0.067723</td>\n",
+       "      <td>-0.236272</td>\n",
+       "      <td>-0.272051</td>\n",
+       "      <td>-0.075880</td>\n",
+       "      <td>-0.028876</td>\n",
+       "      <td>-0.069579</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.010591</td>\n",
+       "      <td>-0.017281</td>\n",
+       "      <td>-0.037480</td>\n",
+       "      <td>-0.025410</td>\n",
+       "      <td>-0.039142</td>\n",
+       "      <td>-0.457587</td>\n",
+       "      <td>-0.010997</td>\n",
+       "      <td>-0.009848</td>\n",
+       "      <td>-0.045440</td>\n",
+       "      <td>-0.037688</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>5 rows × 36 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "        DEM       DEB       MES      DMMP       DPM       JP8      TEPO  \\\n",
+       "0  0.380972  0.063188 -0.325208  0.121068 -0.023968  0.025142  0.193039   \n",
+       "1  0.000545  0.009022  0.009838  0.002942  0.002720  0.011977  0.000974   \n",
+       "2  0.255397  0.421601 -0.151083 -0.144505  0.158301  0.542705  0.066288   \n",
+       "3 -0.242728  0.195723  0.284503  0.072665 -0.010084  0.364973 -0.195223   \n",
+       "4 -0.305106 -0.167326 -0.099838 -0.107286 -0.067723 -0.236272 -0.272051   \n",
+       "\n",
+       "       DtBP   Glycine  Tryptophan  ...  Succinic Acid  Malonic Acid  L-Lysine  \\\n",
+       "0  0.226090  0.012962    0.094619  ...       0.064590      0.104017 -0.008052   \n",
+       "1  0.000808  0.003898    0.001532  ...       0.035480      0.060536  0.012390   \n",
+       "2 -0.062160 -0.179823    0.048508  ...      -0.251732     -0.260941  0.077747   \n",
+       "3 -0.045343  0.618586   -0.228242  ...       0.044983      0.027383  0.442515   \n",
+       "4 -0.075880 -0.028876   -0.069579  ...      -0.010591     -0.017281 -0.037480   \n",
+       "\n",
+       "   Undecane  L-Asparagine  Testosterone  L-Alanine  Fluorene  2-Octanol  \\\n",
+       "0  0.110134      0.191296     -0.028562   0.121412  0.499569  -0.243301   \n",
+       "1  0.000952      0.005387      0.063791   0.005075 -0.003935   0.004697   \n",
+       "2 -0.037645     -0.178066     -0.061395  -0.331054  0.206176  -0.108382   \n",
+       "3 -0.432724      0.447277     -0.363461   0.078999  0.408699  -0.401711   \n",
+       "4 -0.025410     -0.039142     -0.457587  -0.010997 -0.009848  -0.045440   \n",
+       "\n",
+       "   Gamma-Amino-N-Butyric Acid  \n",
+       "0                    0.012944  \n",
+       "1                    0.008486  \n",
+       "2                   -0.003481  \n",
+       "3                    0.426480  \n",
+       "4                   -0.037688  \n",
+       "\n",
+       "[5 rows x 36 columns]"
+      ]
+     },
+     "execution_count": 55,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# filtering out chems with < 5 embeddings\n",
+    "mass_spec_chem_counts = Counter(mass_spec_encoder_generated_embeddings['Label'])\n",
+    "chems_above_5 = [key for key, count in mass_spec_chem_counts.items() if count >= 5]\n",
+    "filtered_mass_spec_embeddings = mass_spec_embeddings[chems_above_5]\n",
+    "filtered_mass_spec_encoder_generated_embeddings = mass_spec_encoder_generated_embeddings[mass_spec_encoder_generated_embeddings['Label'].isin(chems_above_5)]\n",
+    "\n",
+    "# Combine embeddings for IMS simulants and mass spec chems to use for plotting pca\n",
+    "ims_embeddings = pd.DataFrame([emb for emb in name_smiles_embedding_df['Embedding Floats']][1:]).T\n",
+    "cols = name_smiles_embedding_df.index[1:]\n",
+    "ims_embeddings.columns = cols\n",
+    "all_true_embeddings = pd.concat([ims_embeddings, filtered_mass_spec_embeddings], axis=1)\n",
+    "all_true_embeddings.head()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Setting up GPU:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 38,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Selected GPU ID: 0\n",
+      "  Name: NVIDIA GeForce RTX 4090\n",
+      "  Memory Free: 22148.0 MB\n",
+      "  Memory Used: 2069.0 MB\n",
+      "  GPU Load: 90.00%\n",
+      "Current device ID:  cuda:0\n",
+      "PyTorch current device ID: 0\n",
+      "PyTorch current device name: NVIDIA GeForce RTX 4090\n"
+     ]
+    }
+   ],
+   "source": [
+    "if torch.cuda.is_available():\n",
+    "    # Get the list of GPUs\n",
+    "    gpus = GPUtil.getGPUs()\n",
+    "\n",
+    "    # Find the GPU with the most free memory\n",
+    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
+    "\n",
+    "    # Print details about the selected GPU\n",
+    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
+    "    print(f\"  Name: {best_gpu.name}\")\n",
+    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
+    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
+    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
+    "\n",
+    "    # Set the device for later use\n",
+    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
+    "    print('Current device ID: ', device)\n",
+    "\n",
+    "    # Set the current device in PyTorch\n",
+    "    torch.cuda.set_device(best_gpu.id)\n",
+    "else:\n",
+    "    device = torch.device('cpu')\n",
+    "    print('Using CPU')\n",
+    "\n",
+    "# Confirm the currently selected device in PyTorch\n",
+    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
+    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Definitions:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def flatten_and_bin(predicted_embeddings_batches):\n",
+    "    # Function to flatten the list of prediction batches and make each prediction binary\n",
+    "    binary_preds_list = []\n",
     "    \n",
     "    for batch in predicted_embeddings_batches:\n",
     "        for encoding in batch:\n",
@@ -262,7 +613,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -285,7 +636,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 22,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -297,7 +648,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 23,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -341,7 +692,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 24,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -406,7 +757,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 25,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -441,7 +792,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 26,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -578,7 +929,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 27,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -611,7 +962,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 28,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -645,7 +996,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -755,6 +1106,7 @@
     "                    print('-------------------------------------------')\n",
     "            else:\n",
     "                print(f'Validation loss has not improved in {epochs_without_validation_improvement} epochs. Stopping training at epoch {epoch}.')\n",
+    "                wandb.log({'Early Stopping Ecoch':epoch})\n",
     "                plot_pca(\n",
     "                    train_data, combo['batch_size'], encoder, device, \n",
     "                    encoder_criterion, sorted_chem_names, all_embeddings_df, \n",
@@ -792,40 +1144,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# Encoder for truncated dataset\n",
-    "class TruncatedEncoder(nn.Module):\n",
-    "  def __init__(self):\n",
-    "    super().__init__()\n",
-    "    self.encoder = nn.Sequential(\n",
-    "      nn.Linear(1032,968),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(968,904),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(904, 840),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(840, 776),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(776, 712),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(712, 648),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(648, 584),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(584, 512),\n",
-    "    )\n",
-    "\n",
-    "  def forward(self, x):\n",
-    "    x = self.encoder(x)\n",
-    "    return x"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 32,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -847,402 +1166,302 @@
     "    return embeddings_tensor, spectra_tensor, chem_encodings_tensor, spectra_indices_tensor"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Training Encoder on Carls:\n",
+    "---"
+   ]
+  },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/tmp/ipykernel_224248/2621477981.py:5: DtypeWarning: Columns (1677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
-      "  val_carls = pd.read_csv(file_path)\n",
-      "/tmp/ipykernel_224248/2621477981.py:7: DtypeWarning: Columns (1677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
-      "  test_carls = pd.read_csv(file_path)\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
-    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_carls.csv'\n",
-    "train_carls = pd.read_csv(file_path)\n",
-    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls.csv'\n",
-    "val_carls = pd.read_csv(file_path)\n",
-    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls.csv'\n",
-    "test_carls = pd.read_csv(file_path)"
+    "train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = create_dataset_tensors(train_carls, name_smiles_embedding_df, device, carl=True)\n",
+    "val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor = create_dataset_tensors(val_carls, name_smiles_embedding_df, device, carl=True)\n",
+    "test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor = create_dataset_tensors(test_carls, name_smiles_embedding_df, device, carl=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Things that need to be changed for each encoder/dataset/target embedding\n",
+    "notebook_name = '/home/cmdunham/ChemicalDataGeneration/models/carl_encoder.ipynb'\n",
+    "architecture = 'carl_encoder'\n",
+    "dataset_type = 'carls'\n",
+    "target_embedding = 'ChemNet'\n",
+    "encoder_path = '../models/carl_to_chemnet_encoder..pth'"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 58,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "config = {\n",
+    "    'wandb_entity': 'catemerfeld',\n",
+    "    'wandb_project': 'ims_encoder_decoder',\n",
+    "    'gpu':True,\n",
+    "    'threads':1,\n",
+    "}\n",
+    "\n",
+    "os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/html": [
-       "<div>\n",
-       "<style scoped>\n",
-       "    .dataframe tbody tr th:only-of-type {\n",
-       "        vertical-align: middle;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe tbody tr th {\n",
-       "        vertical-align: top;\n",
-       "    }\n",
-       "\n",
-       "    .dataframe thead th {\n",
-       "        text-align: right;\n",
-       "    }\n",
-       "</style>\n",
-       "<table border=\"1\" class=\"dataframe\">\n",
-       "  <thead>\n",
-       "    <tr style=\"text-align: right;\">\n",
-       "      <th></th>\n",
-       "      <th>index</th>\n",
-       "      <th>p_184</th>\n",
-       "      <th>p_185</th>\n",
-       "      <th>p_186</th>\n",
-       "      <th>p_187</th>\n",
-       "      <th>p_188</th>\n",
-       "      <th>p_189</th>\n",
-       "      <th>p_190</th>\n",
-       "      <th>p_191</th>\n",
-       "      <th>p_192</th>\n",
-       "      <th>...</th>\n",
-       "      <th>n_1021</th>\n",
-       "      <th>Label</th>\n",
-       "      <th>DEB</th>\n",
-       "      <th>DEM</th>\n",
-       "      <th>DMMP</th>\n",
-       "      <th>DPM</th>\n",
-       "      <th>DtBP</th>\n",
-       "      <th>JP8</th>\n",
-       "      <th>MES</th>\n",
-       "      <th>TEPO</th>\n",
-       "    </tr>\n",
-       "  </thead>\n",
-       "  <tbody>\n",
-       "    <tr>\n",
-       "      <th>0</th>\n",
-       "      <td>1297539</td>\n",
-       "      <td>-12.0</td>\n",
-       "      <td>-14.0</td>\n",
-       "      <td>-16.0</td>\n",
-       "      <td>-20.0</td>\n",
-       "      <td>-21.0</td>\n",
-       "      <td>-22.0</td>\n",
-       "      <td>-22.0</td>\n",
-       "      <td>-24.0</td>\n",
-       "      <td>-25.0</td>\n",
-       "      <td>...</td>\n",
-       "      <td>-8.0</td>\n",
-       "      <td>DtBP</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>1</th>\n",
-       "      <td>1297539</td>\n",
-       "      <td>-12.0</td>\n",
-       "      <td>-10.0</td>\n",
-       "      <td>-7.0</td>\n",
-       "      <td>-4.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>3.0</td>\n",
-       "      <td>7.0</td>\n",
-       "      <td>6.0</td>\n",
-       "      <td>8.0</td>\n",
-       "      <td>...</td>\n",
-       "      <td>15.0</td>\n",
-       "      <td>DMMP</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>2</th>\n",
-       "      <td>1344903</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>-2.0</td>\n",
-       "      <td>...</td>\n",
-       "      <td>-7.0</td>\n",
-       "      <td>TEPO</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>1.0</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>3</th>\n",
-       "      <td>1344903</td>\n",
-       "      <td>2.0</td>\n",
-       "      <td>6.0</td>\n",
-       "      <td>11.0</td>\n",
-       "      <td>17.0</td>\n",
-       "      <td>24.0</td>\n",
-       "      <td>27.0</td>\n",
-       "      <td>30.0</td>\n",
-       "      <td>31.0</td>\n",
-       "      <td>31.0</td>\n",
-       "      <td>...</td>\n",
-       "      <td>16.0</td>\n",
-       "      <td>DtBP</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>1.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "    </tr>\n",
-       "    <tr>\n",
-       "      <th>4</th>\n",
-       "      <td>1221071</td>\n",
-       "      <td>10.0</td>\n",
-       "      <td>12.0</td>\n",
-       "      <td>15.0</td>\n",
-       "      <td>14.0</td>\n",
-       "      <td>17.0</td>\n",
-       "      <td>19.0</td>\n",
-       "      <td>22.0</td>\n",
-       "      <td>23.0</td>\n",
-       "      <td>26.0</td>\n",
-       "      <td>...</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>TEPO</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>0.0</td>\n",
-       "      <td>1.0</td>\n",
-       "    </tr>\n",
-       "  </tbody>\n",
-       "</table>\n",
-       "<p>5 rows × 1686 columns</p>\n",
-       "</div>"
+       "Finishing last run (ID:cub5qnqs) before initializing another..."
       ],
       "text/plain": [
-       "     index  p_184  p_185  p_186  p_187  p_188  p_189  p_190  p_191  p_192  \\\n",
-       "0  1297539  -12.0  -14.0  -16.0  -20.0  -21.0  -22.0  -22.0  -24.0  -25.0   \n",
-       "1  1297539  -12.0  -10.0   -7.0   -4.0    1.0    3.0    7.0    6.0    8.0   \n",
-       "2  1344903    2.0    2.0    2.0    1.0    2.0    2.0    1.0    1.0   -2.0   \n",
-       "3  1344903    2.0    6.0   11.0   17.0   24.0   27.0   30.0   31.0   31.0   \n",
-       "4  1221071   10.0   12.0   15.0   14.0   17.0   19.0   22.0   23.0   26.0   \n",
-       "\n",
-       "   ...  n_1021  Label  DEB  DEM  DMMP  DPM  DtBP  JP8  MES  TEPO  \n",
-       "0  ...    -8.0   DtBP  0.0  0.0   0.0  0.0   1.0  0.0  0.0   0.0  \n",
-       "1  ...    15.0   DMMP  0.0  0.0   1.0  0.0   0.0  0.0  0.0   0.0  \n",
-       "2  ...    -7.0   TEPO  0.0  0.0   0.0  0.0   0.0  0.0  0.0   1.0  \n",
-       "3  ...    16.0   DtBP  0.0  0.0   0.0  0.0   1.0  0.0  0.0   0.0  \n",
-       "4  ...     0.0   TEPO  0.0  0.0   0.0  0.0   0.0  0.0  0.0   1.0  \n",
-       "\n",
-       "[5 rows x 1686 columns]"
+       "<IPython.core.display.HTML object>"
       ]
      },
-     "execution_count": 8,
      "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "train_carls.head()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {},
-   "outputs": [
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "e3e4adeb80534de586305c0a4ab3f743",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "VBox(children=(Label(value='0.071 MB of 0.071 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "W&B sync reduced upload amount by 1.4%             "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<style>\n",
+       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
+       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
+       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
+       "    </style>\n",
+       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Encoder Training Loss</td><td>▁▅▁█▁</td></tr><tr><td>Encoder Validation Loss</td><td>▂█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Encoder Training Loss</td><td>0.00015</td></tr><tr><td>Encoder Validation Loss</td><td>9e-05</td></tr></table><br/></div></div>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run <strong style=\"color:#cdcd00\">bright-moon-463</strong> at: <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/cub5qnqs' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/cub5qnqs</a><br/> View project at: <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Find logs at: <code>./wandb/run-20241212_130155-cub5qnqs/logs</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Successfully finished last run (ID:cub5qnqs). Initializing new run:<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "wandb version 0.19.0 is available!  To upgrade, please run:\n",
+       " $ pip install wandb --upgrade"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Tracking run with wandb version 0.17.0"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Run data is saved locally in <code>/home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241212_130740-z41v3r5v</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Syncing run <strong><a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/z41v3r5v' target=\"_blank\">treasured-monkey-464</a></strong> to <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View project at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/z41v3r5v' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/z41v3r5v</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
     {
-     "ename": "NameError",
-     "evalue": "name 'create_dataset_tensors' is not defined",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = create_dataset_tensors(train_carls, name_smiles_embedding_df, device, carl=True)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset_tensors\u001b[49m(val_carls, name_smiles_embedding_df, device, carl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor \u001b[38;5;241m=\u001b[39m create_dataset_tensors(test_carls, name_smiles_embedding_df, device, carl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
-      "\u001b[0;31mNameError\u001b[0m: name 'create_dataset_tensors' is not defined"
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Using device: cuda\n",
+      "--------------------------\n",
+      "--------------------------\n",
+      "New run with hyperparameters:\n",
+      "batch_size  :  64\n",
+      "epochs  :  500\n",
+      "learning_rate  :  1e-05\n",
+      "Saved best model at epoch 1\n",
+      "Saved best model at epoch 2\n",
+      "Saved best model at epoch 4\n",
+      "Saved best model at epoch 6\n",
+      "Saved best model at epoch 10\n",
+      "Epoch[10/500]:\n",
+      "   Training loss: 3.3713469173801996e-05\n",
+      "   Validation loss: 2.3985228280260907e-05\n",
+      "-------------------------------------------\n",
+      "Saved best model at epoch 13\n",
+      "Saved best model at epoch 14\n",
+      "Saved best model at epoch 16\n",
+      "Epoch[20/500]:\n",
+      "   Training loss: 1.902977782578017e-05\n",
+      "   Validation loss: 3.0288494381086798e-05\n",
+      "-------------------------------------------\n",
+      "Saved best model at epoch 24\n",
+      "Saved best model at epoch 27\n",
+      "Epoch[30/500]:\n",
+      "   Training loss: 1.0561782293238561e-05\n",
+      "   Validation loss: 1.3314776254543305e-05\n",
+      "-------------------------------------------\n",
+      "Saved best model at epoch 34\n",
+      "Saved best model at epoch 40\n",
+      "Epoch[40/500]:\n",
+      "   Training loss: 9.304660878247519e-06\n",
+      "   Validation loss: 6.4031044615233754e-06\n",
+      "-------------------------------------------\n"
      ]
     }
-   ],
-   "source": [
-    "train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = create_dataset_tensors(train_carls, name_smiles_embedding_df, device, carl=True)\n",
-    "val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor = create_dataset_tensors(val_carls, name_smiles_embedding_df, device, carl=True)\n",
-    "test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor = create_dataset_tensors(test_carls, name_smiles_embedding_df, device, carl=True)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Training Encoder on Carls:\n",
-    "---"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "config = {\n",
-    "    'wandb_entity': 'catemerfeld',\n",
-    "    'wandb_project': 'ims_encoder_decoder',\n",
-    "    'gpu':True,\n",
-    "    'threads':1,\n",
+   ],
+   "source": [
+    "wandb_kwargs = {\n",
+    "    'architecture': architecture,\n",
+    "    'optimizer':'AdamW',\n",
+    "    'loss':'MSELoss',\n",
+    "    'dataset': dataset_type,\n",
+    "    'target_embedding': target_embedding\n",
     "}\n",
     "\n",
-    "os.environ['WANDB_NOTEBOOK_NAME'] = '/home/cmdunham/ChemicalDataGeneration/models/ims_encoder.ipynb'"
+    "sorted_chem_names = list(train_carls.columns[-8:])\n",
+    "\n",
+    "model_hyperparams = {\n",
+    "  'batch_size':[64],\n",
+    "  'epochs': [500],\n",
+    "  'learning_rate':[.00001, .000001]\n",
+    "  }\n",
+    "\n",
+    "train_data = TensorDataset(train_carl_tensor, train_chem_encodings_tensor, train_embeddings_tensor, train_carl_indices_tensor)\n",
+    "val_data = TensorDataset(val_carl_tensor, val_chem_encodings_tensor, val_embeddings_tensor, val_carl_indices_tensor)\n",
+    "test_data = TensorDataset(test_carl_tensor, test_chem_encodings_tensor, test_embeddings_tensor, test_carl_indices_tensor)\n",
+    "\n",
+    "best_hyperparams = train_model(\n",
+    "    'Encoder', train_data, val_data, test_data, \n",
+    "    device, config, wandb_kwargs, \n",
+    "    all_true_embeddings, name_smiles_embedding_df, model_hyperparams, \n",
+    "    sorted_chem_names, encoder_path, save_emb_pca_to_wandb=True, \n",
+    "    show_wandb_run_name=True\n",
+    "    )"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "Most successful run so far trained for 10 epochs at .0001 before training at .00001 for 100 epochs. Clusters were very tight, possibly too tight?"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# set var deciding if embedding pca plot for this run is saved to wandb\n",
-    "save_emb_pca_to_wandb = True\n",
-    "\n",
-    "# Last 8 cols of the df are the chem names\n",
-    "sorted_chem_names = list(train_carls.columns[-8:])\n",
-    "\n",
-    "model_config = {\n",
-    "  'batch_size':[128],\n",
-    "  'epochs': [100],\n",
-    "  'learning_rate':[.00001]\n",
-    "  }\n",
-    "\n",
-    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
-    "lowest_loss = 100\n",
-    "\n",
-    "# model_config = {\n",
-    "#     'batch_size': [128],\n",
-    "#     'epochs': [10],\n",
-    "#     'learning_rate': [.00001]\n",
-    "# }\n",
-    "\n",
-    "keys = model_config.keys()\n",
-    "values = model_config.values()\n",
-    "\n",
-    "# Generate all parameter combinations from model_config using itertools.product\n",
-    "combinations = itertools.product(*values)\n",
-    "\n",
-    "# Iterate through each parameter combination and run model \n",
-    "for combo in combinations:\n",
-    "  combo = dict(zip(keys, combo))\n",
-    "\n",
-    "  train_dataset = DataLoader(TensorDataset(train_carl_tensor, train_chem_encodings_tensor, train_embeddings_tensor, train_carl_indices_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
-    "  val_dataset = DataLoader(TensorDataset(val_carl_tensor, val_chem_encodings_tensor, val_embeddings_tensor, val_carl_indices_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
-    "  encoder = Encoder().to(device)\n",
-    "\n",
-    "  encoder_optimizer = torch.optim.AdamW(encoder.parameters(), lr = combo['learning_rate'])\n",
-    "  encoder_criterion = nn.MSELoss()\n",
-    "\n",
-    "  wandb_kwargs = {\n",
-    "    'learning_rate': combo['learning_rate'],\n",
-    "    'epochs': combo['epochs'],\n",
-    "    'batch_size': combo['batch_size'],\n",
-    "    'model_architecture': 'carl_encoder',\n",
-    "    'optimizer':'AdamW',\n",
-    "    'loss': 'MSELoss'\n",
-    "  }\n",
-    "\n",
-    "  run_with_wandb(config, **wandb_kwargs)\n",
-    "\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "  print('New run with hyperparameters:')\n",
-    "  for key in combo:\n",
-    "    print(key, ' : ', combo[key])\n",
-    "\n",
-    "  for epoch in range(combo['epochs']):\n",
-    "    # Set model to training mode\n",
-    "    encoder.train(True)\n",
-    "\n",
-    "    # do a pass over the data\n",
-    "    # at last epoch get predicted embeddings and chem names\n",
-    "    if (epoch + 1) == combo['epochs']:\n",
-    "      average_loss, predicted_embeddings, output_name_encodings = train_one_epoch(\n",
-    "        train_dataset, device, encoder, encoder_criterion, encoder_optimizer, epoch, combo\n",
-    "        )\n",
-    "    else:\n",
-    "      average_loss = train_one_epoch(\n",
-    "        train_dataset, device, encoder, encoder_criterion, encoder_optimizer, epoch, combo\n",
-    "        )\n",
-    "\n",
-    "    epoch_val_loss = 0  \n",
-    "    # evaluate model on validation data\n",
-    "    encoder.eval() # Set model to evaluation mode\n",
-    "    with torch.no_grad():\n",
-    "      for val_batch, val_name_encodings, val_true_embeddings, val_spectra_indices in val_dataset:\n",
-    "        val_batch = val_batch.to(device)\n",
-    "        val_name_encodings = val_name_encodings.to(device)\n",
-    "        val_true_embeddings = val_true_embeddings.to(device)\n",
-    "\n",
-    "        val_batch_predicted_embeddings = encoder(val_batch)\n",
-    "\n",
-    "        val_loss = encoder_criterion(val_batch_predicted_embeddings, val_true_embeddings)\n",
-    "        # accumulate epoch validation loss\n",
-    "        epoch_val_loss += val_loss.item()\n",
-    "\n",
-    "    # divide by number of batches to calculate average loss\n",
-    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
-    "\n",
-    "    # log losses to wandb\n",
-    "    wandb.log({\"Encoder Training Loss\": average_loss, \"Encoder Validation Loss\": val_average_loss})\n",
-    "    # wandb.log({\"Encoder Training Loss\": average_loss})\n",
-    "\n",
-    "    if (epoch + 1) % 10 == 0:\n",
-    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
-    "      print(f'   Training loss: {average_loss}')\n",
-    "      print(f'   Validation loss: {val_average_loss}')\n",
-    "      print('-------------------------------------------')\n",
-    "\n",
-    "  if save_emb_pca_to_wandb:\n",
-    "    true_embeddings, predicted_embeddings_flattened, chem_names = preds_to_emb_pca_plot(predicted_embeddings, output_name_encodings, sorted_chem_names, name_smiles_embedding_df)\n",
-    "\n",
-    "  if average_loss < lowest_loss:\n",
-    "    best_hyperparams = combo\n",
-    "\n",
-    "  wandb.finish()\n",
-    "\n",
-    "print('Hyperparameters for best model: ')\n",
-    "for key in best_hyperparams:\n",
-    "  print('   ', key, ' : ', best_hyperparams[key])"
+    "## Viewing Encoder Results:\n",
+    "---"
    ]
   },
   {
@@ -1382,6 +1601,14 @@
     "# plot_emb_pca(all_true_embeddings, test_preds_df, log_wandb=False, chemnet_embeddings_to_plot=train_true_embeddings)"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Saving Carl Embeddings:\n",
+    "---"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -1405,6 +1632,131 @@
     "# test_preds_df.to_csv(file_path, index=False)"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Unused Code:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# set var deciding if embedding pca plot for this run is saved to wandb\n",
+    "save_emb_pca_to_wandb = True\n",
+    "\n",
+    "# Last 8 cols of the df are the chem names\n",
+    "sorted_chem_names = list(train_carls.columns[-8:])\n",
+    "\n",
+    "model_config = {\n",
+    "  'batch_size':[128],\n",
+    "  'epochs': [100],\n",
+    "  'learning_rate':[.00001]\n",
+    "  }\n",
+    "\n",
+    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
+    "lowest_loss = 100\n",
+    "\n",
+    "# model_config = {\n",
+    "#     'batch_size': [128],\n",
+    "#     'epochs': [10],\n",
+    "#     'learning_rate': [.00001]\n",
+    "# }\n",
+    "\n",
+    "keys = model_config.keys()\n",
+    "values = model_config.values()\n",
+    "\n",
+    "# Generate all parameter combinations from model_config using itertools.product\n",
+    "combinations = itertools.product(*values)\n",
+    "\n",
+    "# Iterate through each parameter combination and run model \n",
+    "for combo in combinations:\n",
+    "  combo = dict(zip(keys, combo))\n",
+    "\n",
+    "  train_dataset = DataLoader(TensorDataset(train_carl_tensor, train_chem_encodings_tensor, train_embeddings_tensor, train_carl_indices_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
+    "  val_dataset = DataLoader(TensorDataset(val_carl_tensor, val_chem_encodings_tensor, val_embeddings_tensor, val_carl_indices_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
+    "  encoder = Encoder().to(device)\n",
+    "\n",
+    "  encoder_optimizer = torch.optim.AdamW(encoder.parameters(), lr = combo['learning_rate'])\n",
+    "  encoder_criterion = nn.MSELoss()\n",
+    "\n",
+    "  wandb_kwargs = {\n",
+    "    'learning_rate': combo['learning_rate'],\n",
+    "    'epochs': combo['epochs'],\n",
+    "    'batch_size': combo['batch_size'],\n",
+    "    'model_architecture': 'carl_encoder',\n",
+    "    'optimizer':'AdamW',\n",
+    "    'loss': 'MSELoss'\n",
+    "  }\n",
+    "\n",
+    "  run_with_wandb(config, **wandb_kwargs)\n",
+    "\n",
+    "  print('--------------------------')\n",
+    "  print('--------------------------')\n",
+    "  print('New run with hyperparameters:')\n",
+    "  for key in combo:\n",
+    "    print(key, ' : ', combo[key])\n",
+    "\n",
+    "  for epoch in range(combo['epochs']):\n",
+    "    # Set model to training mode\n",
+    "    encoder.train(True)\n",
+    "\n",
+    "    # do a pass over the data\n",
+    "    # at last epoch get predicted embeddings and chem names\n",
+    "    if (epoch + 1) == combo['epochs']:\n",
+    "      average_loss, predicted_embeddings, output_name_encodings = train_one_epoch(\n",
+    "        train_dataset, device, encoder, encoder_criterion, encoder_optimizer, epoch, combo\n",
+    "        )\n",
+    "    else:\n",
+    "      average_loss = train_one_epoch(\n",
+    "        train_dataset, device, encoder, encoder_criterion, encoder_optimizer, epoch, combo\n",
+    "        )\n",
+    "\n",
+    "    epoch_val_loss = 0  \n",
+    "    # evaluate model on validation data\n",
+    "    encoder.eval() # Set model to evaluation mode\n",
+    "    with torch.no_grad():\n",
+    "      for val_batch, val_name_encodings, val_true_embeddings, val_spectra_indices in val_dataset:\n",
+    "        val_batch = val_batch.to(device)\n",
+    "        val_name_encodings = val_name_encodings.to(device)\n",
+    "        val_true_embeddings = val_true_embeddings.to(device)\n",
+    "\n",
+    "        val_batch_predicted_embeddings = encoder(val_batch)\n",
+    "\n",
+    "        val_loss = encoder_criterion(val_batch_predicted_embeddings, val_true_embeddings)\n",
+    "        # accumulate epoch validation loss\n",
+    "        epoch_val_loss += val_loss.item()\n",
+    "\n",
+    "    # divide by number of batches to calculate average loss\n",
+    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "\n",
+    "    # log losses to wandb\n",
+    "    wandb.log({\"Encoder Training Loss\": average_loss, \"Encoder Validation Loss\": val_average_loss})\n",
+    "    # wandb.log({\"Encoder Training Loss\": average_loss})\n",
+    "\n",
+    "    if (epoch + 1) % 10 == 0:\n",
+    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
+    "      print(f'   Training loss: {average_loss}')\n",
+    "      print(f'   Validation loss: {val_average_loss}')\n",
+    "      print('-------------------------------------------')\n",
+    "\n",
+    "  if save_emb_pca_to_wandb:\n",
+    "    true_embeddings, predicted_embeddings_flattened, chem_names = preds_to_emb_pca_plot(predicted_embeddings, output_name_encodings, sorted_chem_names, name_smiles_embedding_df)\n",
+    "\n",
+    "  if average_loss < lowest_loss:\n",
+    "    best_hyperparams = combo\n",
+    "\n",
+    "  wandb.finish()\n",
+    "\n",
+    "print('Hyperparameters for best model: ')\n",
+    "for key in best_hyperparams:\n",
+    "  print('   ', key, ' : ', best_hyperparams[key])"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
diff --git a/models/ims_encoder.ipynb b/models/ims_encoder.ipynb
index dd39e6cd..db704f55 100644
--- a/models/ims_encoder.ipynb
+++ b/models/ims_encoder.ipynb
@@ -65,7 +65,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -84,6 +84,215 @@
     "# mass_spec_encoder_generated_embeddings = mass_spec_encoder_generated_embeddings.drop('Unnamed: 0', axis=1)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>...</th>\n",
+       "      <th>503</th>\n",
+       "      <th>504</th>\n",
+       "      <th>505</th>\n",
+       "      <th>506</th>\n",
+       "      <th>507</th>\n",
+       "      <th>508</th>\n",
+       "      <th>509</th>\n",
+       "      <th>510</th>\n",
+       "      <th>511</th>\n",
+       "      <th>Label</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>0</th>\n",
+       "      <td>0.021717</td>\n",
+       "      <td>0.013678</td>\n",
+       "      <td>-0.124958</td>\n",
+       "      <td>0.377754</td>\n",
+       "      <td>-0.046985</td>\n",
+       "      <td>0.244335</td>\n",
+       "      <td>-0.287048</td>\n",
+       "      <td>-0.164717</td>\n",
+       "      <td>-0.064576</td>\n",
+       "      <td>-0.370969</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.135776</td>\n",
+       "      <td>0.423812</td>\n",
+       "      <td>-0.000535</td>\n",
+       "      <td>-0.172896</td>\n",
+       "      <td>-0.050619</td>\n",
+       "      <td>-0.079060</td>\n",
+       "      <td>0.083757</td>\n",
+       "      <td>0.163587</td>\n",
+       "      <td>-0.326790</td>\n",
+       "      <td>Glycine</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>1</th>\n",
+       "      <td>0.092086</td>\n",
+       "      <td>0.005441</td>\n",
+       "      <td>0.052097</td>\n",
+       "      <td>-0.224653</td>\n",
+       "      <td>-0.066780</td>\n",
+       "      <td>0.239429</td>\n",
+       "      <td>-0.175937</td>\n",
+       "      <td>-0.382511</td>\n",
+       "      <td>-0.231119</td>\n",
+       "      <td>-0.178706</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.125035</td>\n",
+       "      <td>0.624961</td>\n",
+       "      <td>-0.001381</td>\n",
+       "      <td>-0.286743</td>\n",
+       "      <td>-0.050950</td>\n",
+       "      <td>-0.184345</td>\n",
+       "      <td>0.168900</td>\n",
+       "      <td>0.288457</td>\n",
+       "      <td>-0.507141</td>\n",
+       "      <td>Tryptophan</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>0.037631</td>\n",
+       "      <td>0.027620</td>\n",
+       "      <td>-0.225086</td>\n",
+       "      <td>0.041972</td>\n",
+       "      <td>-0.005555</td>\n",
+       "      <td>0.572334</td>\n",
+       "      <td>-0.127380</td>\n",
+       "      <td>0.005321</td>\n",
+       "      <td>-0.036205</td>\n",
+       "      <td>0.048941</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.099907</td>\n",
+       "      <td>0.392833</td>\n",
+       "      <td>-0.000110</td>\n",
+       "      <td>-0.218671</td>\n",
+       "      <td>-0.022165</td>\n",
+       "      <td>0.493825</td>\n",
+       "      <td>0.022181</td>\n",
+       "      <td>-0.003747</td>\n",
+       "      <td>-0.131424</td>\n",
+       "      <td>Glutaric Acid</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>0.094717</td>\n",
+       "      <td>0.006332</td>\n",
+       "      <td>0.020762</td>\n",
+       "      <td>0.648412</td>\n",
+       "      <td>-0.194657</td>\n",
+       "      <td>0.322575</td>\n",
+       "      <td>0.367796</td>\n",
+       "      <td>-0.412032</td>\n",
+       "      <td>-0.405478</td>\n",
+       "      <td>-0.128544</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.325018</td>\n",
+       "      <td>0.611758</td>\n",
+       "      <td>-0.001166</td>\n",
+       "      <td>-0.351308</td>\n",
+       "      <td>-0.043634</td>\n",
+       "      <td>-0.545758</td>\n",
+       "      <td>0.240541</td>\n",
+       "      <td>0.385630</td>\n",
+       "      <td>-0.294906</td>\n",
+       "      <td>Benzyl Benzoate</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>0.235307</td>\n",
+       "      <td>0.002806</td>\n",
+       "      <td>-0.166888</td>\n",
+       "      <td>0.166171</td>\n",
+       "      <td>-0.006953</td>\n",
+       "      <td>0.156601</td>\n",
+       "      <td>0.011887</td>\n",
+       "      <td>-0.108990</td>\n",
+       "      <td>-0.216767</td>\n",
+       "      <td>-0.576114</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.047719</td>\n",
+       "      <td>0.894500</td>\n",
+       "      <td>0.000387</td>\n",
+       "      <td>-0.240181</td>\n",
+       "      <td>-0.137264</td>\n",
+       "      <td>-0.162995</td>\n",
+       "      <td>-0.371316</td>\n",
+       "      <td>0.074220</td>\n",
+       "      <td>-0.373865</td>\n",
+       "      <td>Biphenyl</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>5 rows × 513 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "          0         1         2         3         4         5         6  \\\n",
+       "0  0.021717  0.013678 -0.124958  0.377754 -0.046985  0.244335 -0.287048   \n",
+       "1  0.092086  0.005441  0.052097 -0.224653 -0.066780  0.239429 -0.175937   \n",
+       "2  0.037631  0.027620 -0.225086  0.041972 -0.005555  0.572334 -0.127380   \n",
+       "3  0.094717  0.006332  0.020762  0.648412 -0.194657  0.322575  0.367796   \n",
+       "4  0.235307  0.002806 -0.166888  0.166171 -0.006953  0.156601  0.011887   \n",
+       "\n",
+       "          7         8         9  ...       503       504       505       506  \\\n",
+       "0 -0.164717 -0.064576 -0.370969  ... -0.135776  0.423812 -0.000535 -0.172896   \n",
+       "1 -0.382511 -0.231119 -0.178706  ... -0.125035  0.624961 -0.001381 -0.286743   \n",
+       "2  0.005321 -0.036205  0.048941  ... -0.099907  0.392833 -0.000110 -0.218671   \n",
+       "3 -0.412032 -0.405478 -0.128544  ... -0.325018  0.611758 -0.001166 -0.351308   \n",
+       "4 -0.108990 -0.216767 -0.576114  ... -0.047719  0.894500  0.000387 -0.240181   \n",
+       "\n",
+       "        507       508       509       510       511            Label  \n",
+       "0 -0.050619 -0.079060  0.083757  0.163587 -0.326790          Glycine  \n",
+       "1 -0.050950 -0.184345  0.168900  0.288457 -0.507141       Tryptophan  \n",
+       "2 -0.022165  0.493825  0.022181 -0.003747 -0.131424    Glutaric Acid  \n",
+       "3 -0.043634 -0.545758  0.240541  0.385630 -0.294906  Benzyl Benzoate  \n",
+       "4 -0.137264 -0.162995 -0.371316  0.074220 -0.373865         Biphenyl  \n",
+       "\n",
+       "[5 rows x 513 columns]"
+      ]
+     },
+     "execution_count": 4,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "mass_spec_encoder_generated_embeddings.head()"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": 5,
@@ -818,7 +1027,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -928,6 +1137,7 @@
     "                    print('-------------------------------------------')\n",
     "            else:\n",
     "                print(f'Validation loss has not improved in {epochs_without_validation_improvement} epochs. Stopping training at epoch {epoch}.')\n",
+    "                wandb.log({'Early Stopping Ecoch':epoch})\n",
     "                plot_pca(\n",
     "                    train_data, combo['batch_size'], encoder, device, \n",
     "                    encoder_criterion, sorted_chem_names, all_embeddings_df, \n",
diff --git a/models/ims_to_onehot_encoder.ipynb b/models/ims_to_onehot_encoder.ipynb
index 89b3e5d3..94288b45 100644
--- a/models/ims_to_onehot_encoder.ipynb
+++ b/models/ims_to_onehot_encoder.ipynb
@@ -1156,6 +1156,7 @@
     "                    print('-------------------------------------------')\n",
     "            else:\n",
     "                print(f'Validation loss has not improved in {epochs_without_validation_improvement} epochs. Stopping training at epoch {epoch}.')\n",
+    "                wandb.log({'Early Stopping Ecoch':epoch})\n",
     "                plot_pca(\n",
     "                    train_data, combo['batch_size'], encoder, device, \n",
     "                    encoder_criterion, sorted_chem_names, all_embeddings_df, \n",
diff --git a/models/mnist_encoder.ipynb b/models/mnist_encoder.ipynb
index 3cf91dd1..020baf7e 100644
--- a/models/mnist_encoder.ipynb
+++ b/models/mnist_encoder.ipynb
@@ -841,7 +841,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 50,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -953,6 +953,7 @@
     "                    print('-------------------------------------------')\n",
     "            else:\n",
     "                print(f'Validation loss has not improved in {epochs_without_validation_improvement} epochs. Stopping training at epoch {epoch}.')\n",
+    "                wandb.log({'Early Stopping Ecoch':epoch})\n",
     "                plot_pca(\n",
     "                    train_data, combo['batch_size'], encoder, device, \n",
     "                    encoder_criterion, sorted_chem_names, all_embeddings_df, \n",
diff --git a/models/tmp_plot.png b/models/tmp_plot.png
index c74ee65e..b251399a 100644
Binary files a/models/tmp_plot.png and b/models/tmp_plot.png differ
diff --git a/models/wandb/debug-internal.log b/models/wandb/debug-internal.log
index c2840df9..a0fb2bb3 120000
--- a/models/wandb/debug-internal.log
+++ b/models/wandb/debug-internal.log
@@ -1 +1 @@
-run-20241210_003145-ngvq1jdz/logs/debug-internal.log
\ No newline at end of file
+run-20241212_133434-kzr16w9e/logs/debug-internal.log
\ No newline at end of file
diff --git a/models/wandb/debug.log b/models/wandb/debug.log
index c154948b..51e3e5ef 120000
--- a/models/wandb/debug.log
+++ b/models/wandb/debug.log
@@ -1 +1 @@
-run-20241210_003145-ngvq1jdz/logs/debug.log
\ No newline at end of file
+run-20241212_133434-kzr16w9e/logs/debug.log
\ No newline at end of file
diff --git a/models/wandb/latest-run b/models/wandb/latest-run
index b7a9e86a..f30c6608 120000
--- a/models/wandb/latest-run
+++ b/models/wandb/latest-run
@@ -1 +1 @@
-run-20241210_003145-ngvq1jdz
\ No newline at end of file
+run-20241212_133434-kzr16w9e
\ No newline at end of file
