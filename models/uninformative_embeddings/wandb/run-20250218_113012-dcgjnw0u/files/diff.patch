diff --git a/models/__pycache__/functions.cpython-310.pyc b/models/__pycache__/functions.cpython-310.pyc
index 3eeabbbc..24f65a28 100644
Binary files a/models/__pycache__/functions.cpython-310.pyc and b/models/__pycache__/functions.cpython-310.pyc differ
diff --git a/models/__pycache__/plotting_functions.cpython-310.pyc b/models/__pycache__/plotting_functions.cpython-310.pyc
index e83d11d4..68ee32f7 100644
Binary files a/models/__pycache__/plotting_functions.cpython-310.pyc and b/models/__pycache__/plotting_functions.cpython-310.pyc differ
diff --git a/models/plot_generator_results.py b/models/plot_generator_results.py
index b2aa8af2..e80ea452 100644
--- a/models/plot_generator_results.py
+++ b/models/plot_generator_results.py
@@ -3,6 +3,7 @@ import pandas as pd
 import torch.nn as nn
 import importlib
 import functions as f
+import plotting_functions as pf
 import random
 
 #%%
@@ -11,7 +12,7 @@ importlib.reload(f)
 file_path = '../../scratch/test_data.feather'
 spectra = pd.read_feather(file_path)
 #%%
-chem = 'JP8'
+chem = 'MES'
 # gen_type = 'Individual'
 file_path = f'../data/ims_data/synthetic_test_{chem}_spectra.csv'
 synthetic_spectra_df = pd.read_csv(file_path)
@@ -47,6 +48,6 @@ for i in range(num_plots):
         )
 #%
 #%%
-f.plot_similarity_comparison(spectra, chem, synthetic_spectra_df, 'Individual', 2, -1)#, similarity_type='spect_avg')
-f.plot_similarity_comparison(spectra, chem, synthetic_spectra_df_universal, 'Universal', 0, -1)#, similarity_type='spect_avg')
+pf.plot_similarity_comparison(spectra, chem, synthetic_spectra_df, 'Individual', 2, -1)#, similarity_type='spect_avg')
+pf.plot_similarity_comparison(spectra, chem, synthetic_spectra_df_universal, 'Universal', 0, -1)#, similarity_type='spect_avg')
 #%%
\ No newline at end of file
diff --git a/models/uninformative_embeddings/ims_to_onehot_encoder.py b/models/uninformative_embeddings/ims_to_onehot_encoder.py
index fddf4351..fce998d2 100644
--- a/models/uninformative_embeddings/ims_to_onehot_encoder.py
+++ b/models/uninformative_embeddings/ims_to_onehot_encoder.py
@@ -4,16 +4,21 @@ import numpy as np
 from torch.utils.data import TensorDataset
 import os
 import importlib
+import sys
+
+parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
+sys.path.append(parent_dir)
+import plotting_functions as pf
 import functions as f
 # Reload the functions module after updates
 # importlib.reload(f)
 
 # Loading Data:
-file_path = '../../scratch/train_data.feather'
+file_path = '../../../scratch/train_data.feather'
 train_spectra = pd.read_feather(file_path)
-file_path = '../../scratch/val_data.feather'
+file_path = '../../../scratch/val_data.feather'
 val_spectra = pd.read_feather(file_path)
-file_path = '../../scratch/test_data.feather'
+file_path = '../../../scratch/test_data.feather'
 test_spectra = pd.read_feather(file_path)
 
 # file_path = '../data/name_smiles_embedding_file.csv'
@@ -46,11 +51,11 @@ y_test, x_test, test_chem_encodings_tensor, test_carl_indices_tensor = f.create_
 del test_spectra
 #%%
 # Things that need to be changed for each encoder/dataset/target embedding
-notebook_name = '/home/cmdunham/ChemicalDataGeneration/models/ims_to_onehot_encoder.py'
+notebook_name = '/home/cmdunham/ChemicalDataGeneration/models/uninformative_embeddings/ims_to_onehot_encoder.py'
 architecture = 'ims_to_onehot_encoder'
 dataset_type = 'ims'
 target_embedding = 'OneHot'
-encoder_path = 'trained_models/ims_to_onehot_encoder.pth'
+encoder_path = '../trained_models/ims_to_onehot_encoder.pth'
 model_type = 'IMStoOneHotEncoder'
 
 config = {
@@ -75,7 +80,7 @@ wandb_kwargs = {
 }
 
 model_hyperparams = {
-  'batch_size':[16,8],
+  'batch_size':[8,4],
   'epochs': [500],
   'learning_rate':[.00001, .000001],
   }
diff --git a/models/uninformative_embeddings/ims_to_onehot_encoder_inference.py b/models/uninformative_embeddings/ims_to_onehot_encoder_inference.py
index b2ba5f1c..cc2e3246 100644
--- a/models/uninformative_embeddings/ims_to_onehot_encoder_inference.py
+++ b/models/uninformative_embeddings/ims_to_onehot_encoder_inference.py
@@ -5,11 +5,16 @@ import numpy as np
 import torch
 import torch.nn as nn
 from torch.utils.data import DataLoader, TensorDataset
-
+import os
+import sys
+#%%
+import importlib
 # from collections import Counter
 # import importlib
-import functions as f
+parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
+sys.path.append(parent_dir)
 import plotting_functions as pf
+import functions as f
 
 #%%
 device = f.set_up_gpu()
@@ -134,19 +139,20 @@ train_preds_df.to_csv(file_path, index=False)
 # # del y_test, x_test, test_chem_encodings_tensor, test_indices_tensor
 
 # #%%
-
-# file_path = '../data/encoder_embedding_predictions/ims_to_onehot_encoder_val_preds.csv'
-# test_preds_df = pd.read_csv(file_path)
-# test_preds_df.head()
-# sorted_chem_names = ['DEB','DEM','DMMP','DPM','DtBP','JP8','MES','TEPO']
-# encodings_list = test_preds_df[sorted_chem_names].values.tolist()
-# # spectra_labels = [sorted_chem_names[list(enc).index(1)] for enc in encodings_list]
-# embeddings_only = test_preds_df.iloc[:,1:-8].copy()
-# embeddings_only.columns = all_true_embeddings.T.columns
-# # embeddings_only['Label'] = test['Label']
-
-# embeddings_only['Label'] = [sorted_chem_names[enc.index(1)] for enc in encodings_list]
-# pf.plot_emb_pca(
-#     all_true_embeddings, embeddings_only, 'Validation', 'IMS', embedding_type='OneHot',
-#     log_wandb=False, chemnet_embeddings_to_plot=all_true_embeddings,
-#     show_wandb_run_name=False)
\ No newline at end of file
+importlib.reload(pf)
+#%%
+file_path = '../../data/encoder_embedding_predictions/ims_to_onehot_encoder_test_preds.csv'
+test_preds_df = pd.read_csv(file_path)
+test_preds_df.head()
+sorted_chem_names = ['DEB','DEM','DMMP','DPM','DtBP','JP8','MES','TEPO']
+encodings_list = test_preds_df[sorted_chem_names].values.tolist()
+# spectra_labels = [sorted_chem_names[list(enc).index(1)] for enc in encodings_list]
+embeddings_only = test_preds_df.iloc[:,1:-8].copy()
+embeddings_only.columns = all_true_embeddings.T.columns
+# embeddings_only['Label'] = test['Label']
+
+embeddings_only['Label'] = [sorted_chem_names[enc.index(1)] for enc in encodings_list]
+pf.plot_emb_pca(
+    all_true_embeddings, embeddings_only, 'Test', 'IMS', embedding_type='OneHot',
+    log_wandb=False, chemnet_embeddings_to_plot=all_true_embeddings,
+    show_wandb_run_name=False)
\ No newline at end of file
diff --git a/models/uninformative_embeddings/onehot_to_ims_universal_generator.py b/models/uninformative_embeddings/onehot_to_ims_universal_generator.py
index 52fbffd6..daaadc5c 100644
--- a/models/uninformative_embeddings/onehot_to_ims_universal_generator.py
+++ b/models/uninformative_embeddings/onehot_to_ims_universal_generator.py
@@ -67,9 +67,9 @@ wandb_kwargs = {
     'early stopping threshold':20
 }
 model_hyperparams = {
-    'batch_size':[32, 16],
+    'batch_size':[16, 8],
     'epochs': [500],
-    'learning_rate':[.0001,.00001],
+    'learning_rate':[.001, .0001],
     }
 
 num_plots = 5
diff --git a/models/uninformative_embeddings/tmp_plot.png b/models/uninformative_embeddings/tmp_plot.png
index f2551c46..d4e84499 100644
Binary files a/models/uninformative_embeddings/tmp_plot.png and b/models/uninformative_embeddings/tmp_plot.png differ
diff --git a/models/uninformative_embeddings/wandb/debug-internal.log b/models/uninformative_embeddings/wandb/debug-internal.log
index 6f16fd33..0038203d 120000
--- a/models/uninformative_embeddings/wandb/debug-internal.log
+++ b/models/uninformative_embeddings/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250217_232345-iu1d6bis/logs/debug-internal.log
\ No newline at end of file
+run-20250218_113012-dcgjnw0u/logs/debug-internal.log
\ No newline at end of file
diff --git a/models/uninformative_embeddings/wandb/debug.log b/models/uninformative_embeddings/wandb/debug.log
index f128796c..9573f336 120000
--- a/models/uninformative_embeddings/wandb/debug.log
+++ b/models/uninformative_embeddings/wandb/debug.log
@@ -1 +1 @@
-run-20250217_232345-iu1d6bis/logs/debug.log
\ No newline at end of file
+run-20250218_113012-dcgjnw0u/logs/debug.log
\ No newline at end of file
diff --git a/models/uninformative_embeddings/wandb/latest-run b/models/uninformative_embeddings/wandb/latest-run
index 1ff20eef..528a3117 120000
--- a/models/uninformative_embeddings/wandb/latest-run
+++ b/models/uninformative_embeddings/wandb/latest-run
@@ -1 +1 @@
-run-20250217_232345-iu1d6bis
\ No newline at end of file
+run-20250218_113012-dcgjnw0u
\ No newline at end of file
