Using device: cuda
--------------------------
--------------------------
New run with hyperparameters:
batch_size  :  64
epochs  :  500
learning_rate  :  1e-07
Epoch[10/500]:
   Training loss: 0.005788553174381566
   Validation loss: 0.005304490546254313
-------------------------------------------
Epoch[20/500]:
   Training loss: 0.0014558020404510097
   Validation loss: 0.0013581420837375656
-------------------------------------------
Epoch[30/500]:
   Training loss: 0.0006255111950143891
   Validation loss: 0.0006037531904816017
-------------------------------------------
Epoch[40/500]:
   Training loss: 0.0003854569823359176
   Validation loss: 0.00037642468136066877
-------------------------------------------
Epoch[50/500]:
   Training loss: 0.0002732070201387092
   Validation loss: 0.0002717701988707443
-------------------------------------------
Epoch[60/500]:
   Training loss: 0.00020825314813119382
   Validation loss: 0.00021101473099594746
-------------------------------------------
Epoch[70/500]:
   Training loss: 0.00016630639564682811
   Validation loss: 0.00017040289467645223
-------------------------------------------
Epoch[80/500]:
   Training loss: 0.00013714080792719233
   Validation loss: 0.00014150536334266785
-------------------------------------------
Epoch[90/500]:
   Training loss: 0.00011560993306367103
   Validation loss: 0.00012016525922796251
-------------------------------------------
Epoch[100/500]:
   Training loss: 9.944101345079451e-05
   Validation loss: 0.00010351311157294198
-------------------------------------------
Epoch[110/500]:
   Training loss: 8.679895445047841e-05
   Validation loss: 9.121613450996189e-05
-------------------------------------------
Epoch[120/500]:
   Training loss: 7.682217026652495e-05
   Validation loss: 8.153342894823661e-05
-------------------------------------------
Epoch[130/500]:
   Training loss: 6.861766994860893e-05
   Validation loss: 7.508545945711376e-05
-------------------------------------------
Epoch[140/500]:
   Training loss: 6.181259772399275e-05
   Validation loss: 6.742406894145774e-05
-------------------------------------------
Epoch[150/500]:
   Training loss: 5.610764166166571e-05
   Validation loss: 6.379866750523497e-05
-------------------------------------------
Epoch[160/500]:
   Training loss: 5.127427526623365e-05
   Validation loss: 5.76506851898714e-05
-------------------------------------------
Epoch[170/500]:
   Training loss: 4.702643471756082e-05
   Validation loss: 5.276250841108839e-05
-------------------------------------------
Epoch[180/500]:
   Training loss: 4.346515909791963e-05
   Validation loss: 4.885715494812506e-05
-------------------------------------------
Epoch[190/500]:
   Training loss: 4.030483049065368e-05
   Validation loss: 4.572760551895797e-05
-------------------------------------------
Epoch[200/500]:
   Training loss: 3.744794581686602e-05
   Validation loss: 4.2959554654207125e-05
-------------------------------------------
Epoch[210/500]:
   Training loss: 3.502514069131305e-05
   Validation loss: 4.05510792351661e-05
-------------------------------------------
Epoch[220/500]:
   Training loss: 3.28246911559488e-05
   Validation loss: 3.8296519874322635e-05
-------------------------------------------
Epoch[230/500]:
   Training loss: 3.080940093271519e-05
   Validation loss: 3.7916722656769584e-05
-------------------------------------------
Epoch[240/500]:
   Training loss: 2.8942084011276392e-05
   Validation loss: 3.4538579621641536e-05
-------------------------------------------
Epoch[250/500]:
   Training loss: 2.73673714401025e-05
   Validation loss: 3.281632025234636e-05
-------------------------------------------
Epoch[260/500]:
   Training loss: 2.587384575454157e-05
   Validation loss: 3.144699651564788e-05
-------------------------------------------
Epoch[270/500]:
   Training loss: 2.4460212796594215e-05
   Validation loss: 3.163528895090073e-05
-------------------------------------------
Epoch[280/500]:
   Training loss: 2.3230947184019277e-05
   Validation loss: 3.026371862508062e-05
-------------------------------------------
Epoch[290/500]:
   Training loss: 2.2152357852698172e-05
   Validation loss: 2.769545383491026e-05
-------------------------------------------
Epoch[300/500]:
   Training loss: 2.1014354587593258e-05
   Validation loss: 2.7041198516850145e-05
-------------------------------------------
Epoch[310/500]:
   Training loss: 2.0030040055936835e-05
   Validation loss: 2.5425316464563744e-05
-------------------------------------------
Epoch[320/500]:
   Training loss: 1.9122535852952946e-05
   Validation loss: 2.5065123340623517e-05
-------------------------------------------
Epoch[330/500]:
   Training loss: 1.8266657918004233e-05
   Validation loss: 2.4103759405283326e-05
-------------------------------------------
Epoch[340/500]:
   Training loss: 1.748132540208302e-05
   Validation loss: 2.2996811212343692e-05
-------------------------------------------
Epoch[350/500]:
   Training loss: 1.6786380155447515e-05
   Validation loss: 2.253024045777583e-05
-------------------------------------------
Epoch[360/500]:
   Training loss: 1.6081700895632522e-05
   Validation loss: 2.1972774937960914e-05
-------------------------------------------
Epoch[370/500]:
   Training loss: 1.532184740323294e-05
   Validation loss: 2.7629961964286286e-05
-------------------------------------------
Epoch[380/500]:
   Training loss: 1.4771324016709234e-05
   Validation loss: 2.0282387755945947e-05
-------------------------------------------
Epoch[390/500]:
   Training loss: 1.4268790565134793e-05
   Validation loss: 1.971867866673008e-05
-------------------------------------------
Epoch[400/500]:
   Training loss: 1.3747378376018496e-05
   Validation loss: 1.922343116787268e-05
-------------------------------------------
Epoch[410/500]:
   Training loss: 1.3244730664622492e-05
   Validation loss: 1.8886793786178117e-05
