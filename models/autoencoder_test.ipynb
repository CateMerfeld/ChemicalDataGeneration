{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# from ray import tune\n",
    "from torch.utils.data import DataLoader, TensorDataset #, Dataset\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import GPUtil\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data:\n",
    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_data.csv'\n",
    "val = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Linear(1676,652),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected GPU ID: 1\n",
      "  Name: NVIDIA GeForce RTX 4090\n",
      "  Memory Free: 17082.0 MB\n",
      "  Memory Used: 7135.0 MB\n",
      "  GPU Load: 0.00%\n",
      "Current device ID:  cuda:1\n",
      "PyTorch current device ID: 1\n",
      "PyTorch current device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the list of GPUs\n",
    "    gpus = GPUtil.getGPUs()\n",
    "\n",
    "    # Find the GPU with the most free memory\n",
    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
    "\n",
    "    # Print details about the selected GPU\n",
    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
    "    print(f\"  Name: {best_gpu.name}\")\n",
    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
    "\n",
    "    # Set the device for later use\n",
    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
    "    print('Current device ID: ', device)\n",
    "\n",
    "    # Set the current device in PyTorch\n",
    "    torch.cuda.set_device(best_gpu.id)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Confirm the currently selected device in PyTorch\n",
    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_dataset, device, model, criterion, optimizer, epoch, combo):\n",
    "  epoch_training_loss = 0\n",
    "\n",
    "  predicted_spectra = []\n",
    "  output_name_encodings = []\n",
    "  original_spectra = []\n",
    "\n",
    "  # train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
    "  for true_spectra, name_encodings, true_embeddings in train_dataset:\n",
    "    # move inputs to device\n",
    "    true_spectra = true_spectra.to(device)\n",
    "    name_encodings = name_encodings.to(device)\n",
    "    true_embeddings = true_embeddings.to(device)\n",
    "\n",
    "    # backprapogation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    batch_predicted_spectra = model(true_embeddings)\n",
    "\n",
    "    loss = criterion(batch_predicted_spectra, true_spectra)\n",
    "    # accumulate epoch training loss\n",
    "    epoch_training_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # at last epoch store output embeddings and corresponding labels to output list\n",
    "    if (epoch + 1) == combo['epochs']:\n",
    "      for enc, spec, true_spec in zip(name_encodings, batch_predicted_spectra, true_spectra):\n",
    "        output_name_encodings.append(enc.cpu().detach().numpy())\n",
    "        predicted_spectra.append(spec.cpu().detach().numpy())\n",
    "        original_spectra.append(true_spec.cpu().detach().numpy())\n",
    "\n",
    "  # divide by number of batches to calculate average loss\n",
    "  average_loss = epoch_training_loss/len(train_dataset)\n",
    "  if (epoch + 1) == combo['epochs']:\n",
    "    return average_loss, predicted_spectra, output_name_encodings, original_spectra\n",
    "  else:\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
    "\n",
    "val_spectra = val.iloc[:,2:-9]\n",
    "val_chem_encodings = val.iloc[:,-8:]\n",
    "\n",
    "# create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
    "\n",
    "val_chem_labels = list(val['Label'])\n",
    "val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
    "val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
    "val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set var deciding if results plot for this run is saved to wandb\n",
    "log_wandb = True\n",
    "\n",
    "# Last 8 cols of the df are the chem names\n",
    "sorted_chem_names = list(val.columns[-8:])\n",
    "model_config = {\n",
    "    'batch_size': [32],\n",
    "    'epochs': [30],\n",
    "learning_rate = .001\n",
    "\n",
    "val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
    "generator = Autoencoder().to(device)\n",
    "\n",
    "generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
    "generator_criterion = nn.MSELoss()\n",
    "\n",
    "wandb_kwargs = {\n",
    "  'learning_rate': combo['learning_rate'],\n",
    "  'epochs': combo['epochs'],\n",
    "  'batch_size': combo['batch_size'],\n",
    "  'model_architecture': 'generator',\n",
    "  'optimizer':'AdamW',\n",
    "  'loss': 'MSELoss'\n",
    "}\n",
    "\n",
    "run_with_wandb(config, **wandb_kwargs)\n",
    "\n",
    "print('--------------------------')\n",
    "print('--------------------------')\n",
    "print('New run with hyperparameters:')\n",
    "for key in combo:\n",
    "  print(key, ' : ', combo[key])\n",
    "print('--------------------------')\n",
    "print('--------------------------')\n",
    "\n",
    "for epoch in range(combo['epochs']):\n",
    "  # Set model to training mode\n",
    "  generator.train(True)\n",
    "\n",
    "  # do a pass over the data\n",
    "  # at last epoch get predicted embeddings and chem names\n",
    "  if (epoch + 1) == combo['epochs']:\n",
    "    average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
    "      train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
    "      )\n",
    "  else:\n",
    "    average_loss = train_one_epoch(\n",
    "      train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
    "      )\n",
    "\n",
    "  epoch_val_loss = 0  \n",
    "  # evaluate model on validation data\n",
    "  generator.eval() # Set model to evaluation mode\n",
    "  with torch.no_grad():\n",
    "    for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
    "      val_true_spectra = val_true_spectra.to(device)\n",
    "      val_name_encodings = val_name_encodings.to(device)\n",
    "      val_true_embeddings = val_true_embeddings.to(device)\n",
    "\n",
    "      val_batch_predicted_spectra = generator(val_true_embeddings)\n",
    "\n",
    "      val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
    "      # accumulate epoch validation loss\n",
    "      epoch_val_loss += val_loss.item()\n",
    "\n",
    "  # divide by number of batches to calculate average loss\n",
    "  val_average_loss = epoch_val_loss/len(val_dataset)\n",
    "\n",
    "  # log losses to wandb\n",
    "  wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
    "\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
    "    print(f'   Training loss: {average_loss}')\n",
    "    print(f'   Validation loss: {val_average_loss}')\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "\n",
    "  if average_loss < lowest_loss:\n",
    "    lowest_loss = average_loss\n",
    "    best_hyperparams = combo\n",
    "\n",
    "  wandb.finish()\n",
    "\n",
    "print('Hyperparameters for best model: ')\n",
    "for key in best_hyperparams:\n",
    "  print('   ', key, ' : ', best_hyperparams[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem_data_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
