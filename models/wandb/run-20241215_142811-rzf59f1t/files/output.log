Using device: cuda
--------------------------
--------------------------
New run with hyperparameters:
batch_size  :  32
epochs  :  500
learning_rate  :  0.0001
Saved best model at epoch 1
Saved best model at epoch 2
Saved best model at epoch 3
Saved best model at epoch 4
Saved best model at epoch 5
Saved best model at epoch 10
Epoch[10/500]:
   Training loss: 0.005553811512344318
   Validation loss: 4.009270460874534e-05
-------------------------------------------
Saved best model at epoch 12
Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.
Saved best model at epoch 19
Saved best model at epoch 20
Epoch[20/500]:
   Training loss: 1.8432823114344237e-05
   Validation loss: 8.909529087568407e-06
-------------------------------------------
Saved best model at epoch 22
Saved best model at epoch 24
Saved best model at epoch 25
Saved best model at epoch 26
Saved best model at epoch 27
Saved best model at epoch 28
Epoch[30/500]:
   Training loss: 4.601335810216144e-06
   Validation loss: 1.3007766241045856e-05
-------------------------------------------
Saved best model at epoch 34
Saved best model at epoch 35
Epoch[40/500]:
   Training loss: 4.980313998861122e-06
   Validation loss: 4.677129718986202e-06
-------------------------------------------
Epoch 00041: reducing learning rate of group 0 to 1.0000e-06.
Saved best model at epoch 42
Saved best model at epoch 43
Saved best model at epoch 44
Saved best model at epoch 45
Saved best model at epoch 46
Saved best model at epoch 47
Saved best model at epoch 48
Saved best model at epoch 49
Epoch[50/500]:
   Training loss: 5.36700172267921e-07
   Validation loss: 2.495943495046346e-06
-------------------------------------------
Saved best model at epoch 53
Saved best model at epoch 58
Saved best model at epoch 59
Epoch[60/500]:
   Training loss: 3.6595618942169463e-07
   Validation loss: 2.341514914679036e-06
-------------------------------------------
Saved best model at epoch 63
Saved best model at epoch 64
Saved best model at epoch 67
Saved best model at epoch 69
Saved best model at epoch 70
Epoch[70/500]:
   Training loss: 2.746947368164536e-07
   Validation loss: 2.2433722206770487e-06
-------------------------------------------
Saved best model at epoch 72
Saved best model at epoch 76
Saved best model at epoch 78
Saved best model at epoch 79
Saved best model at epoch 80
Epoch[80/500]:
   Training loss: 1.9427510418037327e-07
   Validation loss: 2.133211739869186e-06
-------------------------------------------
Saved best model at epoch 82
Saved best model at epoch 83
Epoch 00089: reducing learning rate of group 0 to 1.0000e-07.
Epoch[90/500]:
   Training loss: 1.5039419964008617e-07
   Validation loss: 2.175712905553284e-06
-------------------------------------------
Validation loss has not improved in 10 epochs. Stopping training at epoch 93.
-------------------------------------------
-------------------------------------------
Dataset:  carls
Target Embeddings:  ChemNet
-------------------------------------------
-------------------------------------------
Encoder(
  (encoder): Sequential(
    (0): Linear(in_features=1676, out_features=1548, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=1548, out_features=1420, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=1420, out_features=1292, bias=True)
    (5): LeakyReLU(negative_slope=0.01, inplace=True)
    (6): Linear(in_features=1292, out_features=1164, bias=True)
    (7): LeakyReLU(negative_slope=0.01, inplace=True)
    (8): Linear(in_features=1164, out_features=1036, bias=True)
    (9): LeakyReLU(negative_slope=0.01, inplace=True)
    (10): Linear(in_features=1036, out_features=908, bias=True)
    (11): LeakyReLU(negative_slope=0.01, inplace=True)
    (12): Linear(in_features=908, out_features=780, bias=True)
    (13): LeakyReLU(negative_slope=0.01, inplace=True)
    (14): Linear(in_features=780, out_features=652, bias=True)
    (15): LeakyReLU(negative_slope=0.01, inplace=True)
    (16): Linear(in_features=652, out_features=512, bias=True)
  )
)
-------------------------------------------
-------------------------------------------