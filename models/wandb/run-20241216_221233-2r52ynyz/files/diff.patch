diff --git a/data_preprocessing/ims_data_preprocessing.ipynb b/data_preprocessing/ims_data_preprocessing.ipynb
index 75cb4d7f..96559bb7 100644
--- a/data_preprocessing/ims_data_preprocessing.ipynb
+++ b/data_preprocessing/ims_data_preprocessing.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -781,7 +781,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
@@ -831,7 +831,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -850,7 +850,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -878,7 +878,7 @@
     "\n",
     "    index_col = np.repeat(spectra_df['index'].values, p)\n",
     "    differences_df.insert(0, 'index', index_col)\n",
-    "    label_info = pd.DataFrame(np.repeat(spectra_df.iloc[:,-9:].values, 2, axis=0))\n",
+    "    label_info = pd.DataFrame(np.repeat(spectra_df.iloc[:,-9:].values, p, axis=0))\n",
     "    differences_df = pd.concat([differences_df, label_info], axis=1)\n",
     "    differences_df.columns = spectra_df.columns[1:]\n",
     "\n",
@@ -887,17 +887,19 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 11,
    "metadata": {},
    "outputs": [],
    "source": [
-    "n=10\n",
+    "n=50\n",
     "train_background_sample = background.sample(n=n, random_state=42).iloc[:,1:-1]\n",
+    "# removing train bkg spectra so they are not used in val or test\n",
     "reduced_background = background.drop(train_background_sample.index)\n",
     "train_background_sample.reset_index(inplace=True)\n",
     "train_background_sample.drop(columns=['index'], inplace=True)\n",
     "\n",
     "val_background_sample = reduced_background.sample(n=n, random_state=42).iloc[:,1:-1]\n",
+    "# removing val bkg spectra so they are not used in test\n",
     "reduced_background = reduced_background.drop(val_background_sample.index)\n",
     "val_background_sample.reset_index(inplace=True)\n",
     "val_background_sample.drop(columns=['index'], inplace=True)\n",
@@ -909,7 +911,108 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 16,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/train_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(train, train_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(val, val_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(test, test_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Generating Carls Using More BKG Spectra but Only Samples of Simulant Datasets:\n",
+    "---\n",
+    "Using the entire datasets was making the data very large to work with"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# creating smaller samples to make carls from\n",
+    "n=3000\n",
+    "train_sample = train.sample(n=n, random_state=42).iloc[:,1:]\n",
+    "train_sample.reset_index(inplace=True)\n",
+    "\n",
+    "val_sample = val.sample(n=int(n/3), random_state=42).iloc[:,1:]\n",
+    "val_sample.reset_index(inplace=True)\n",
+    "\n",
+    "test_sample = test.sample(n=int(n/3), random_state=42).iloc[:,1:]\n",
+    "test_sample.reset_index(inplace=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/train_sample_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(train_sample, train_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 29,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/val_sample_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(val_sample, val_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 30,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_sample_carls_dif_backgrounds.csv'\n",
+    "differences_df = generate_differences_df(test_sample, test_background_sample)\n",
+    "differences_df.to_csv(file_path, index=False, header=True)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Plotting:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -936,7 +1039,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -976,39 +1079,6 @@
     "plot_spectra(test_background_sample)"
    ]
   },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/train_carls_dif_backgrounds.csv'\n",
-    "differences_df = generate_differences_df(train, train_background_sample)\n",
-    "differences_df.to_csv(file_path, index=False, header=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_dif_backgrounds.csv'\n",
-    "differences_df = generate_differences_df(val, val_background_sample)\n",
-    "differences_df.to_csv(file_path, index=False, header=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_dif_backgrounds.csv'\n",
-    "differences_df = generate_differences_df(test, test_background_sample)\n",
-    "differences_df.to_csv(file_path, index=False, header=True)"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
diff --git a/models/__pycache__/functions.cpython-38.pyc b/models/__pycache__/functions.cpython-38.pyc
index f378c7db..2cb6fc45 100644
Binary files a/models/__pycache__/functions.cpython-38.pyc and b/models/__pycache__/functions.cpython-38.pyc differ
diff --git a/models/carl_encoder.ipynb b/models/carl_encoder.ipynb
index 4a7b58c7..a8c82298 100644
--- a/models/carl_encoder.ipynb
+++ b/models/carl_encoder.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -17,16 +17,14 @@
     "import wandb\n",
     "import os\n",
     "from sklearn.decomposition import PCA\n",
-    "import GPUtil\n",
     "import itertools\n",
-    "import io\n",
     "\n",
     "from collections import Counter"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
@@ -35,7 +33,7 @@
        "<module 'functions' from '/home/cmdunham/ChemicalDataGeneration/models/functions.py'>"
       ]
      },
-     "execution_count": 13,
+     "execution_count": 2,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -43,7 +41,6 @@
    "source": [
     "import importlib\n",
     "import functions as f\n",
-    "\n",
     "# Reload the functions module after updates\n",
     "importlib.reload(f)"
    ]
@@ -58,7 +55,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -67,12 +64,18 @@
     "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_dif_backgrounds.csv'\n",
     "val_carls = pd.read_csv(file_path)\n",
     "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_dif_backgrounds.csv'\n",
-    "test_carls = pd.read_csv(file_path)"
+    "test_carls = pd.read_csv(file_path)\n",
+    "# file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_carls_.csv'\n",
+    "# train_carls = pd.read_csv(file_path)\n",
+    "# file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_.csv'\n",
+    "# val_carls = pd.read_csv(file_path)\n",
+    "# file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_.csv'\n",
+    "# test_carls = pd.read_csv(file_path)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 13,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -211,7 +214,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -440,13 +443,13 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## Setting up GPU:\n",
+    "# Training Encoder on Carls:\n",
     "---"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 7,
    "metadata": {},
    "outputs": [
     {
@@ -465,57 +468,37 @@
     }
    ],
    "source": [
-    "if torch.cuda.is_available():\n",
-    "    # Get the list of GPUs\n",
-    "    gpus = GPUtil.getGPUs()\n",
-    "\n",
-    "    # Find the GPU with the most free memory\n",
-    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
-    "\n",
-    "    # Print details about the selected GPU\n",
-    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
-    "    print(f\"  Name: {best_gpu.name}\")\n",
-    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
-    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
-    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
-    "\n",
-    "    # Set the device for later use\n",
-    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
-    "    print('Current device ID: ', device)\n",
-    "\n",
-    "    # Set the current device in PyTorch\n",
-    "    torch.cuda.set_device(best_gpu.id)\n",
-    "else:\n",
-    "    device = torch.device('cpu')\n",
-    "    print('Using CPU')\n",
-    "\n",
-    "# Confirm the currently selected device in PyTorch\n",
-    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
-    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
+    "device = f.set_up_gpu()"
    ]
   },
   {
-   "cell_type": "markdown",
+   "cell_type": "code",
+   "execution_count": 8,
    "metadata": {},
+   "outputs": [],
    "source": [
-    "# Training Encoder on Carls:\n",
-    "---"
+    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_carls_dif_backgrounds.csv'\n",
+    "train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = f.create_dataset_tensors_with_dask(file_path, name_smiles_embedding_df, device, carl=True)\n",
+    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_dif_backgrounds.csv'\n",
+    "val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor = f.create_dataset_tensors_with_dask(file_path, name_smiles_embedding_df, device, carl=True)\n",
+    "file_path='/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_dif_backgrounds.csv'\n",
+    "test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor = f.create_dataset_tensors_with_dask(file_path, name_smiles_embedding_df, device, carl=True)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = f.create_dataset_tensors(train_carls, name_smiles_embedding_df, device, carl=True)\n",
-    "val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor = f.create_dataset_tensors(val_carls, name_smiles_embedding_df, device, carl=True)\n",
-    "test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor = f.create_dataset_tensors(test_carls, name_smiles_embedding_df, device, carl=True)"
+    "# train_embeddings_tensor, train_carl_tensor, train_chem_encodings_tensor, train_carl_indices_tensor = f.create_dataset_tensors(train_carls, name_smiles_embedding_df, device, carl=True)\n",
+    "# val_embeddings_tensor, val_carl_tensor, val_chem_encodings_tensor, val_carl_indices_tensor = f.create_dataset_tensors(val_carls, name_smiles_embedding_df, device, carl=True)\n",
+    "# test_embeddings_tensor, test_carl_tensor, test_chem_encodings_tensor, test_carl_indices_tensor = f.create_dataset_tensors(test_carls, name_smiles_embedding_df, device, carl=True)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 9,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -524,12 +507,21 @@
     "architecture = 'carl_encoder'\n",
     "dataset_type = 'carls'\n",
     "target_embedding = 'ChemNet'\n",
-    "encoder_path = '../models/carl_to_chemnet_encoder..pth'"
+    "encoder_path = '../models/carl_to_chemnet_encoder.pth'\n",
+    "\n",
+    "config = {\n",
+    "    'wandb_entity': 'catemerfeld',\n",
+    "    'wandb_project': 'ims_encoder_decoder',\n",
+    "    'gpu':True,\n",
+    "    'threads':1,\n",
+    "}\n",
+    "\n",
+    "os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 21,
+   "execution_count": 10,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -545,7 +537,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 25,
+   "execution_count": 11,
    "metadata": {},
    "outputs": [
     {
@@ -554,7 +546,7 @@
        "<module 'functions' from '/home/cmdunham/ChemicalDataGeneration/models/functions.py'>"
       ]
      },
-     "execution_count": 25,
+     "execution_count": 11,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -568,7 +560,133 @@
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcatemerfeld\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
+       " $ pip install wandb --upgrade"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Tracking run with wandb version 0.17.0"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Run data is saved locally in <code>/home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_213747-nhh0p7yo</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Syncing run <strong><a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nhh0p7yo' target=\"_blank\">sandy-puddle-499</a></strong> to <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View project at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nhh0p7yo' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nhh0p7yo</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Using device: cuda\n",
+      "--------------------------\n",
+      "--------------------------\n",
+      "New run with hyperparameters:\n",
+      "batch_size  :  64\n",
+      "epochs  :  500\n",
+      "learning_rate  :  1e-05\n",
+      "Saved best model at epoch 1\n",
+      "Saved best model at epoch 3\n",
+      "Saved best model at epoch 4\n",
+      "Saved best model at epoch 5\n",
+      "Saved best model at epoch 6\n",
+      "Saved best model at epoch 7\n",
+      "Saved best model at epoch 9\n",
+      "Saved best model at epoch 10\n",
+      "Epoch[10/500]:\n",
+      "   Training loss: 1.915830373660559e-05\n",
+      "   Validation loss: 0.006593671172373744\n",
+      "-------------------------------------------\n",
+      "Saved best model at epoch 11\n",
+      "Saved best model at epoch 12\n",
+      "Saved best model at epoch 17\n",
+      "Saved best model at epoch 19\n",
+      "Epoch[20/500]:\n",
+      "   Training loss: 1.2219062269495896e-05\n",
+      "   Validation loss: 0.006006265291592826\n",
+      "-------------------------------------------\n",
+      "Saved best model at epoch 21\n",
+      "Saved best model at epoch 24\n",
+      "Saved best model at epoch 25\n",
+      "Epoch[30/500]:\n",
+      "   Training loss: 1.1010536751406518e-05\n",
+      "   Validation loss: 0.004825665585039147\n",
+      "-------------------------------------------\n",
+      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-06.\n",
+      "Saved best model at epoch 32\n",
+      "Saved best model at epoch 33\n",
+      "Saved best model at epoch 34\n",
+      "Saved best model at epoch 36\n"
+     ]
+    }
+   ],
    "source": [
     "wandb_kwargs = {\n",
     "    'architecture': architecture,\n",
@@ -583,7 +701,7 @@
     "model_hyperparams = {\n",
     "  'batch_size':[64],\n",
     "  'epochs': [500],\n",
-    "  'learning_rate':[.0001]\n",
+    "  'learning_rate':[.00001]\n",
     "  }\n",
     "\n",
     "train_data = TensorDataset(train_carl_tensor, train_chem_encodings_tensor, train_embeddings_tensor, train_carl_indices_tensor)\n",
@@ -594,11 +712,164 @@
     "    'Encoder', train_data, val_data, test_data, \n",
     "    device, config, wandb_kwargs, \n",
     "    all_true_embeddings, name_smiles_embedding_df, model_hyperparams, \n",
-    "    sorted_chem_names, encoder_path, save_emb_pca_to_wandb=True, early_stop_threshold=20,\n",
+    "    sorted_chem_names, encoder_path, save_emb_pca_to_wandb=True, #early_stop_threshold=15,\n",
     "    input_type='Carl', show_wandb_run_name=True, lr_scheduler=True\n",
     "    )"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Encoder(\n",
+      "  (encoder): Sequential(\n",
+      "    (0): Linear(in_features=1676, out_features=1548, bias=True)\n",
+      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (2): Linear(in_features=1548, out_features=1420, bias=True)\n",
+      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (4): Linear(in_features=1420, out_features=1292, bias=True)\n",
+      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (6): Linear(in_features=1292, out_features=1164, bias=True)\n",
+      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (8): Linear(in_features=1164, out_features=1036, bias=True)\n",
+      "    (9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (10): Linear(in_features=1036, out_features=908, bias=True)\n",
+      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (12): Linear(in_features=908, out_features=780, bias=True)\n",
+      "    (13): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (14): Linear(in_features=780, out_features=652, bias=True)\n",
+      "    (15): LeakyReLU(negative_slope=0.01, inplace=True)\n",
+      "    (16): Linear(in_features=652, out_features=512, bias=True)\n",
+      "  )\n",
+      ")\n"
+     ]
+    }
+   ],
+   "source": [
+    "best_model = f.Encoder().to(device)\n",
+    "best_model.load_state_dict(torch.load(encoder_path))\n",
+    "encoder_criterion = nn.MSELoss()\n",
+    "print(best_model)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "encoder_path = '../models/best_carl_to_chemnet_encoder.pth'\n",
+    "torch.save(best_model.state_dict(), encoder_path)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Generating Carl Embedding Predictions:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 64,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_dataset = DataLoader(\n",
+    "    TensorDataset(\n",
+    "        train_carl_tensor, \n",
+    "        train_chem_encodings_tensor, \n",
+    "        train_embeddings_tensor,\n",
+    "        train_carl_indices_tensor\n",
+    "        ), \n",
+    "        batch_size=best_hyperparams['batch_size'], \n",
+    "        shuffle=False\n",
+    "        )\n",
+    "predicted_embeddings, output_name_encodings, average_loss, input_carl_indices = f.predict_embeddings(train_dataset, best_model, device, encoder_criterion)\n",
+    "input_carl_indices = [idx.cpu().detach().numpy() for idx_list in input_carl_indices for idx in idx_list]\n",
+    "predicted_embeddings = [emb.cpu().detach().numpy() for emb_list in predicted_embeddings for emb in emb_list]\n",
+    "output_name_encodings = [enc.cpu().detach().numpy() for enc_list in output_name_encodings for enc in enc_list]\n",
+    "train_preds_df = pd.DataFrame(predicted_embeddings)\n",
+    "train_preds_df.insert(0, 'index', input_carl_indices)\n",
+    "name_encodings_df = pd.DataFrame(output_name_encodings)\n",
+    "name_encodings_df.columns = train_carls.columns[-8:]\n",
+    "train_preds_df = pd.concat([train_preds_df, name_encodings_df], axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "val_dataset = DataLoader(\n",
+    "    TensorDataset(\n",
+    "        val_carl_tensor, \n",
+    "        val_chem_encodings_tensor, \n",
+    "        val_embeddings_tensor,\n",
+    "        val_carl_indices_tensor\n",
+    "        ), \n",
+    "        batch_size=best_hyperparams['batch_size'], \n",
+    "        shuffle=False\n",
+    "        )\n",
+    "predicted_embeddings, output_name_encodings, average_loss, input_carl_indices = f.predict_embeddings(val_dataset, best_model, device, encoder_criterion)\n",
+    "input_carl_indices = [idx.cpu().detach().numpy() for idx_list in input_carl_indices for idx in idx_list]\n",
+    "predicted_embeddings = [emb.cpu().detach().numpy() for emb_list in predicted_embeddings for emb in emb_list]\n",
+    "output_name_encodings = [enc.cpu().detach().numpy() for enc_list in output_name_encodings for enc in enc_list]\n",
+    "val_preds_df = pd.DataFrame(predicted_embeddings)\n",
+    "val_preds_df.insert(0, 'index', input_carl_indices)\n",
+    "name_encodings_df = pd.DataFrame(output_name_encodings)\n",
+    "name_encodings_df.columns = val_carls.columns[-8:]\n",
+    "val_preds_df = pd.concat([val_preds_df, name_encodings_df], axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 65,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "test_dataset = DataLoader(\n",
+    "    TensorDataset(\n",
+    "        test_carl_tensor, \n",
+    "        test_chem_encodings_tensor, \n",
+    "        test_embeddings_tensor,\n",
+    "        test_carl_indices_tensor\n",
+    "        ), \n",
+    "        batch_size=best_hyperparams['batch_size'], \n",
+    "        shuffle=False\n",
+    "        )\n",
+    "predicted_embeddings, output_name_encodings, average_loss, input_carl_indices = f.predict_embeddings(test_dataset, best_model, device, encoder_criterion)\n",
+    "input_carl_indices = [idx.cpu().detach().numpy() for idx_list in input_carl_indices for idx in idx_list]\n",
+    "predicted_embeddings = [emb.cpu().detach().numpy() for emb_list in predicted_embeddings for emb in emb_list]\n",
+    "output_name_encodings = [enc.cpu().detach().numpy() for enc_list in output_name_encodings for enc in enc_list]\n",
+    "test_preds_df = pd.DataFrame(predicted_embeddings)\n",
+    "test_preds_df.insert(0, 'index', input_carl_indices)\n",
+    "name_encodings_df = pd.DataFrame(output_name_encodings)\n",
+    "name_encodings_df.columns = test_carls.columns[-8:]\n",
+    "test_preds_df = pd.concat([test_preds_df, name_encodings_df], axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 66,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "file_path = '/mnt/usb/cmdunham/carl_encoder_train_embeddings.csv'\n",
+    "train_preds_df.to_csv(file_path, index=False)\n",
+    "file_path = '/mnt/usb/cmdunham/carl_encoder_val_embeddings.csv'\n",
+    "val_preds_df.to_csv(file_path, index=False)\n",
+    "file_path = '/mnt/usb/cmdunham/carl_encoder_test_embeddings.csv'\n",
+    "test_preds_df.to_csv(file_path, index=False)"
+   ]
+  },
   {
    "cell_type": "markdown",
    "metadata": {},
diff --git a/models/carl_to_chemnet_encoder..pth b/models/carl_to_chemnet_encoder..pth
index fc445505..ee7ece26 100644
Binary files a/models/carl_to_chemnet_encoder..pth and b/models/carl_to_chemnet_encoder..pth differ
diff --git a/models/functions.py b/models/functions.py
index f6905237..3fef88e3 100644
--- a/models/functions.py
+++ b/models/functions.py
@@ -9,6 +9,11 @@ from torch.utils.data import DataLoader
 import wandb
 from sklearn.decomposition import PCA
 import itertools
+import GPUtil
+
+import dask.dataframe as dd
+import pandas as pd
+import torch
 
 def flatten_and_bin(predicted_embeddings_batches):
     """
@@ -812,8 +817,11 @@ def create_dataset_tensors(spectra_dataset, embedding_df, device, carl=False):
     # drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra
     if carl: # carl dataset has no 'Unnamed: 0' column
         spectra = spectra_dataset.iloc[:,1:-9]
+        # embeddings_tensor = torch.Tensor([embedding_df['Embedding Floats'][chem_name] for chem_name in chem_labels]).to(device)
     else:
         spectra = spectra_dataset.iloc[:,2:-9]
+        # embeddings_tensor = torch.Tensor([embedding_df['Embedding Floats'][chem_name] for chem_name in chem_labels]).to(device)
+        
     chem_encodings = spectra_dataset.iloc[:,-8:]
 
     # create tensors of spectra, true embeddings, and chemical name encodings for train and val
@@ -824,3 +832,179 @@ def create_dataset_tensors(spectra_dataset, embedding_df, device, carl=False):
     spectra_indices_tensor = torch.Tensor(spectra_dataset['index'].to_numpy()).to(device)
 
     return embeddings_tensor, spectra_tensor, chem_encodings_tensor, spectra_indices_tensor
+
+def create_dataset_tensors_with_dask(spectra_file, embedding_df, device, carl=False):
+    """
+    Create tensors from the provided spectra dataset and embedding DataFrame using Dask.
+
+    Parameters:
+    ----------
+    spectra_file : str
+        Path to the CSV file containing spectral data and chemical labels.
+
+    embedding_df : pd.DataFrame
+        DataFrame containing embeddings for chemicals.
+
+    device : torch.device
+        The device (CPU or GPU) on which to store the tensors.
+
+    carl : bool, optional
+        If True, processes the dataset assuming it has a different structure.
+
+    Returns:
+    -------
+    tuple
+        A tuple containing:
+        - embeddings_tensor (torch.Tensor)
+        - spectra_tensor (torch.Tensor)
+        - chem_encodings_tensor (torch.Tensor)
+        - spectra_indices_tensor (torch.Tensor)
+    """
+    # Load the dataset as a Dask DataFrame
+    spectra_dd = dd.read_csv(spectra_file)
+
+    # Compute the necessary tensors
+    if carl:
+        spectra = spectra_dd.iloc[:, 1:-9]
+    else:
+        spectra = spectra_dd.iloc[:, 2:-9]
+
+    chem_labels = spectra_dd['Label'].compute().tolist()
+    embeddings = [embedding_df['Embedding Floats'][chem_name] for chem_name in chem_labels]
+
+    # Create tensors directly from Dask DataFrame
+    spectra_tensor = torch.tensor(spectra.compute().values, dtype=torch.float32).to(device)
+    chem_encodings = spectra_dd.iloc[:, -8:].compute()
+    chem_encodings_tensor = torch.tensor(chem_encodings.values, dtype=torch.float32).to(device)
+    spectra_indices_tensor = torch.tensor(spectra_dd['index'].compute().values, dtype=torch.float32).to(device)
+
+    # Convert embeddings to tensor
+    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)
+
+    return embeddings_tensor, spectra_tensor, chem_encodings_tensor, spectra_indices_tensor
+
+def create_dataset_tensors_from_chunks(spectra_dataset, embedding_df, device, chunk_size=None, carl=False):
+    """
+    Create tensors from the provided spectra dataset and embedding DataFrame.
+
+    Parameters:
+    ----------
+    spectra_dataset : pd.DataFrame
+        DataFrame containing spectral data and chemical labels. Assumes specific 
+        columns for processing based on the `carl` flag.
+
+    embedding_df : pd.DataFrame
+        DataFrame containing embeddings for chemicals, with 'Embedding Floats' 
+        column corresponding to chemical names.
+
+    device : torch.device
+        The device (CPU or GPU) on which to store the tensors.
+
+    carl : bool, optional
+        If True, processes the dataset assuming it has a different structure 
+        (specifically without an 'Unnamed: 0' column). Default is False.
+
+    Returns:
+    -------
+    tuple
+        A tuple containing:
+        - embeddings_tensor (torch.Tensor): Tensor of true embeddings for the chemicals.
+        - spectra_tensor (torch.Tensor): Tensor of spectral data.
+        - chem_encodings_tensor (torch.Tensor): Tensor of chemical name encodings.
+        - spectra_indices_tensor (torch.Tensor): Tensor of indices corresponding to the spectra.
+    """
+    embeddings_list = []
+    spectra_list = []
+    chem_encodings_list = []
+    indices_list = []
+
+    # Process the dataset in chunks
+    for chunk in pd.read_csv(spectra_dataset, chunksize=chunk_size):
+        if carl:
+            spectra = chunk.iloc[:, 1:-9]
+            chem_labels = list(chunk['Label'])
+            embeddings = [embedding_df['Embedding Floats'][chem_name] for chem_name in chem_labels]
+        else:
+            spectra = chunk.iloc[:, 2:-9]
+            chem_labels = list(chunk['Label'])
+            embeddings = [embedding_df['Embedding Floats'][chem_name] for chem_name in chem_labels]
+
+        # Convert to tensors
+        embeddings_tensor = torch.Tensor(embeddings).to(device)
+        spectra_tensor = torch.Tensor(spectra.values).to(device)
+        chem_encodings = chunk.iloc[:, -8:]
+        chem_encodings_tensor = torch.Tensor(chem_encodings.values).to(device)
+        spectra_indices_tensor = torch.Tensor(chunk['index'].to_numpy()).to(device)
+
+        # Append to lists
+        embeddings_list.append(embeddings_tensor)
+        spectra_list.append(spectra_tensor)
+        chem_encodings_list.append(chem_encodings_tensor)
+        indices_list.append(spectra_indices_tensor)
+
+    # Concatenate all tensors
+    embeddings_tensor = torch.cat(embeddings_list).to(device)
+    spectra_tensor = torch.cat(spectra_list).to(device)
+    chem_encodings_tensor = torch.cat(chem_encodings_list).to(device)
+    spectra_indices_tensor = torch.cat(indices_list).to(device)
+
+    return embeddings_tensor, spectra_tensor, chem_encodings_tensor, spectra_indices_tensor
+
+
+class Generator(nn.Module):
+  def __init__(self):
+    super().__init__()
+    self.encoder = nn.Sequential(
+      nn.Linear(512,652),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(652,780),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(780, 908),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(908, 1036),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(1036, 1164),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(1164, 1292),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(1292, 1420),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(1420, 1548),
+      nn.LeakyReLU(inplace=True),
+      nn.Linear(1548, 1676),
+    )
+
+  def forward(self, x):
+    x = self.encoder(x)
+    return x
+  
+def set_up_gpu():
+    if torch.cuda.is_available():
+        # Get the list of GPUs
+        gpus = GPUtil.getGPUs()
+
+        # Find the GPU with the most free memory
+        best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)
+
+        # Print details about the selected GPU
+        print(f"Selected GPU ID: {best_gpu.id}")
+        print(f"  Name: {best_gpu.name}")
+        print(f"  Memory Free: {best_gpu.memoryFree} MB")
+        print(f"  Memory Used: {best_gpu.memoryUsed} MB")
+        print(f"  GPU Load: {best_gpu.load * 100:.2f}%")
+
+        # Set the device for later use
+        device = torch.device(f'cuda:{best_gpu.id}')
+        print('Current device ID: ', device)
+
+        # Set the current device in PyTorch
+        torch.cuda.set_device(best_gpu.id)
+    else:
+        device = torch.device('cpu')
+        print('Using CPU')
+
+    # Confirm the currently selected device in PyTorch
+    print("PyTorch current device ID:", torch.cuda.current_device())
+    print("PyTorch current device name:", torch.cuda.get_device_name(torch.cuda.current_device()))
+
+    return device
\ No newline at end of file
diff --git a/models/ims_generator.ipynb b/models/ims_generator.ipynb
index a619aed3..d7355d4b 100644
--- a/models/ims_generator.ipynb
+++ b/models/ims_generator.ipynb
@@ -22,26 +22,615 @@
     "import wandb\n",
     "import os\n",
     "from sklearn.decomposition import PCA\n",
-    "import GPUtil\n",
     "import itertools\n",
     "import io\n",
     "\n",
     "from collections import Counter\n",
     "from sklearn.preprocessing import StandardScaler\n",
-    "from sklearn.decomposition import KernelPCA"
+    "from sklearn.decomposition import KernelPCA\n",
+    "\n",
+    "import importlib\n",
+    "import functions as f"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "<module 'functions' from '/home/cmdunham/ChemicalDataGeneration/models/functions.py'>"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Reload the functions module after updates\n",
+    "importlib.reload(f)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Loading Data:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Loading CARLS and CARL-Based Embedding Preds:\n",
+    "---"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Loading CARLS:\n",
+    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_carls_.csv'\n",
+    "train_carls = pd.read_csv(file_path)\n",
+    "# file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_carls_.csv'\n",
+    "# val_carls = pd.read_csv(file_path)\n",
+    "# file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/test_carls_.csv'\n",
+    "# test_carls = pd.read_csv(file_path)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Loading Encoder embedding predictions\n",
+    "# file_path = '/mnt/usb/cmdunham/carl_encoder_train_embeddings.csv'\n",
+    "# train = pd.read_csv(file_path)\n",
+    "file_path = '/mnt/usb/cmdunham/carl_encoder_val_embeddings.csv'\n",
+    "val = pd.read_csv(file_path)\n",
+    "# file_path = '/mnt/usb/cmdunham/carl_encoder_test_embeddings.csv'\n",
+    "# test = pd.read_csv(file_path)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "<bound method NDFrame.tail of             index         0         1         2         3         4         5  \\\n",
+       "0        655550.0  0.063262  0.009075  0.421479  0.195605 -0.167351  0.370043   \n",
+       "1        655550.0  0.063245  0.009044  0.421597  0.195607 -0.167283  0.369979   \n",
+       "2        384167.0  0.381048  0.000580  0.255326 -0.242713 -0.305131  0.380516   \n",
+       "3        384167.0  0.381021  0.000578  0.255351 -0.242761 -0.305113  0.380425   \n",
+       "4       1141374.0  0.121070  0.002962 -0.144593  0.072692 -0.107195  0.053167   \n",
+       "...           ...       ...       ...       ...       ...       ...       ...   \n",
+       "296687        NaN  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
+       "296688        NaN  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
+       "296689        NaN  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
+       "296690        NaN  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
+       "296691        NaN  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
+       "\n",
+       "               6         7         8  ...  6.2  7.2  DEB  DEM  DMMP  DPM  \\\n",
+       "0      -0.151087 -0.137699 -0.159923  ...  0.0  0.0  1.0  0.0   0.0  0.0   \n",
+       "1      -0.151087 -0.137658 -0.159873  ...  0.0  0.0  1.0  0.0   0.0  0.0   \n",
+       "2      -0.060894 -0.209100 -0.421298  ...  0.0  0.0  0.0  1.0   0.0  0.0   \n",
+       "3      -0.060873 -0.209114 -0.421232  ...  0.0  0.0  0.0  1.0   0.0  0.0   \n",
+       "4      -0.027065 -0.212120 -0.133358  ...  0.0  0.0  0.0  0.0   1.0  0.0   \n",
+       "...          ...       ...       ...  ...  ...  ...  ...  ...   ...  ...   \n",
+       "296687  1.000000  0.000000       NaN  ...  1.0  0.0  0.0  0.0   0.0  0.0   \n",
+       "296688  0.000000  0.000000       NaN  ...  0.0  0.0  0.0  0.0   0.0  1.0   \n",
+       "296689  0.000000  0.000000       NaN  ...  0.0  0.0  0.0  0.0   0.0  1.0   \n",
+       "296690  0.000000  1.000000       NaN  ...  0.0  1.0  0.0  0.0   0.0  0.0   \n",
+       "296691  0.000000  1.000000       NaN  ...  0.0  1.0  0.0  0.0   0.0  0.0   \n",
+       "\n",
+       "        DtBP  JP8  MES  TEPO  \n",
+       "0        0.0  0.0  0.0   0.0  \n",
+       "1        0.0  0.0  0.0   0.0  \n",
+       "2        0.0  0.0  0.0   0.0  \n",
+       "3        0.0  0.0  0.0   0.0  \n",
+       "4        0.0  0.0  0.0   0.0  \n",
+       "...      ...  ...  ...   ...  \n",
+       "296687   0.0  0.0  1.0   0.0  \n",
+       "296688   0.0  0.0  0.0   0.0  \n",
+       "296689   0.0  0.0  0.0   0.0  \n",
+       "296690   0.0  0.0  0.0   1.0  \n",
+       "296691   0.0  0.0  0.0   1.0  \n",
+       "\n",
+       "[296692 rows x 537 columns]>"
+      ]
+     },
+     "execution_count": 7,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>index</th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>...</th>\n",
+       "      <th>6.2</th>\n",
+       "      <th>7.2</th>\n",
+       "      <th>DEB</th>\n",
+       "      <th>DEM</th>\n",
+       "      <th>DMMP</th>\n",
+       "      <th>DPM</th>\n",
+       "      <th>DtBP</th>\n",
+       "      <th>JP8</th>\n",
+       "      <th>MES</th>\n",
+       "      <th>TEPO</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>0</th>\n",
+       "      <td>655550.0</td>\n",
+       "      <td>0.063262</td>\n",
+       "      <td>0.009075</td>\n",
+       "      <td>0.421479</td>\n",
+       "      <td>0.195605</td>\n",
+       "      <td>-0.167351</td>\n",
+       "      <td>0.370043</td>\n",
+       "      <td>-0.151087</td>\n",
+       "      <td>-0.137699</td>\n",
+       "      <td>-0.159923</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>1</th>\n",
+       "      <td>655550.0</td>\n",
+       "      <td>0.063245</td>\n",
+       "      <td>0.009044</td>\n",
+       "      <td>0.421597</td>\n",
+       "      <td>0.195607</td>\n",
+       "      <td>-0.167283</td>\n",
+       "      <td>0.369979</td>\n",
+       "      <td>-0.151087</td>\n",
+       "      <td>-0.137658</td>\n",
+       "      <td>-0.159873</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>384167.0</td>\n",
+       "      <td>0.381048</td>\n",
+       "      <td>0.000580</td>\n",
+       "      <td>0.255326</td>\n",
+       "      <td>-0.242713</td>\n",
+       "      <td>-0.305131</td>\n",
+       "      <td>0.380516</td>\n",
+       "      <td>-0.060894</td>\n",
+       "      <td>-0.209100</td>\n",
+       "      <td>-0.421298</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>384167.0</td>\n",
+       "      <td>0.381021</td>\n",
+       "      <td>0.000578</td>\n",
+       "      <td>0.255351</td>\n",
+       "      <td>-0.242761</td>\n",
+       "      <td>-0.305113</td>\n",
+       "      <td>0.380425</td>\n",
+       "      <td>-0.060873</td>\n",
+       "      <td>-0.209114</td>\n",
+       "      <td>-0.421232</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>1141374.0</td>\n",
+       "      <td>0.121070</td>\n",
+       "      <td>0.002962</td>\n",
+       "      <td>-0.144593</td>\n",
+       "      <td>0.072692</td>\n",
+       "      <td>-0.107195</td>\n",
+       "      <td>0.053167</td>\n",
+       "      <td>-0.027065</td>\n",
+       "      <td>-0.212120</td>\n",
+       "      <td>-0.133358</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>5 rows × 537 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "       index         0         1         2         3         4         5  \\\n",
+       "0   655550.0  0.063262  0.009075  0.421479  0.195605 -0.167351  0.370043   \n",
+       "1   655550.0  0.063245  0.009044  0.421597  0.195607 -0.167283  0.369979   \n",
+       "2   384167.0  0.381048  0.000580  0.255326 -0.242713 -0.305131  0.380516   \n",
+       "3   384167.0  0.381021  0.000578  0.255351 -0.242761 -0.305113  0.380425   \n",
+       "4  1141374.0  0.121070  0.002962 -0.144593  0.072692 -0.107195  0.053167   \n",
+       "\n",
+       "          6         7         8  ...  6.2  7.2  DEB  DEM  DMMP  DPM  DtBP  \\\n",
+       "0 -0.151087 -0.137699 -0.159923  ...  0.0  0.0  1.0  0.0   0.0  0.0   0.0   \n",
+       "1 -0.151087 -0.137658 -0.159873  ...  0.0  0.0  1.0  0.0   0.0  0.0   0.0   \n",
+       "2 -0.060894 -0.209100 -0.421298  ...  0.0  0.0  0.0  1.0   0.0  0.0   0.0   \n",
+       "3 -0.060873 -0.209114 -0.421232  ...  0.0  0.0  0.0  1.0   0.0  0.0   0.0   \n",
+       "4 -0.027065 -0.212120 -0.133358  ...  0.0  0.0  0.0  0.0   1.0  0.0   0.0   \n",
+       "\n",
+       "   JP8  MES  TEPO  \n",
+       "0  0.0  0.0   0.0  \n",
+       "1  0.0  0.0   0.0  \n",
+       "2  0.0  0.0   0.0  \n",
+       "3  0.0  0.0   0.0  \n",
+       "4  0.0  0.0   0.0  \n",
+       "\n",
+       "[5 rows x 537 columns]"
+      ]
+     },
+     "execution_count": 9,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "val.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "148346"
+      ]
+     },
+     "execution_count": 38,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     ]
+    }
+   ],
+   "source": [
+    "len(set(list(val['index'])))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>index</th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>...</th>\n",
+       "      <th>511</th>\n",
+       "      <th>DEB</th>\n",
+       "      <th>DEM</th>\n",
+       "      <th>DMMP</th>\n",
+       "      <th>DPM</th>\n",
+       "      <th>DtBP</th>\n",
+       "      <th>JP8</th>\n",
+       "      <th>MES</th>\n",
+       "      <th>TEPO</th>\n",
+       "      <th>bkg_idx</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>0</th>\n",
+       "      <td>1333416.0</td>\n",
+       "      <td>0.193031</td>\n",
+       "      <td>0.000955</td>\n",
+       "      <td>0.066271</td>\n",
+       "      <td>-0.195277</td>\n",
+       "      <td>-0.272022</td>\n",
+       "      <td>0.076430</td>\n",
+       "      <td>-0.109237</td>\n",
+       "      <td>-0.088802</td>\n",
+       "      <td>-0.423475</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.294681</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>1</th>\n",
+       "      <td>1333416.0</td>\n",
+       "      <td>0.193044</td>\n",
+       "      <td>0.000954</td>\n",
+       "      <td>0.066276</td>\n",
+       "      <td>-0.195231</td>\n",
+       "      <td>-0.272047</td>\n",
+       "      <td>0.076444</td>\n",
+       "      <td>-0.109254</td>\n",
+       "      <td>-0.088775</td>\n",
+       "      <td>-0.423473</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.294751</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>1226215.0</td>\n",
+       "      <td>0.193062</td>\n",
+       "      <td>0.000981</td>\n",
+       "      <td>0.066309</td>\n",
+       "      <td>-0.195170</td>\n",
+       "      <td>-0.272027</td>\n",
+       "      <td>0.076440</td>\n",
+       "      <td>-0.109279</td>\n",
+       "      <td>-0.088847</td>\n",
+       "      <td>-0.423466</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.294772</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>1226215.0</td>\n",
+       "      <td>0.193020</td>\n",
+       "      <td>0.000946</td>\n",
+       "      <td>0.066276</td>\n",
+       "      <td>-0.195235</td>\n",
+       "      <td>-0.272053</td>\n",
+       "      <td>0.076501</td>\n",
+       "      <td>-0.109234</td>\n",
+       "      <td>-0.088805</td>\n",
+       "      <td>-0.423484</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.294796</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>1430326.0</td>\n",
+       "      <td>0.193067</td>\n",
+       "      <td>0.000959</td>\n",
+       "      <td>0.066269</td>\n",
+       "      <td>-0.195242</td>\n",
+       "      <td>-0.272052</td>\n",
+       "      <td>0.076480</td>\n",
+       "      <td>-0.109274</td>\n",
+       "      <td>-0.088808</td>\n",
+       "      <td>-0.423508</td>\n",
+       "      <td>...</td>\n",
+       "      <td>-0.294735</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>5 rows × 522 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "       index         0         1         2         3         4         5  \\\n",
+       "0  1333416.0  0.193031  0.000955  0.066271 -0.195277 -0.272022  0.076430   \n",
+       "1  1333416.0  0.193044  0.000954  0.066276 -0.195231 -0.272047  0.076444   \n",
+       "2  1226215.0  0.193062  0.000981  0.066309 -0.195170 -0.272027  0.076440   \n",
+       "3  1226215.0  0.193020  0.000946  0.066276 -0.195235 -0.272053  0.076501   \n",
+       "4  1430326.0  0.193067  0.000959  0.066269 -0.195242 -0.272052  0.076480   \n",
+       "\n",
+       "          6         7         8  ...       511  DEB  DEM  DMMP  DPM  DtBP  \\\n",
+       "0 -0.109237 -0.088802 -0.423475  ... -0.294681  0.0  0.0   0.0  0.0   0.0   \n",
+       "1 -0.109254 -0.088775 -0.423473  ... -0.294751  0.0  0.0   0.0  0.0   0.0   \n",
+       "2 -0.109279 -0.088847 -0.423466  ... -0.294772  0.0  0.0   0.0  0.0   0.0   \n",
+       "3 -0.109234 -0.088805 -0.423484  ... -0.294796  0.0  0.0   0.0  0.0   0.0   \n",
+       "4 -0.109274 -0.088808 -0.423508  ... -0.294735  0.0  0.0   0.0  0.0   0.0   \n",
+       "\n",
+       "   JP8  MES  TEPO  bkg_idx  \n",
+       "0  0.0  0.0   1.0        1  \n",
+       "1  0.0  0.0   1.0        2  \n",
+       "2  0.0  0.0   1.0        1  \n",
+       "3  0.0  0.0   1.0        2  \n",
+       "4  0.0  0.0   1.0        1  \n",
+       "\n",
+       "[5 rows x 522 columns]"
+      ]
+     },
+     "execution_count": 24,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# currently carls do not have labels to distinguish which were created with bkg spec 1 vs 2. Need to add those earlier but this works for now.\n",
+    "train['bkg_idx'] = [1, 2] * int(len(train)/2)\n",
+    "val['bkg_idx'] = [1, 2] * int(len(val)/2)\n",
+    "test['bkg_idx'] = [1, 2] * int(len(test)/2)\n",
+    "train_carls['bkg_idx'] = [1, 2] * int(len(train_carls)/2)\n",
+    "val_carls['bkg_idx'] = [1, 2] * int(len(val_carls)/2)\n",
+    "test_carls['bkg_idx'] = [1, 2] * int(len(test_carls)/2)\n",
+    "train.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Unused Code:\n",
+    "---"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Data Loading and Setup:\n",
+    "## Data Loading and Setup:\n",
     "---"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -59,7 +648,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 21,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -250,9 +839,8 @@
        "[5 rows x 1687 columns]"
       ]
      },
-     "execution_count": 21,
      "metadata": {},
-     "output_type": "execute_result"
+     "output_type": "display_data"
     }
    ],
    "source": [
@@ -261,7 +849,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 25,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -273,64 +861,6 @@
     "test_embs = pd.read_csv(file_path)"
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## GPU Setup:\n",
-    "---"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Selected GPU ID: 0\n",
-      "  Name: NVIDIA GeForce RTX 4090\n",
-      "  Memory Free: 24185.0 MB\n",
-      "  Memory Used: 31.0 MB\n",
-      "  GPU Load: 0.00%\n",
-      "Current device ID:  cuda:0\n",
-      "PyTorch current device ID: 0\n",
-      "PyTorch current device name: NVIDIA GeForce RTX 4090\n"
-     ]
-    }
-   ],
-   "source": [
-    "if torch.cuda.is_available():\n",
-    "    # Get the list of GPUs\n",
-    "    gpus = GPUtil.getGPUs()\n",
-    "\n",
-    "    # Find the GPU with the most free memory\n",
-    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
-    "\n",
-    "    # Print details about the selected GPU\n",
-    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
-    "    print(f\"  Name: {best_gpu.name}\")\n",
-    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
-    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
-    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
-    "\n",
-    "    # Set the device for later use\n",
-    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
-    "    print('Current device ID: ', device)\n",
-    "\n",
-    "    # Set the current device in PyTorch\n",
-    "    torch.cuda.set_device(best_gpu.id)\n",
-    "else:\n",
-    "    device = torch.device('cpu')\n",
-    "    print('Using CPU')\n",
-    "\n",
-    "# Confirm the currently selected device in PyTorch\n",
-    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
-    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
@@ -814,34 +1344,34 @@
     }
    ],
    "source": [
-    "# drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
-    "train_spectra = train.iloc[:,2:-9]\n",
-    "train_chem_encodings = train.iloc[:,-8:]\n",
-    "\n",
-    "val_spectra = val.iloc[:,2:-9]\n",
-    "val_chem_encodings = val.iloc[:,-8:]\n",
-    "\n",
-    "test_spectra = test.iloc[:,2:-9]\n",
-    "test_chem_encodings = test.iloc[:,-8:]\n",
-    "\n",
-    "# create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
-    "train_chem_labels = list(train['Label'])\n",
-    "train_embeddings_tensor = torch.Tensor([train_embs['index'][idx] for idx in list(train['index'])]).to(device)\n",
-    "# train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
-    "train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
-    "train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
-    "\n",
-    "val_chem_labels = list(val['Label'])\n",
-    "val_embeddings_tensor = torch.Tensor([val_embs['index'][idx] for idx in list(val['index'])]).to(device)\n",
-    "val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
-    "val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
-    "val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)\n",
-    "\n",
-    "test_chem_labels = list(test['Label'])\n",
-    "test_embeddings_tensor = torch.Tensor([train_embs['index'][idx] for idx in list(test['index'])]).to(device)\n",
-    "test_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in test_chem_labels]).to(device)\n",
-    "test_spectra_tensor = torch.Tensor(test_spectra.values).to(device)\n",
-    "test_chem_encodings_tensor = torch.Tensor(test_chem_encodings.values).to(device)"
+    "# # drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
+    "# train_spectra = train.iloc[:,2:-9]\n",
+    "# train_chem_encodings = train.iloc[:,-8:]\n",
+    "\n",
+    "# val_spectra = val.iloc[:,2:-9]\n",
+    "# val_chem_encodings = val.iloc[:,-8:]\n",
+    "\n",
+    "# test_spectra = test.iloc[:,2:-9]\n",
+    "# test_chem_encodings = test.iloc[:,-8:]\n",
+    "\n",
+    "# # create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
+    "# train_chem_labels = list(train['Label'])\n",
+    "# train_embeddings_tensor = torch.Tensor([train_embs['index'][idx] for idx in list(train['index'])]).to(device)\n",
+    "# # train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
+    "# train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
+    "# train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
+    "\n",
+    "# val_chem_labels = list(val['Label'])\n",
+    "# val_embeddings_tensor = torch.Tensor([val_embs['index'][idx] for idx in list(val['index'])]).to(device)\n",
+    "# val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
+    "# val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
+    "# val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)\n",
+    "\n",
+    "# test_chem_labels = list(test['Label'])\n",
+    "# test_embeddings_tensor = torch.Tensor([train_embs['index'][idx] for idx in list(test['index'])]).to(device)\n",
+    "# test_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in test_chem_labels]).to(device)\n",
+    "# test_spectra_tensor = torch.Tensor(test_spectra.values).to(device)\n",
+    "# test_chem_encodings_tensor = torch.Tensor(test_chem_encodings.values).to(device)"
    ]
   },
   {
@@ -906,7 +1436,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -921,52 +1451,18 @@
     }
    ],
    "source": [
-    "train_chem_labels = len(train_spectra) * ['DMMP']\n",
-    "plot_spectra_pca(train_spectra, train_chem_labels)"
+    "# train_chem_labels = len(train_spectra) * ['DMMP']\n",
+    "# plot_spectra_pca(train_spectra, train_chem_labels)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Definitions:\n",
+    "## Definitions:\n",
     "---"
    ]
   },
-  {
-   "cell_type": "code",
-   "execution_count": 12,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "class Generator(nn.Module):\n",
-    "  def __init__(self):\n",
-    "    super().__init__()\n",
-    "    self.encoder = nn.Sequential(\n",
-    "      nn.Linear(512,652),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(652,780),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(780, 908),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(908, 1036),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(1036, 1164),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(1164, 1292),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(1292, 1420),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(1420, 1548),\n",
-    "      nn.LeakyReLU(inplace=True),\n",
-    "      nn.Linear(1548, 1676),\n",
-    "    )\n",
-    "\n",
-    "  def forward(self, x):\n",
-    "    x = self.encoder(x)\n",
-    "    return x"
-   ]
-  },
   {
    "cell_type": "code",
    "execution_count": 13,
@@ -1238,7 +1734,7 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Running the Generator:\n",
+    "## Running the Generator:\n",
     "---"
    ]
   },
@@ -1256,124 +1752,124 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "# set var deciding if results plot for this run is saved to wandb\n",
-    "log_wandb = True\n",
-    "# # simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
-    "# plotting_chem = 'TEPO'\n",
-    "\n",
-    "# Last 8 cols of the df are the chem names\n",
-    "sorted_chem_names = list(train.columns[-8:])\n",
-    "\n",
-    "# model_config = {\n",
-    "#   'batch_size':[128, 256],\n",
-    "#   'epochs': [100],\n",
-    "#   'learning_rate':[.01, .001]\n",
-    "  # }\n",
-    "\n",
-    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
-    "lowest_loss = 10000000\n",
-    "\n",
-    "model_config = {\n",
-    "    'batch_size': [128],\n",
-    "    'epochs': [100],\n",
-    "    'learning_rate': [.001]\n",
-    "}\n",
-    "\n",
-    "keys = model_config.keys()\n",
-    "values = model_config.values()\n",
-    "\n",
-    "# Generate all parameter combinations from model_config using itertools.product\n",
-    "combinations = itertools.product(*values)\n",
-    "\n",
-    "# Iterate through each parameter combination and run model \n",
-    "for combo in combinations:\n",
-    "  combo = dict(zip(keys, combo))\n",
-    "\n",
-    "  train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
-    "  val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
-    "  generator = Generator().to(device)\n",
-    "\n",
-    "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
-    "  generator_criterion = nn.MSELoss()\n",
-    "\n",
-    "  wandb_kwargs = {\n",
-    "    'learning_rate': combo['learning_rate'],\n",
-    "    'epochs': combo['epochs'],\n",
-    "    'batch_size': combo['batch_size'],\n",
-    "    'model_architecture': 'generator',\n",
-    "    'optimizer':'AdamW',\n",
-    "    'loss': 'MSELoss'\n",
-    "  }\n",
-    "\n",
-    "  run_with_wandb(config, **wandb_kwargs)\n",
-    "\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "  print('New run with hyperparameters:')\n",
-    "  for key in combo:\n",
-    "    print(key, ' : ', combo[key])\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "\n",
-    "  for epoch in range(combo['epochs']):\n",
-    "    # Set model to training mode\n",
-    "    generator.train(True)\n",
-    "\n",
-    "    # do a pass over the data\n",
-    "    # at last epoch get predicted embeddings and chem names\n",
-    "    if (epoch + 1) == combo['epochs']:\n",
-    "      average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "    else:\n",
-    "      average_loss = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "\n",
-    "    epoch_val_loss = 0  \n",
-    "    # evaluate model on validation data\n",
-    "    generator.eval() # Set model to evaluation mode\n",
-    "    with torch.no_grad():\n",
-    "      for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
-    "        val_true_spectra = val_true_spectra.to(device)\n",
-    "        val_name_encodings = val_name_encodings.to(device)\n",
-    "        val_true_embeddings = val_true_embeddings.to(device)\n",
-    "\n",
-    "        val_batch_predicted_spectra = generator(val_true_embeddings)\n",
-    "\n",
-    "        val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
-    "        # accumulate epoch validation loss\n",
-    "        epoch_val_loss += val_loss.item()\n",
+    "# # set var deciding if results plot for this run is saved to wandb\n",
+    "# log_wandb = True\n",
+    "# # # simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
+    "# # plotting_chem = 'TEPO'\n",
     "\n",
-    "    # divide by number of batches to calculate average loss\n",
-    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "# # Last 8 cols of the df are the chem names\n",
+    "# sorted_chem_names = list(train.columns[-8:])\n",
     "\n",
-    "    # log losses to wandb\n",
-    "    wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
+    "# # model_config = {\n",
+    "# #   'batch_size':[128, 256],\n",
+    "# #   'epochs': [100],\n",
+    "# #   'learning_rate':[.01, .001]\n",
+    "#   # }\n",
     "\n",
-    "    if (epoch + 1) % 10 == 0:\n",
-    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
-    "      print(f'   Training loss: {average_loss}')\n",
-    "      print(f'   Validation loss: {val_average_loss}')\n",
-    "      print('-------------------------------------------')\n",
+    "# # loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
+    "# lowest_loss = 10000000\n",
     "\n",
-    "  # saving comparison plots for first 5 spectra\n",
-    "  plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
-    "\n",
-    "  if average_loss < lowest_loss:\n",
-    "    lowest_loss = average_loss\n",
-    "    best_hyperparams = combo\n",
-    "\n",
-    "  wandb.finish()\n",
-    "\n",
-    "print('Hyperparameters for best model: ')\n",
-    "for key in best_hyperparams:\n",
-    "  print('   ', key, ' : ', best_hyperparams[key])"
+    "# model_config = {\n",
+    "#     'batch_size': [128],\n",
+    "#     'epochs': [100],\n",
+    "#     'learning_rate': [.001]\n",
+    "# }\n",
+    "\n",
+    "# keys = model_config.keys()\n",
+    "# values = model_config.values()\n",
+    "\n",
+    "# # Generate all parameter combinations from model_config using itertools.product\n",
+    "# combinations = itertools.product(*values)\n",
+    "\n",
+    "# # Iterate through each parameter combination and run model \n",
+    "# for combo in combinations:\n",
+    "#   combo = dict(zip(keys, combo))\n",
+    "\n",
+    "#   train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
+    "#   val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
+    "#   generator = Generator().to(device)\n",
+    "\n",
+    "#   generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
+    "#   generator_criterion = nn.MSELoss()\n",
+    "\n",
+    "#   wandb_kwargs = {\n",
+    "#     'learning_rate': combo['learning_rate'],\n",
+    "#     'epochs': combo['epochs'],\n",
+    "#     'batch_size': combo['batch_size'],\n",
+    "#     'model_architecture': 'generator',\n",
+    "#     'optimizer':'AdamW',\n",
+    "#     'loss': 'MSELoss'\n",
+    "#   }\n",
+    "\n",
+    "#   run_with_wandb(config, **wandb_kwargs)\n",
+    "\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "#   print('New run with hyperparameters:')\n",
+    "#   for key in combo:\n",
+    "#     print(key, ' : ', combo[key])\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "\n",
+    "#   for epoch in range(combo['epochs']):\n",
+    "#     # Set model to training mode\n",
+    "#     generator.train(True)\n",
+    "\n",
+    "#     # do a pass over the data\n",
+    "#     # at last epoch get predicted embeddings and chem names\n",
+    "#     if (epoch + 1) == combo['epochs']:\n",
+    "#       average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "#     else:\n",
+    "#       average_loss = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "\n",
+    "#     epoch_val_loss = 0  \n",
+    "#     # evaluate model on validation data\n",
+    "#     generator.eval() # Set model to evaluation mode\n",
+    "#     with torch.no_grad():\n",
+    "#       for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
+    "#         val_true_spectra = val_true_spectra.to(device)\n",
+    "#         val_name_encodings = val_name_encodings.to(device)\n",
+    "#         val_true_embeddings = val_true_embeddings.to(device)\n",
+    "\n",
+    "#         val_batch_predicted_spectra = generator(val_true_embeddings)\n",
+    "\n",
+    "#         val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
+    "#         # accumulate epoch validation loss\n",
+    "#         epoch_val_loss += val_loss.item()\n",
+    "\n",
+    "#     # divide by number of batches to calculate average loss\n",
+    "#     val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "\n",
+    "#     # log losses to wandb\n",
+    "#     wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
+    "\n",
+    "#     if (epoch + 1) % 10 == 0:\n",
+    "#       print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
+    "#       print(f'   Training loss: {average_loss}')\n",
+    "#       print(f'   Validation loss: {val_average_loss}')\n",
+    "#       print('-------------------------------------------')\n",
+    "\n",
+    "#   # saving comparison plots for first 5 spectra\n",
+    "#   plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
+    "\n",
+    "#   if average_loss < lowest_loss:\n",
+    "#     lowest_loss = average_loss\n",
+    "#     best_hyperparams = combo\n",
+    "\n",
+    "#   wandb.finish()\n",
+    "\n",
+    "# print('Hyperparameters for best model: ')\n",
+    "# for key in best_hyperparams:\n",
+    "#   print('   ', key, ' : ', best_hyperparams[key])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 55,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -1480,12 +1976,12 @@
     }
    ],
    "source": [
-    "plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem='TEPO', log_wandb=log_wandb, idx=[3,9])"
+    "# plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem='TEPO', log_wandb=log_wandb, idx=[3,9])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 54,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -1592,7 +2088,7 @@
     }
    ],
    "source": [
-    "plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, log_wandb=log_wandb, idx=[3,9])"
+    "# plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, log_wandb=log_wandb, idx=[3,9])"
    ]
   },
   {
@@ -1605,29 +2101,29 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 133,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "# drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
-    "single_chem_train = train[train['Label'] == 'TEPO']\n",
-    "single_chem_train_spectra = single_chem_train.iloc[:,2:-9]\n",
-    "single_chem_train_chem_encodings = single_chem_train.iloc[:,-8:]\n",
-    "\n",
-    "single_chem_val = val[val['Label'] == 'TEPO']\n",
-    "single_chem_val_spectra = single_chem_val.iloc[:,2:-9]\n",
-    "single_chem_val_chem_encodings = single_chem_val.iloc[:,-8:]\n",
-    "\n",
-    "# create tensors of spectra, true embeddings, and chemical name encodings *for single chemical* for train and val\n",
-    "single_chem_train_chem_labels = list(single_chem_train['Label'])\n",
-    "single_chem_train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in single_chem_train_chem_labels]).to(device)\n",
-    "single_chem_train_spectra_tensor = torch.Tensor(single_chem_train_spectra.values).to(device)\n",
-    "single_chem_train_chem_encodings_tensor = torch.Tensor(single_chem_train_chem_encodings.values).to(device)\n",
-    "\n",
-    "single_chem_val_chem_labels = list(single_chem_val['Label'])\n",
-    "single_chem_val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in single_chem_val_chem_labels]).to(device)\n",
-    "single_chem_val_spectra_tensor = torch.Tensor(single_chem_val_spectra.values).to(device)\n",
-    "single_chem_val_chem_encodings_tensor = torch.Tensor(single_chem_val_chem_encodings.values).to(device)"
+    "# # drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
+    "# single_chem_train = train[train['Label'] == 'TEPO']\n",
+    "# single_chem_train_spectra = single_chem_train.iloc[:,2:-9]\n",
+    "# single_chem_train_chem_encodings = single_chem_train.iloc[:,-8:]\n",
+    "\n",
+    "# single_chem_val = val[val['Label'] == 'TEPO']\n",
+    "# single_chem_val_spectra = single_chem_val.iloc[:,2:-9]\n",
+    "# single_chem_val_chem_encodings = single_chem_val.iloc[:,-8:]\n",
+    "\n",
+    "# # create tensors of spectra, true embeddings, and chemical name encodings *for single chemical* for train and val\n",
+    "# single_chem_train_chem_labels = list(single_chem_train['Label'])\n",
+    "# single_chem_train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in single_chem_train_chem_labels]).to(device)\n",
+    "# single_chem_train_spectra_tensor = torch.Tensor(single_chem_train_spectra.values).to(device)\n",
+    "# single_chem_train_chem_encodings_tensor = torch.Tensor(single_chem_train_chem_encodings.values).to(device)\n",
+    "\n",
+    "# single_chem_val_chem_labels = list(single_chem_val['Label'])\n",
+    "# single_chem_val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in single_chem_val_chem_labels]).to(device)\n",
+    "# single_chem_val_spectra_tensor = torch.Tensor(single_chem_val_spectra.values).to(device)\n",
+    "# single_chem_val_chem_encodings_tensor = torch.Tensor(single_chem_val_chem_encodings.values).to(device)"
    ]
   },
   {
@@ -1636,19 +2132,19 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "# Freeze some layers of the generator to maintain some info learned during pre-training\n",
-    "\n",
-    "# set number of layers of pre-trained model to freeze\n",
-    "num_freeze_layers = 3\n",
-    "for i, param in enumerate(generator.parameters()):\n",
-    "    if i < 3:\n",
-    "        # if i % 2 == 0:\n",
-    "        param.requires_grad = False"
+    "# # Freeze some layers of the generator to maintain some info learned during pre-training\n",
+    "\n",
+    "# # set number of layers of pre-trained model to freeze\n",
+    "# num_freeze_layers = 3\n",
+    "# for i, param in enumerate(generator.parameters()):\n",
+    "#     if i < 3:\n",
+    "#         # if i % 2 == 0:\n",
+    "#         param.requires_grad = False"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 145,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -1988,115 +2484,115 @@
     }
    ],
    "source": [
-    "# set var deciding if results plot for this run is saved to wandb\n",
-    "log_wandb = True\n",
-    "# simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
-    "plotting_chem = 'TEPO'\n",
-    "\n",
-    "# Last 8 cols of the df are the chem names\n",
-    "sorted_chem_names = list(train.columns[-8:])\n",
-    "\n",
-    "# model_config = {\n",
-    "#   'batch_size':[128, 256],\n",
-    "#   'epochs': [100],\n",
-    "#   'learning_rate':[.01, .001]\n",
-    "  # }\n",
-    "\n",
-    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
-    "lowest_loss = 10000000\n",
-    "\n",
-    "model_config = {\n",
-    "    'batch_size': [128],\n",
-    "    'epochs': [30],\n",
-    "    'learning_rate': [.001]\n",
-    "}\n",
-    "\n",
-    "keys = model_config.keys()\n",
-    "values = model_config.values()\n",
-    "\n",
-    "# Generate all parameter combinations from model_config using itertools.product\n",
-    "combinations = itertools.product(*values)\n",
-    "\n",
-    "# Iterate through each parameter combination and run model \n",
-    "for combo in combinations:\n",
-    "  combo = dict(zip(keys, combo))\n",
-    "\n",
-    "  train_dataset = DataLoader(TensorDataset(single_chem_train_spectra_tensor, single_chem_train_chem_encodings_tensor, single_chem_train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
-    "  val_dataset = DataLoader(TensorDataset(single_chem_val_spectra_tensor, single_chem_val_chem_encodings_tensor, single_chem_val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
-    "\n",
-    "  wandb_kwargs = {\n",
-    "    'learning_rate': combo['learning_rate'],\n",
-    "    'epochs': combo['epochs'],\n",
-    "    'batch_size': combo['batch_size'],\n",
-    "    'model_architecture': 'single_chem_generator',\n",
-    "    'optimizer':'AdamW',\n",
-    "    'loss': 'MSELoss'\n",
-    "  }\n",
-    "\n",
-    "  run_with_wandb(config, **wandb_kwargs)\n",
-    "\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "  print('New run with hyperparameters:')\n",
-    "  for key in combo:\n",
-    "    print(key, ' : ', combo[key])\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "\n",
-    "  for epoch in range(combo['epochs']):\n",
-    "    # Set model to training mode\n",
-    "    generator.train(True)\n",
-    "\n",
-    "    # do a pass over the data\n",
-    "    # at last epoch get predicted embeddings and chem names\n",
-    "    if (epoch + 1) == combo['epochs']:\n",
-    "      average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "    else:\n",
-    "      average_loss = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "\n",
-    "    epoch_val_loss = 0  \n",
-    "    # evaluate model on validation data\n",
-    "    generator.eval() # Set model to evaluation mode\n",
-    "    with torch.no_grad():\n",
-    "      for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
-    "        val_true_spectra = val_true_spectra.to(device)\n",
-    "        val_name_encodings = val_name_encodings.to(device)\n",
-    "        val_true_embeddings = val_true_embeddings.to(device)\n",
-    "\n",
-    "        val_batch_predicted_spectra = generator(val_true_embeddings)\n",
-    "\n",
-    "        val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
-    "        # accumulate epoch validation loss\n",
-    "        epoch_val_loss += val_loss.item()\n",
-    "\n",
-    "    # divide by number of batches to calculate average loss\n",
-    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
-    "\n",
-    "    # log losses to wandb\n",
-    "    wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
-    "\n",
-    "    if (epoch + 1) % 10 == 0:\n",
-    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
-    "      print(f'   Training loss: {average_loss}')\n",
-    "      print(f'   Validation loss: {val_average_loss}')\n",
-    "      print('-------------------------------------------')\n",
+    "# # set var deciding if results plot for this run is saved to wandb\n",
+    "# log_wandb = True\n",
+    "# # simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
+    "# plotting_chem = 'TEPO'\n",
     "\n",
-    "  # saving comparison plots for first 5 spectra\n",
-    "  plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
+    "# # Last 8 cols of the df are the chem names\n",
+    "# sorted_chem_names = list(train.columns[-8:])\n",
     "\n",
-    "  if average_loss < lowest_loss:\n",
-    "    lowest_loss = average_loss\n",
-    "    best_hyperparams = combo\n",
+    "# # model_config = {\n",
+    "# #   'batch_size':[128, 256],\n",
+    "# #   'epochs': [100],\n",
+    "# #   'learning_rate':[.01, .001]\n",
+    "#   # }\n",
     "\n",
-    "  wandb.finish()\n",
+    "# # loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
+    "# lowest_loss = 10000000\n",
     "\n",
-    "print('Hyperparameters for best model: ')\n",
-    "for key in best_hyperparams:\n",
-    "  print('   ', key, ' : ', best_hyperparams[key])"
+    "# model_config = {\n",
+    "#     'batch_size': [128],\n",
+    "#     'epochs': [30],\n",
+    "#     'learning_rate': [.001]\n",
+    "# }\n",
+    "\n",
+    "# keys = model_config.keys()\n",
+    "# values = model_config.values()\n",
+    "\n",
+    "# # Generate all parameter combinations from model_config using itertools.product\n",
+    "# combinations = itertools.product(*values)\n",
+    "\n",
+    "# # Iterate through each parameter combination and run model \n",
+    "# for combo in combinations:\n",
+    "#   combo = dict(zip(keys, combo))\n",
+    "\n",
+    "#   train_dataset = DataLoader(TensorDataset(single_chem_train_spectra_tensor, single_chem_train_chem_encodings_tensor, single_chem_train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
+    "#   val_dataset = DataLoader(TensorDataset(single_chem_val_spectra_tensor, single_chem_val_chem_encodings_tensor, single_chem_val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
+    "\n",
+    "#   wandb_kwargs = {\n",
+    "#     'learning_rate': combo['learning_rate'],\n",
+    "#     'epochs': combo['epochs'],\n",
+    "#     'batch_size': combo['batch_size'],\n",
+    "#     'model_architecture': 'single_chem_generator',\n",
+    "#     'optimizer':'AdamW',\n",
+    "#     'loss': 'MSELoss'\n",
+    "#   }\n",
+    "\n",
+    "#   run_with_wandb(config, **wandb_kwargs)\n",
+    "\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "#   print('New run with hyperparameters:')\n",
+    "#   for key in combo:\n",
+    "#     print(key, ' : ', combo[key])\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "\n",
+    "#   for epoch in range(combo['epochs']):\n",
+    "#     # Set model to training mode\n",
+    "#     generator.train(True)\n",
+    "\n",
+    "#     # do a pass over the data\n",
+    "#     # at last epoch get predicted embeddings and chem names\n",
+    "#     if (epoch + 1) == combo['epochs']:\n",
+    "#       average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "#     else:\n",
+    "#       average_loss = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "\n",
+    "#     epoch_val_loss = 0  \n",
+    "#     # evaluate model on validation data\n",
+    "#     generator.eval() # Set model to evaluation mode\n",
+    "#     with torch.no_grad():\n",
+    "#       for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
+    "#         val_true_spectra = val_true_spectra.to(device)\n",
+    "#         val_name_encodings = val_name_encodings.to(device)\n",
+    "#         val_true_embeddings = val_true_embeddings.to(device)\n",
+    "\n",
+    "#         val_batch_predicted_spectra = generator(val_true_embeddings)\n",
+    "\n",
+    "#         val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
+    "#         # accumulate epoch validation loss\n",
+    "#         epoch_val_loss += val_loss.item()\n",
+    "\n",
+    "#     # divide by number of batches to calculate average loss\n",
+    "#     val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "\n",
+    "#     # log losses to wandb\n",
+    "#     wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
+    "\n",
+    "#     if (epoch + 1) % 10 == 0:\n",
+    "#       print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
+    "#       print(f'   Training loss: {average_loss}')\n",
+    "#       print(f'   Validation loss: {val_average_loss}')\n",
+    "#       print('-------------------------------------------')\n",
+    "\n",
+    "#   # saving comparison plots for first 5 spectra\n",
+    "#   plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
+    "\n",
+    "#   if average_loss < lowest_loss:\n",
+    "#     lowest_loss = average_loss\n",
+    "#     best_hyperparams = combo\n",
+    "\n",
+    "#   wandb.finish()\n",
+    "\n",
+    "# print('Hyperparameters for best model: ')\n",
+    "# for key in best_hyperparams:\n",
+    "#   print('   ', key, ' : ', best_hyperparams[key])"
    ]
   },
   {
@@ -2267,32 +2763,32 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "# drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
-    "train_spectra = train_truncated.iloc[:,2:-9]\n",
-    "train_chem_encodings = train_truncated.iloc[:,-8:]\n",
-    "\n",
-    "val_spectra = val_truncated.iloc[:,2:-9]\n",
-    "val_chem_encodings = val_truncated.iloc[:,-8:]\n",
-    "\n",
-    "# create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
-    "train_chem_labels = list(train_truncated['Label'])\n",
-    "train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
-    "train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
-    "train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
-    "\n",
-    "val_chem_labels = list(val_truncated['Label'])\n",
-    "val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
-    "val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
-    "val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
+    "# # drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
+    "# train_spectra = train_truncated.iloc[:,2:-9]\n",
+    "# train_chem_encodings = train_truncated.iloc[:,-8:]\n",
+    "\n",
+    "# val_spectra = val_truncated.iloc[:,2:-9]\n",
+    "# val_chem_encodings = val_truncated.iloc[:,-8:]\n",
+    "\n",
+    "# # create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
+    "# train_chem_labels = list(train_truncated['Label'])\n",
+    "# train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
+    "# train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
+    "# train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
+    "\n",
+    "# val_chem_labels = list(val_truncated['Label'])\n",
+    "# val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
+    "# val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
+    "# val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -2796,120 +3292,120 @@
     }
    ],
    "source": [
-    "# set var deciding if results plot for this run is saved to wandb\n",
-    "log_wandb = True\n",
-    "# simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
-    "plotting_chem = 'TEPO'\n",
-    "\n",
-    "# Last 8 cols of the df are the chem names\n",
-    "sorted_chem_names = list(train.columns[-8:])\n",
-    "\n",
-    "# model_config = {\n",
-    "#   'batch_size':[128, 256],\n",
-    "#   'epochs': [100],\n",
-    "#   'learning_rate':[.01, .001]\n",
-    "  # }\n",
-    "\n",
-    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
-    "lowest_loss = 10000000\n",
-    "\n",
-    "model_config = {\n",
-    "    'batch_size': [128],\n",
-    "    'epochs': [30],\n",
-    "    'learning_rate': [.0001, .01]\n",
-    "}\n",
-    "\n",
-    "keys = model_config.keys()\n",
-    "values = model_config.values()\n",
-    "\n",
-    "# Generate all parameter combinations from model_config using itertools.product\n",
-    "combinations = itertools.product(*values)\n",
-    "\n",
-    "# Iterate through each parameter combination and run model \n",
-    "for combo in combinations:\n",
-    "  combo = dict(zip(keys, combo))\n",
-    "\n",
-    "  train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
-    "  val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
-    "  generator = TruncatedDataGenerator().to(device)\n",
-    "\n",
-    "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
-    "  generator_criterion = nn.MSELoss()\n",
-    "\n",
-    "  wandb_kwargs = {\n",
-    "    'learning_rate': combo['learning_rate'],\n",
-    "    'epochs': combo['epochs'],\n",
-    "    'batch_size': combo['batch_size'],\n",
-    "    'model_architecture': 'generator',\n",
-    "    'optimizer':'AdamW',\n",
-    "    'loss': 'MSELoss',\n",
-    "    'training_data':'Truncated at p_700 and n_700',\n",
-    "  }\n",
-    "\n",
-    "  run_with_wandb(config, **wandb_kwargs)\n",
-    "\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "  print('New run with hyperparameters:')\n",
-    "  for key in combo:\n",
-    "    print(key, ' : ', combo[key])\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "\n",
-    "  for epoch in range(combo['epochs']):\n",
-    "    # Set model to training mode\n",
-    "    generator.train(True)\n",
-    "\n",
-    "    # do a pass over the data\n",
-    "    # at last epoch get predicted embeddings and chem names\n",
-    "    if (epoch + 1) == combo['epochs']:\n",
-    "      average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "    else:\n",
-    "      average_loss = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "\n",
-    "    epoch_val_loss = 0  \n",
-    "    # evaluate model on validation data\n",
-    "    generator.eval() # Set model to evaluation mode\n",
-    "    with torch.no_grad():\n",
-    "      for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
-    "        val_true_spectra = val_true_spectra.to(device)\n",
-    "        val_name_encodings = val_name_encodings.to(device)\n",
-    "        val_true_embeddings = val_true_embeddings.to(device)\n",
-    "\n",
-    "        val_batch_predicted_spectra = generator(val_true_embeddings)\n",
-    "\n",
-    "        val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
-    "        # accumulate epoch validation loss\n",
-    "        epoch_val_loss += val_loss.item()\n",
-    "\n",
-    "    # divide by number of batches to calculate average loss\n",
-    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
-    "\n",
-    "    # log losses to wandb\n",
-    "    wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
-    "\n",
-    "    if (epoch + 1) % 10 == 0:\n",
-    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
-    "      print(f'   Training loss: {average_loss}')\n",
-    "      print(f'   Validation loss: {val_average_loss}')\n",
-    "      print('-------------------------------------------')\n",
+    "# # set var deciding if results plot for this run is saved to wandb\n",
+    "# log_wandb = True\n",
+    "# # simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
+    "# plotting_chem = 'TEPO'\n",
     "\n",
-    "  # saving comparison plots for first 5 spectra\n",
-    "  plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
+    "# # Last 8 cols of the df are the chem names\n",
+    "# sorted_chem_names = list(train.columns[-8:])\n",
     "\n",
-    "  if average_loss < lowest_loss:\n",
-    "    lowest_loss = average_loss\n",
-    "    best_hyperparams = combo\n",
+    "# # model_config = {\n",
+    "# #   'batch_size':[128, 256],\n",
+    "# #   'epochs': [100],\n",
+    "# #   'learning_rate':[.01, .001]\n",
+    "#   # }\n",
     "\n",
-    "  wandb.finish()\n",
+    "# # loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
+    "# lowest_loss = 10000000\n",
     "\n",
-    "print('Hyperparameters for best model: ')\n",
-    "for key in best_hyperparams:\n",
-    "  print('   ', key, ' : ', best_hyperparams[key])"
+    "# model_config = {\n",
+    "#     'batch_size': [128],\n",
+    "#     'epochs': [30],\n",
+    "#     'learning_rate': [.0001, .01]\n",
+    "# }\n",
+    "\n",
+    "# keys = model_config.keys()\n",
+    "# values = model_config.values()\n",
+    "\n",
+    "# # Generate all parameter combinations from model_config using itertools.product\n",
+    "# combinations = itertools.product(*values)\n",
+    "\n",
+    "# # Iterate through each parameter combination and run model \n",
+    "# for combo in combinations:\n",
+    "#   combo = dict(zip(keys, combo))\n",
+    "\n",
+    "#   train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
+    "#   val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
+    "#   generator = TruncatedDataGenerator().to(device)\n",
+    "\n",
+    "#   generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
+    "#   generator_criterion = nn.MSELoss()\n",
+    "\n",
+    "#   wandb_kwargs = {\n",
+    "#     'learning_rate': combo['learning_rate'],\n",
+    "#     'epochs': combo['epochs'],\n",
+    "#     'batch_size': combo['batch_size'],\n",
+    "#     'model_architecture': 'generator',\n",
+    "#     'optimizer':'AdamW',\n",
+    "#     'loss': 'MSELoss',\n",
+    "#     'training_data':'Truncated at p_700 and n_700',\n",
+    "#   }\n",
+    "\n",
+    "#   run_with_wandb(config, **wandb_kwargs)\n",
+    "\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "#   print('New run with hyperparameters:')\n",
+    "#   for key in combo:\n",
+    "#     print(key, ' : ', combo[key])\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "\n",
+    "#   for epoch in range(combo['epochs']):\n",
+    "#     # Set model to training mode\n",
+    "#     generator.train(True)\n",
+    "\n",
+    "#     # do a pass over the data\n",
+    "#     # at last epoch get predicted embeddings and chem names\n",
+    "#     if (epoch + 1) == combo['epochs']:\n",
+    "#       average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "#     else:\n",
+    "#       average_loss = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "\n",
+    "#     epoch_val_loss = 0  \n",
+    "#     # evaluate model on validation data\n",
+    "#     generator.eval() # Set model to evaluation mode\n",
+    "#     with torch.no_grad():\n",
+    "#       for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
+    "#         val_true_spectra = val_true_spectra.to(device)\n",
+    "#         val_name_encodings = val_name_encodings.to(device)\n",
+    "#         val_true_embeddings = val_true_embeddings.to(device)\n",
+    "\n",
+    "#         val_batch_predicted_spectra = generator(val_true_embeddings)\n",
+    "\n",
+    "#         val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
+    "#         # accumulate epoch validation loss\n",
+    "#         epoch_val_loss += val_loss.item()\n",
+    "\n",
+    "#     # divide by number of batches to calculate average loss\n",
+    "#     val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "\n",
+    "#     # log losses to wandb\n",
+    "#     wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
+    "\n",
+    "#     if (epoch + 1) % 10 == 0:\n",
+    "#       print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
+    "#       print(f'   Training loss: {average_loss}')\n",
+    "#       print(f'   Validation loss: {val_average_loss}')\n",
+    "#       print('-------------------------------------------')\n",
+    "\n",
+    "#   # saving comparison plots for first 5 spectra\n",
+    "#   plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
+    "\n",
+    "#   if average_loss < lowest_loss:\n",
+    "#     lowest_loss = average_loss\n",
+    "#     best_hyperparams = combo\n",
+    "\n",
+    "#   wandb.finish()\n",
+    "\n",
+    "# print('Hyperparameters for best model: ')\n",
+    "# for key in best_hyperparams:\n",
+    "#   print('   ', key, ' : ', best_hyperparams[key])"
    ]
   },
   {
@@ -2997,27 +3493,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 78,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
-    "# drop first two cols ('Unnamed:0' and 'index') \n",
-    "train_spectra = train_truncated_p.iloc[:,2:-8]\n",
-    "train_chem_encodings = train.iloc[:,-8:]\n",
-    "\n",
-    "val_spectra = val_truncated_p.iloc[:,2:-8]\n",
-    "val_chem_encodings = val.iloc[:,-8:]\n",
-    "\n",
-    "# create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
-    "train_chem_labels = list(train['Label'])\n",
-    "train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
-    "train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
-    "train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
-    "\n",
-    "val_chem_labels = list(val['Label'])\n",
-    "val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
-    "val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
-    "val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
+    "# # drop first two cols ('Unnamed:0' and 'index') \n",
+    "# train_spectra = train_truncated_p.iloc[:,2:-8]\n",
+    "# train_chem_encodings = train.iloc[:,-8:]\n",
+    "\n",
+    "# val_spectra = val_truncated_p.iloc[:,2:-8]\n",
+    "# val_chem_encodings = val.iloc[:,-8:]\n",
+    "\n",
+    "# # create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
+    "# train_chem_labels = list(train['Label'])\n",
+    "# train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
+    "# train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
+    "# train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
+    "\n",
+    "# val_chem_labels = list(val['Label'])\n",
+    "# val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
+    "# val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
+    "# val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
    ]
   },
   {
@@ -3271,121 +3767,121 @@
     }
    ],
    "source": [
-    "# set var deciding if results plot for this run is saved to wandb\n",
-    "log_wandb = True\n",
-    "# simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
-    "plotting_chem = 'DMMP'\n",
-    "\n",
-    "# Last 8 cols of the df are the chem names\n",
-    "sorted_chem_names = list(train.columns[-8:])\n",
-    "\n",
-    "# model_config = {\n",
-    "#   'batch_size':[128, 256],\n",
-    "#   'epochs': [100],\n",
-    "#   'learning_rate':[.01, .001]\n",
-    "  # }\n",
-    "\n",
-    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
-    "lowest_loss = 10000000\n",
-    "\n",
-    "model_config = {\n",
-    "    'batch_size': [128],\n",
-    "    'epochs': [30],\n",
-    "    'learning_rate': [.001]\n",
-    "}\n",
-    "\n",
-    "keys = model_config.keys()\n",
-    "values = model_config.values()\n",
-    "\n",
-    "# Generate all parameter combinations from model_config using itertools.product\n",
-    "combinations = itertools.product(*values)\n",
-    "\n",
-    "# Iterate through each parameter combination and run model \n",
-    "for combo in combinations:\n",
-    "  combo = dict(zip(keys, combo))\n",
-    "\n",
-    "  train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
-    "  val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
-    "  generator = PosSpectrumGenerator().to(device)\n",
-    "\n",
-    "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
-    "  generator_criterion = nn.MSELoss()\n",
-    "\n",
-    "  wandb_kwargs = {\n",
-    "    'learning_rate': combo['learning_rate'],\n",
-    "    'epochs': combo['epochs'],\n",
-    "    'batch_size': combo['batch_size'],\n",
-    "    'model_architecture': 'generator',\n",
-    "    'optimizer':'AdamW',\n",
-    "    'loss': 'MSELoss',\n",
-    "    'training_data':'Truncated at p_700',\n",
-    "    'chemical_labels':'yes',\n",
-    "  }\n",
-    "\n",
-    "  run_with_wandb(config, **wandb_kwargs)\n",
-    "\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "  print('New run with hyperparameters:')\n",
-    "  for key in combo:\n",
-    "    print(key, ' : ', combo[key])\n",
-    "  print('--------------------------')\n",
-    "  print('--------------------------')\n",
-    "\n",
-    "  for epoch in range(combo['epochs']):\n",
-    "    # Set model to training mode\n",
-    "    generator.train(True)\n",
-    "\n",
-    "    # do a pass over the data\n",
-    "    # at last epoch get predicted embeddings and chem names\n",
-    "    if (epoch + 1) == combo['epochs']:\n",
-    "      average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "    else:\n",
-    "      average_loss = train_one_epoch(\n",
-    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
-    "        )\n",
-    "\n",
-    "    epoch_val_loss = 0  \n",
-    "    # evaluate model on validation data\n",
-    "    generator.eval() # Set model to evaluation mode\n",
-    "    with torch.no_grad():\n",
-    "      for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
-    "        val_true_spectra = val_true_spectra.to(device)\n",
-    "        val_name_encodings = val_name_encodings.to(device)\n",
-    "        val_true_embeddings = val_true_embeddings.to(device)\n",
-    "\n",
-    "        val_batch_predicted_spectra = generator(val_true_embeddings)\n",
-    "\n",
-    "        val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
-    "        # accumulate epoch validation loss\n",
-    "        epoch_val_loss += val_loss.item()\n",
-    "\n",
-    "    # divide by number of batches to calculate average loss\n",
-    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
-    "\n",
-    "    # log losses to wandb\n",
-    "    wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
-    "\n",
-    "    if (epoch + 1) % 10 == 0:\n",
-    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
-    "      print(f'   Training loss: {average_loss}')\n",
-    "      print(f'   Validation loss: {val_average_loss}')\n",
-    "      print('-------------------------------------------')\n",
+    "# # set var deciding if results plot for this run is saved to wandb\n",
+    "# log_wandb = True\n",
+    "# # simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
+    "# plotting_chem = 'DMMP'\n",
     "\n",
-    "  # saving comparison plots for first 5 spectra\n",
-    "  plot_single_type_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5], label='Positive')\n",
+    "# # Last 8 cols of the df are the chem names\n",
+    "# sorted_chem_names = list(train.columns[-8:])\n",
     "\n",
-    "  if average_loss < lowest_loss:\n",
-    "    lowest_loss = average_loss\n",
-    "    best_hyperparams = combo\n",
+    "# # model_config = {\n",
+    "# #   'batch_size':[128, 256],\n",
+    "# #   'epochs': [100],\n",
+    "# #   'learning_rate':[.01, .001]\n",
+    "#   # }\n",
     "\n",
-    "  wandb.finish()\n",
+    "# # loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
+    "# lowest_loss = 10000000\n",
     "\n",
-    "print('Hyperparameters for best model: ')\n",
-    "for key in best_hyperparams:\n",
-    "  print('   ', key, ' : ', best_hyperparams[key])"
+    "# model_config = {\n",
+    "#     'batch_size': [128],\n",
+    "#     'epochs': [30],\n",
+    "#     'learning_rate': [.001]\n",
+    "# }\n",
+    "\n",
+    "# keys = model_config.keys()\n",
+    "# values = model_config.values()\n",
+    "\n",
+    "# # Generate all parameter combinations from model_config using itertools.product\n",
+    "# combinations = itertools.product(*values)\n",
+    "\n",
+    "# # Iterate through each parameter combination and run model \n",
+    "# for combo in combinations:\n",
+    "#   combo = dict(zip(keys, combo))\n",
+    "\n",
+    "#   train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
+    "#   val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
+    "#   generator = PosSpectrumGenerator().to(device)\n",
+    "\n",
+    "#   generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
+    "#   generator_criterion = nn.MSELoss()\n",
+    "\n",
+    "#   wandb_kwargs = {\n",
+    "#     'learning_rate': combo['learning_rate'],\n",
+    "#     'epochs': combo['epochs'],\n",
+    "#     'batch_size': combo['batch_size'],\n",
+    "#     'model_architecture': 'generator',\n",
+    "#     'optimizer':'AdamW',\n",
+    "#     'loss': 'MSELoss',\n",
+    "#     'training_data':'Truncated at p_700',\n",
+    "#     'chemical_labels':'yes',\n",
+    "#   }\n",
+    "\n",
+    "#   run_with_wandb(config, **wandb_kwargs)\n",
+    "\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "#   print('New run with hyperparameters:')\n",
+    "#   for key in combo:\n",
+    "#     print(key, ' : ', combo[key])\n",
+    "#   print('--------------------------')\n",
+    "#   print('--------------------------')\n",
+    "\n",
+    "#   for epoch in range(combo['epochs']):\n",
+    "#     # Set model to training mode\n",
+    "#     generator.train(True)\n",
+    "\n",
+    "#     # do a pass over the data\n",
+    "#     # at last epoch get predicted embeddings and chem names\n",
+    "#     if (epoch + 1) == combo['epochs']:\n",
+    "#       average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "#     else:\n",
+    "#       average_loss = train_one_epoch(\n",
+    "#         train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
+    "#         )\n",
+    "\n",
+    "#     epoch_val_loss = 0  \n",
+    "#     # evaluate model on validation data\n",
+    "#     generator.eval() # Set model to evaluation mode\n",
+    "#     with torch.no_grad():\n",
+    "#       for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
+    "#         val_true_spectra = val_true_spectra.to(device)\n",
+    "#         val_name_encodings = val_name_encodings.to(device)\n",
+    "#         val_true_embeddings = val_true_embeddings.to(device)\n",
+    "\n",
+    "#         val_batch_predicted_spectra = generator(val_true_embeddings)\n",
+    "\n",
+    "#         val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
+    "#         # accumulate epoch validation loss\n",
+    "#         epoch_val_loss += val_loss.item()\n",
+    "\n",
+    "#     # divide by number of batches to calculate average loss\n",
+    "#     val_average_loss = epoch_val_loss/len(val_dataset)\n",
+    "\n",
+    "#     # log losses to wandb\n",
+    "#     wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
+    "\n",
+    "#     if (epoch + 1) % 10 == 0:\n",
+    "#       print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
+    "#       print(f'   Training loss: {average_loss}')\n",
+    "#       print(f'   Validation loss: {val_average_loss}')\n",
+    "#       print('-------------------------------------------')\n",
+    "\n",
+    "#   # saving comparison plots for first 5 spectra\n",
+    "#   plot_single_type_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5], label='Positive')\n",
+    "\n",
+    "#   if average_loss < lowest_loss:\n",
+    "#     lowest_loss = average_loss\n",
+    "#     best_hyperparams = combo\n",
+    "\n",
+    "#   wandb.finish()\n",
+    "\n",
+    "# print('Hyperparameters for best model: ')\n",
+    "# for key in best_hyperparams:\n",
+    "#   print('   ', key, ' : ', best_hyperparams[key])"
    ]
   },
   {
@@ -3881,7 +4377,7 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Plotting Spectra Across Simulants:\n",
+    "## Plotting Spectra Across Simulants:\n",
     "---"
    ]
   },
@@ -4170,7 +4666,7 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Looking at Train Metadata:\n",
+    "## Looking at Train Metadata:\n",
     "---"
    ]
   },
diff --git a/models/tmp_plot.png b/models/tmp_plot.png
index d40b2fbd..5a8b0053 100644
Binary files a/models/tmp_plot.png and b/models/tmp_plot.png differ
diff --git a/models/wandb/debug-internal.log b/models/wandb/debug-internal.log
index b5a86c2e..e83f1502 120000
--- a/models/wandb/debug-internal.log
+++ b/models/wandb/debug-internal.log
@@ -1 +1 @@
-run-20241216_122321-fwj5g5bb/logs/debug-internal.log
\ No newline at end of file
+run-20241216_221233-2r52ynyz/logs/debug-internal.log
\ No newline at end of file
diff --git a/models/wandb/debug.log b/models/wandb/debug.log
index d4953b14..b0067771 120000
--- a/models/wandb/debug.log
+++ b/models/wandb/debug.log
@@ -1 +1 @@
-run-20241216_122321-fwj5g5bb/logs/debug.log
\ No newline at end of file
+run-20241216_221233-2r52ynyz/logs/debug.log
\ No newline at end of file
diff --git a/models/wandb/latest-run b/models/wandb/latest-run
index 2dcf714d..f46ca689 120000
--- a/models/wandb/latest-run
+++ b/models/wandb/latest-run
@@ -1 +1 @@
-run-20241216_122321-fwj5g5bb
\ No newline at end of file
+run-20241216_221233-2r52ynyz
\ No newline at end of file
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml b/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml
index 62e5a0c4..253937e7 100644
--- a/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml
+++ b/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml
@@ -57,6 +57,7 @@ _wandb:
       - 53
       - 55
       3:
+      - 2
       - 16
       - 23
       4: 3.8.10
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log b/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log
index 681d64ef..8f50f705 100644
--- a/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log
+++ b/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log
@@ -11,3 +11,4 @@ Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.
 Epoch[10/500]:
    Training loss: 0.002854047005858566
    Validation loss: 2158.8611302098543
+-------------------------------------------
\ No newline at end of file
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json b/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
index 07ea5f44..65371242 100644
--- a/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
+++ b/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
@@ -1 +1 @@
-{"Encoder Training Loss": 3.3890635190263284e-05, "Encoder Validation Loss": 1785.7471227751018, "_timestamp": 1734369985.538888, "_runtime": 178.49497985839844, "_step": 12}
\ No newline at end of file
+{"Encoder Training Loss": 3.3890635190263284e-05, "Encoder Validation Loss": 1785.7471227751018, "_timestamp": 1734369985.538888, "_runtime": 178.49497985839844, "_step": 12, "_wandb": {"runtime": 984}}
\ No newline at end of file
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug-internal.log b/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug-internal.log
index 48d736d6..21061473 100644
--- a/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug-internal.log
+++ b/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug-internal.log
@@ -231,3 +231,377 @@
 2024-12-16 12:28:46,578 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
 2024-12-16 12:28:51,579 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
 2024-12-16 12:28:56,579 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:28:58,246 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:28:59,423 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:28:59,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:29:02,542 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:07,542 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:12,543 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:14,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:29:14,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:29:18,500 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:23,501 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:28,247 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:29:29,248 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:29,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:29:29,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:29:34,581 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:39,582 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:44,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:29:44,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:29:45,502 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:50,502 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:55,504 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:29:58,248 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:29:59,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:29:59,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:30:00,519 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:05,520 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:10,520 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:14,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:30:14,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:30:15,555 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:20,555 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:25,556 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:28,248 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:30:29,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:30:29,424 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:30:30,566 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:35,567 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:40,567 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:44,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:30:44,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:30:45,582 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:50,583 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:55,584 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:30:58,250 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:30:59,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:30:59,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:31:00,588 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:05,588 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:10,589 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:14,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:31:14,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:31:16,513 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:21,514 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:26,514 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:28,250 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:31:29,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:31:29,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:31:31,550 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:36,550 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:41,551 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:44,424 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:31:44,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:31:47,526 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:52,527 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:57,527 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:31:58,250 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:31:59,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:31:59,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:32:03,487 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:08,487 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:13,488 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:14,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:32:14,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:32:18,574 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:23,574 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:28,251 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:32:29,251 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:29,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:32:29,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:32:34,486 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:39,487 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:44,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:32:44,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:32:44,580 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:49,580 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:54,581 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:32:58,251 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:32:59,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:32:59,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:33:00,530 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:05,530 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:10,531 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:14,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:33:14,425 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:33:16,500 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:21,500 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:26,501 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:28,252 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:33:29,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:33:29,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:33:31,576 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:36,576 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:41,577 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:44,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:33:44,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:33:47,537 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:52,538 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:57,538 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:33:58,253 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:33:59,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:33:59,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:34:02,562 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:07,562 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:12,563 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:14,425 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:34:14,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:34:17,576 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:22,576 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:27,577 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:28,253 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:34:29,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:34:29,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:34:32,584 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:37,585 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:42,586 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:44,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:34:44,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:34:48,529 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:53,530 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:58,254 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:34:59,255 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:34:59,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:34:59,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:35:04,547 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:09,548 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:14,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:35:14,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:35:15,495 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:20,495 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:25,496 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:28,254 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:35:29,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:35:29,426 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:35:30,557 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:35,558 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:40,558 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:44,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:35:44,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:35:45,585 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:50,586 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:55,586 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:35:58,255 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:35:59,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:35:59,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:36:01,503 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:06,504 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:11,504 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:14,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:36:14,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:36:16,557 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:21,557 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:26,558 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:28,256 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:36:29,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:36:29,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:36:31,567 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:36,567 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:41,568 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:44,426 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:36:44,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:36:46,589 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:51,590 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:56,590 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:36:58,256 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:36:59,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:36:59,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:37:02,554 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:07,555 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:12,556 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:14,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:37:14,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:37:18,542 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:23,542 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:28,257 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:37:29,257 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:29,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:37:29,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:37:34,519 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:39,519 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:44,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:37:44,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:37:44,578 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:49,579 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:54,579 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:37:58,257 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:37:59,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:37:59,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:38:00,504 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:05,505 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:10,505 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:14,427 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:38:14,427 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:38:15,573 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:20,574 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:25,575 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:28,258 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:38:31,259 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:35,918 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:38:35,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:38:37,051 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:42,051 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:47,052 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:50,918 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:38:50,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:38:52,061 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:57,061 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:38:58,258 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:39:02,260 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:05,918 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:39:05,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:39:08,077 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:13,078 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:18,079 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:20,918 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:39:20,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:39:24,006 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:28,260 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:39:29,261 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:34,261 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:35,919 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:39:35,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:39:40,003 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:45,003 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:50,004 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:50,919 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: stop_status
+2024-12-16 12:39:50,919 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: stop_status
+2024-12-16 12:39:52,337 DEBUG   SenderThread:281812 [sender.py:send():378] send: telemetry
+2024-12-16 12:39:52,337 DEBUG   SenderThread:281812 [sender.py:send():378] send: exit
+2024-12-16 12:39:52,337 INFO    SenderThread:281812 [sender.py:send_exit():585] handling exit code: 0
+2024-12-16 12:39:52,337 INFO    SenderThread:281812 [sender.py:send_exit():587] handling runtime: 984
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:_save_file():1389] saving file wandb-summary.json with policy end
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:send_exit():593] send defer
+2024-12-16 12:39:52,338 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,338 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 0
+2024-12-16 12:39:52,338 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 0
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 1
+2024-12-16 12:39:52,338 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,338 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 1
+2024-12-16 12:39:52,338 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 1
+2024-12-16 12:39:52,338 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 2
+2024-12-16 12:39:52,338 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,339 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 2
+2024-12-16 12:39:52,339 INFO    HandlerThread:281812 [system_monitor.py:finish():203] Stopping system monitor
+2024-12-16 12:39:52,339 INFO    HandlerThread:281812 [interfaces.py:finish():200] Joined cpu monitor
+2024-12-16 12:39:52,339 INFO    HandlerThread:281812 [interfaces.py:finish():200] Joined disk monitor
+2024-12-16 12:39:52,339 DEBUG   SystemMonitor:281812 [system_monitor.py:_start():179] Finished system metrics aggregation loop
+2024-12-16 12:39:52,339 DEBUG   SystemMonitor:281812 [system_monitor.py:_start():183] Publishing last batch of metrics
+2024-12-16 12:39:52,410 INFO    Thread-40 :281812 [dir_watcher.py:_on_file_modified():288] file/dir modified: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
+2024-12-16 12:39:52,429 INFO    HandlerThread:281812 [interfaces.py:finish():200] Joined gpu monitor
+2024-12-16 12:39:52,429 INFO    HandlerThread:281812 [interfaces.py:finish():200] Joined memory monitor
+2024-12-16 12:39:52,429 INFO    HandlerThread:281812 [interfaces.py:finish():200] Joined network monitor
+2024-12-16 12:39:52,430 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,430 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 2
+2024-12-16 12:39:52,430 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 3
+2024-12-16 12:39:52,430 DEBUG   SenderThread:281812 [sender.py:send():378] send: stats
+2024-12-16 12:39:52,430 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,430 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 3
+2024-12-16 12:39:52,430 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,430 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 3
+2024-12-16 12:39:52,430 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 4
+2024-12-16 12:39:52,431 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,431 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 4
+2024-12-16 12:39:52,431 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,431 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 4
+2024-12-16 12:39:52,431 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 5
+2024-12-16 12:39:52,431 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,431 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 5
+2024-12-16 12:39:52,431 DEBUG   SenderThread:281812 [sender.py:send():378] send: summary
+2024-12-16 12:39:52,432 INFO    SenderThread:281812 [sender.py:_save_file():1389] saving file wandb-summary.json with policy end
+2024-12-16 12:39:52,432 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,432 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 5
+2024-12-16 12:39:52,432 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 6
+2024-12-16 12:39:52,432 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,432 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 6
+2024-12-16 12:39:52,432 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,432 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 6
+2024-12-16 12:39:52,436 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: status_report
+2024-12-16 12:39:52,525 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 7
+2024-12-16 12:39:52,525 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,525 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 7
+2024-12-16 12:39:52,525 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,525 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 7
+2024-12-16 12:39:52,642 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 8
+2024-12-16 12:39:52,642 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,642 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 8
+2024-12-16 12:39:52,642 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:52,642 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 8
+2024-12-16 12:39:52,642 INFO    SenderThread:281812 [job_builder.py:build():432] Attempting to build job artifact
+2024-12-16 12:39:52,643 INFO    SenderThread:281812 [job_builder.py:_get_source_type():565] is repo sourced job
+2024-12-16 12:39:52,643 INFO    SenderThread:281812 [job_builder.py:_get_program_relpath():583] run is notebook based run
+2024-12-16 12:39:52,646 INFO    SenderThread:281812 [job_builder.py:build():541] adding wandb-job metadata file
+2024-12-16 12:39:52,654 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 9
+2024-12-16 12:39:52,654 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:52,655 DEBUG   SenderThread:281812 [sender.py:send():378] send: artifact
+2024-12-16 12:39:52,655 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 9
+2024-12-16 12:39:53,156 INFO    wandb-upload_1:281812 [upload_job.py:push():85] Skipped uploading /home/cmdunham/.local/share/wandb/artifacts/staging/tmpq857qyct
+2024-12-16 12:39:53,157 INFO    wandb-upload_2:281812 [upload_job.py:push():85] Skipped uploading /tmp/tmptvr8c05c/wandb-job.json
+2024-12-16 12:39:53,338 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: poll_exit
+2024-12-16 12:39:53,410 INFO    Thread-40 :281812 [dir_watcher.py:_on_file_modified():288] file/dir modified: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log
+2024-12-16 12:39:53,410 INFO    Thread-40 :281812 [dir_watcher.py:_on_file_modified():288] file/dir modified: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
+2024-12-16 12:39:53,410 INFO    Thread-40 :281812 [dir_watcher.py:_on_file_modified():288] file/dir modified: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml
+2024-12-16 12:39:53,521 INFO    wandb-upload_0:281812 [upload_job.py:push():88] Uploaded file /home/cmdunham/.local/share/wandb/artifacts/staging/tmpryykey16
+2024-12-16 12:39:54,182 INFO    SenderThread:281812 [sender.py:send_artifact():1467] sent artifact job-https___github.com_CateMerfeld_ChemicalDataGeneration.git__home_cmdunham_ChemicalDataGeneration_models_carl_encoder.ipynb - {'id': 'QXJ0aWZhY3Q6MTM3NTAxNDYzOA==', 'state': 'PENDING', 'artifactSequence': {'id': 'QXJ0aWZhY3RDb2xsZWN0aW9uOjUwMjk5NTQ1MQ==', 'latestArtifact': {'id': 'QXJ0aWZhY3Q6MTM3NDk4NzI2MA==', 'versionIndex': 17}}}
+2024-12-16 12:39:54,183 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:54,183 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 9
+2024-12-16 12:39:54,183 INFO    SenderThread:281812 [dir_watcher.py:finish():358] shutting down directory watcher
+2024-12-16 12:39:54,410 INFO    SenderThread:281812 [dir_watcher.py:finish():388] scan: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files
+2024-12-16 12:39:54,411 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log output.log
+2024-12-16 12:39:54,411 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-metadata.json wandb-metadata.json
+2024-12-16 12:39:54,411 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json wandb-summary.json
+2024-12-16 12:39:54,411 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml config.yaml
+2024-12-16 12:39:54,413 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/requirements.txt requirements.txt
+2024-12-16 12:39:54,413 INFO    SenderThread:281812 [dir_watcher.py:finish():402] scan save: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/diff.patch diff.patch
+2024-12-16 12:39:54,413 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 10
+2024-12-16 12:39:54,413 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: poll_exit
+2024-12-16 12:39:54,413 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:54,414 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 10
+2024-12-16 12:39:54,420 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:54,420 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 10
+2024-12-16 12:39:54,421 INFO    SenderThread:281812 [file_pusher.py:finish():169] shutting down file pusher
+2024-12-16 12:39:54,616 INFO    wandb-upload_1:281812 [upload_job.py:push():130] Uploaded file /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/output.log
+2024-12-16 12:39:54,685 INFO    wandb-upload_2:281812 [upload_job.py:push():130] Uploaded file /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/config.yaml
+2024-12-16 12:39:54,692 INFO    wandb-upload_3:281812 [upload_job.py:push():130] Uploaded file /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/wandb-summary.json
+2024-12-16 12:39:54,696 INFO    wandb-upload_0:281812 [upload_job.py:push():130] Uploaded file /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/files/requirements.txt
+2024-12-16 12:39:54,896 INFO    Thread-39 :281812 [sender.py:transition_state():613] send defer: 11
+2024-12-16 12:39:54,896 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:54,896 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 11
+2024-12-16 12:39:54,897 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:54,897 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 11
+2024-12-16 12:39:54,897 INFO    SenderThread:281812 [file_pusher.py:join():175] waiting for file pusher
+2024-12-16 12:39:54,897 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 12
+2024-12-16 12:39:54,897 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:54,897 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 12
+2024-12-16 12:39:54,897 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:54,898 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 12
+2024-12-16 12:39:54,898 INFO    SenderThread:281812 [file_stream.py:finish():601] file stream finish called
+2024-12-16 12:39:55,325 INFO    SenderThread:281812 [file_stream.py:finish():605] file stream finish is done
+2024-12-16 12:39:55,325 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 13
+2024-12-16 12:39:55,325 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:55,325 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 13
+2024-12-16 12:39:55,325 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:55,325 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 13
+2024-12-16 12:39:55,325 INFO    SenderThread:281812 [sender.py:transition_state():613] send defer: 14
+2024-12-16 12:39:55,326 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: defer
+2024-12-16 12:39:55,326 INFO    HandlerThread:281812 [handler.py:handle_request_defer():184] handle defer: 14
+2024-12-16 12:39:55,326 DEBUG   SenderThread:281812 [sender.py:send():378] send: final
+2024-12-16 12:39:55,326 DEBUG   SenderThread:281812 [sender.py:send():378] send: footer
+2024-12-16 12:39:55,326 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: defer
+2024-12-16 12:39:55,326 INFO    SenderThread:281812 [sender.py:send_request_defer():609] handle sender defer: 14
+2024-12-16 12:39:55,327 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: poll_exit
+2024-12-16 12:39:55,327 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: poll_exit
+2024-12-16 12:39:55,331 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: internal_messages
+2024-12-16 12:39:55,331 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: server_info
+2024-12-16 12:39:55,331 DEBUG   SenderThread:281812 [sender.py:send_request():405] send_request: server_info
+2024-12-16 12:39:55,373 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: get_summary
+2024-12-16 12:39:55,373 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: sampled_history
+2024-12-16 12:39:55,418 DEBUG   HandlerThread:281812 [handler.py:handle_request():158] handle_request: shutdown
+2024-12-16 12:39:55,418 INFO    HandlerThread:281812 [handler.py:finish():882] shutting down handler
+2024-12-16 12:39:56,332 INFO    WriterThread:281812 [datastore.py:close():296] close: /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_122321-fwj5g5bb/run-fwj5g5bb.wandb
+2024-12-16 12:39:56,375 INFO    SenderThread:281812 [sender.py:finish():1545] shutting down sender
+2024-12-16 12:39:56,375 INFO    SenderThread:281812 [file_pusher.py:finish():169] shutting down file pusher
+2024-12-16 12:39:56,375 INFO    SenderThread:281812 [file_pusher.py:join():175] waiting for file pusher
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug.log b/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug.log
index a7943ba9..4320bda9 100644
--- a/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug.log
+++ b/models/wandb/run-20241216_122321-fwj5g5bb/logs/debug.log
@@ -39,3 +39,25 @@ config: {'wandb_entity': 'catemerfeld', 'wandb_project': 'ims_encoder_decoder',
 2024-12-16 12:23:29,420 INFO    MainThread:281232 [wandb_run.py:_redirect():2294] Wrapping output streams.
 2024-12-16 12:23:29,420 INFO    MainThread:281232 [wandb_run.py:_redirect():2319] Redirects installed.
 2024-12-16 12:23:29,420 INFO    MainThread:281232 [wandb_init.py:init():838] run started, returning control to user process
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Current SDK version is 0.17.0
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Configure stats pid to 281232
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Loading settings from /home/cmdunham/.config/wandb/settings
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Loading settings from /home/cmdunham/ChemicalDataGeneration/models/wandb/settings
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'notebook_name': '/home/cmdunham/ChemicalDataGeneration/models/carl_encoder.ipynb'}
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program': '<python with no main file>'}
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_setup.py:_flush():76] Applying login settings: {}
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_init.py:_log_setup():520] Logging user logs to /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_123952-v2bzzpoo/logs/debug.log
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_init.py:_log_setup():521] Logging internal logs to /home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241216_123952-v2bzzpoo/logs/debug-internal.log
+2024-12-16 12:39:52,335 INFO    MainThread:281232 [wandb_init.py:_jupyter_setup():466] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x7efda13a26d0>
+2024-12-16 12:39:52,336 INFO    MainThread:281232 [wandb_init.py:init():560] calling init triggers
+2024-12-16 12:39:52,336 INFO    MainThread:281232 [wandb_init.py:init():567] wandb.init called with sweep_config: {}
+config: {'wandb_entity': 'catemerfeld', 'wandb_project': 'ims_encoder_decoder', 'gpu': True, 'threads': 1, 'architecture': 'carl_encoder', 'optimizer': 'AdamW', 'loss': 'MSELoss', 'dataset': 'carls', 'target_embedding': 'ChemNet', 'batch_size': 32, 'epochs': 500, 'learning_rate': 1e-05}
+2024-12-16 12:39:52,336 INFO    MainThread:281232 [wandb_init.py:init():585] re-initializing run, found existing run on stack: fwj5g5bb
+2024-12-16 12:39:52,337 INFO    MainThread:281232 [wandb_run.py:_finish():2103] finishing run catemerfeld/ims_encoder_decoder/fwj5g5bb
+2024-12-16 12:39:52,337 INFO    MainThread:281232 [wandb_run.py:_atexit_cleanup():2343] got exitcode: 0
+2024-12-16 12:39:52,337 INFO    MainThread:281232 [wandb_run.py:_restore():2326] restore
+2024-12-16 12:39:52,337 INFO    MainThread:281232 [wandb_run.py:_restore():2332] restore done
+2024-12-16 12:39:56,419 INFO    MainThread:281232 [wandb_run.py:_footer_history_summary_info():3994] rendering history
+2024-12-16 12:39:56,420 INFO    MainThread:281232 [wandb_run.py:_footer_history_summary_info():4026] rendering summary
+2024-12-16 12:39:56,428 INFO    MainThread:281232 [wandb_run.py:_footer_sync_info():3953] logging synced files
diff --git a/models/wandb/run-20241216_122321-fwj5g5bb/run-fwj5g5bb.wandb b/models/wandb/run-20241216_122321-fwj5g5bb/run-fwj5g5bb.wandb
index 7a31ba16..beb9cc55 100644
Binary files a/models/wandb/run-20241216_122321-fwj5g5bb/run-fwj5g5bb.wandb and b/models/wandb/run-20241216_122321-fwj5g5bb/run-fwj5g5bb.wandb differ
diff --git a/requirements.txt b/requirements.txt
index cd057963..0c4b7382 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,6 +1,7 @@
 beautifulsoup4==4.12.3
 click==8.1.7
 # git+https://github.com/rossant/ipycache.git # for %%cache. 
+dask==2023.5.0 
 ipykernel==6.29.5
 ipywidgets==8.1.5
 numpy==1.24.4 # most recent version of numpy that works with python 3.8.10
