{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# from ray import tune\n",
    "from torch.utils.data import DataLoader, TensorDataset #, Dataset\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import GPUtil\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Setup:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data:\n",
    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/train_data.csv'\n",
    "train = pd.read_csv(file_path)\n",
    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/val_data.csv'\n",
    "val = pd.read_csv(file_path)\n",
    "file_path = '/mnt/usb/cmdunham/preprocessed_ims_data/test_data.csv'\n",
    "test = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "file_path = '../data/name_smiles_embedding_file.csv'\n",
    "name_smiles_embedding_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected GPU ID: 1\n",
      "  Name: NVIDIA GeForce RTX 4090\n",
      "  Memory Free: 20653.0 MB\n",
      "  Memory Used: 3563.0 MB\n",
      "  GPU Load: 0.00%\n",
      "Current device ID:  cuda:1\n",
      "PyTorch current device ID: 1\n",
      "PyTorch current device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the list of GPUs\n",
    "    gpus = GPUtil.getGPUs()\n",
    "\n",
    "    # Find the GPU with the most free memory\n",
    "    best_gpu = max(gpus, key=lambda gpu: gpu.memoryFree)\n",
    "\n",
    "    # Print details about the selected GPU\n",
    "    print(f\"Selected GPU ID: {best_gpu.id}\")\n",
    "    print(f\"  Name: {best_gpu.name}\")\n",
    "    print(f\"  Memory Free: {best_gpu.memoryFree} MB\")\n",
    "    print(f\"  Memory Used: {best_gpu.memoryUsed} MB\")\n",
    "    print(f\"  GPU Load: {best_gpu.load * 100:.2f}%\")\n",
    "\n",
    "    # Set the device for later use\n",
    "    device = torch.device(f'cuda:{best_gpu.id}')\n",
    "    print('Current device ID: ', device)\n",
    "\n",
    "    # Set the current device in PyTorch\n",
    "    torch.cuda.set_device(best_gpu.id)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Confirm the currently selected device in PyTorch\n",
    "print(\"PyTorch current device ID:\", torch.cuda.current_device())\n",
    "print(\"PyTorch current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB Setup:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['WANDB_API_KEY'] = '9729ad7b1f3a60f1072cdf7af979b737955733d4'\n",
    "config = {\n",
    "    'wandb_entity': 'catemerfeld',\n",
    "    'wandb_project': 'ims_encoder_decoder',\n",
    "    'gpu':True,\n",
    "    'threads':1,\n",
    "}\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '/home/cmdunham/ChemicalDataGeneration/models/ims_generator.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_wandb(config, **kwargs):\n",
    "    config.update(kwargs)\n",
    "\n",
    "    wandb.init(entity=config['wandb_entity'],\n",
    "               project=config['wandb_project'],\n",
    "               config=config)\n",
    "\n",
    "    # Set the number of threads\n",
    "    torch.set_num_threads(config['threads'])\n",
    "\n",
    "    # Find out is there is a GPU available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if not config['gpu']:\n",
    "        device = torch.device('cpu')\n",
    "    print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BKG</th>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEM</th>\n",
       "      <td>Diethyl Malonate</td>\n",
       "      <td>CCOC(=O)CC(=O)OCC</td>\n",
       "      <td>[0.3809721, 0.0005454041, 0.25539744, -0.24272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEB</th>\n",
       "      <td>1,2,3,4-Diepoxybutane</td>\n",
       "      <td>C1C(O1)C2CO2</td>\n",
       "      <td>[0.06318794, 0.009022224, 0.42160064, 0.195722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MES</th>\n",
       "      <td>2-(N-morpholino)ethanesulfonic acid</td>\n",
       "      <td>C1COCCN1CCS(=O)(=O)O</td>\n",
       "      <td>[-0.32520828, 0.009838344, -0.15108332, 0.2845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMMP</th>\n",
       "      <td>Dimethyl methylphosphonate</td>\n",
       "      <td>COP(=O)(C)OC</td>\n",
       "      <td>[0.12106811, 0.00294244, -0.14450458, 0.072665...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Name                SMILES  \\\n",
       "Unnamed: 0                                                              \n",
       "BKG                                  background                   NaN   \n",
       "DEM                            Diethyl Malonate     CCOC(=O)CC(=O)OCC   \n",
       "DEB                       1,2,3,4-Diepoxybutane          C1C(O1)C2CO2   \n",
       "MES         2-(N-morpholino)ethanesulfonic acid  C1COCCN1CCS(=O)(=O)O   \n",
       "DMMP                 Dimethyl methylphosphonate          COP(=O)(C)OC   \n",
       "\n",
       "                                                    embedding  \n",
       "Unnamed: 0                                                     \n",
       "BKG                                                       NaN  \n",
       "DEM         [0.3809721, 0.0005454041, 0.25539744, -0.24272...  \n",
       "DEB         [0.06318794, 0.009022224, 0.42160064, 0.195722...  \n",
       "MES         [-0.32520828, 0.009838344, -0.15108332, 0.2845...  \n",
       "DMMP        [0.12106811, 0.00294244, -0.14450458, 0.072665...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the df index to be the chemical abbreviations in col 'Unnamed: 0'\n",
    "name_smiles_embedding_df.set_index('Unnamed: 0', inplace=True)\n",
    "name_smiles_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_floats = []\n",
    "for chem_name in name_smiles_embedding_df.index:\n",
    "    if chem_name == 'BKG':\n",
    "        embedding_floats.append(None)\n",
    "    else:\n",
    "        embedding_float = name_smiles_embedding_df['embedding'][chem_name].split('[')[1]\n",
    "        embedding_float = embedding_float.split(']')[0]\n",
    "        embedding_float = [np.float32(num) for num in embedding_float.split(',')]\n",
    "        embedding_floats.append(embedding_float)\n",
    "\n",
    "name_smiles_embedding_df['Embedding Floats'] = embedding_floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectra:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first two cols ('Unnamed:0' and 'index') and last 9 cols ('Label' and OneHot encodings) to get just spectra\n",
    "train_spectra = train.iloc[:,2:-9]\n",
    "train_chem_encodings = train.iloc[:,-8:]\n",
    "\n",
    "val_spectra = val.iloc[:,2:-9]\n",
    "val_chem_encodings = val.iloc[:,-8:]\n",
    "\n",
    "# create tensors of spectra, true embeddings, and chemical name encodings for train and val\n",
    "train_chem_labels = list(train['Label'])\n",
    "train_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in train_chem_labels]).to(device)\n",
    "train_spectra_tensor = torch.Tensor(train_spectra.values).to(device)\n",
    "train_chem_encodings_tensor = torch.Tensor(train_chem_encodings.values).to(device)\n",
    "\n",
    "val_chem_labels = list(val['Label'])\n",
    "val_embeddings_tensor = torch.Tensor([name_smiles_embedding_df['Embedding Floats'][chem_name] for chem_name in val_chem_labels]).to(device)\n",
    "val_spectra_tensor = torch.Tensor(val_spectra.values).to(device)\n",
    "val_chem_encodings_tensor = torch.Tensor(val_chem_encodings.values).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Linear(512,652),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(652,780),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(780, 908),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(908, 1036),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(1036, 1164),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(1164, 1292),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(1292, 1420),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(1420, 1548),\n",
    "      nn.LeakyReLU(inplace=True),\n",
    "      nn.Linear(1548, 1676),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_dataset, device, model, criterion, optimizer, epoch, combo):\n",
    "  epoch_training_loss = 0\n",
    "\n",
    "  predicted_spectra = []\n",
    "  output_name_encodings = []\n",
    "  original_spectra = []\n",
    "\n",
    "  for true_spectra, name_encodings, true_embeddings in train_dataset:\n",
    "    # move inputs to device\n",
    "    true_spectra = true_spectra.to(device)\n",
    "    name_encodings = name_encodings.to(device)\n",
    "    true_embeddings = true_embeddings.to(device)\n",
    "\n",
    "    # backprapogation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    batch_predicted_spectra = model(true_embeddings)\n",
    "\n",
    "    loss = criterion(batch_predicted_spectra, true_spectra)\n",
    "    # accumulate epoch training loss\n",
    "    epoch_training_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # at last epoch store output embeddings and corresponding labels to output list\n",
    "    if (epoch + 1) == combo['epochs']:\n",
    "      for enc, spec, true_spec in zip(name_encodings, batch_predicted_spectra, true_spectra):\n",
    "        output_name_encodings.append(enc.cpu().detach().numpy())\n",
    "        predicted_spectra.append(spec.cpu().detach().numpy())\n",
    "        original_spectra.append(true_spec.cpu().detach().numpy())\n",
    "\n",
    "  # divide by number of batches to calculate average loss\n",
    "  average_loss = epoch_training_loss/len(train_dataset)\n",
    "  if (epoch + 1) == combo['epochs']:\n",
    "    return average_loss, predicted_spectra, output_name_encodings, original_spectra\n",
    "  else:\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Generator:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_cosine_similarity(output, target):\n",
    "  # it is common to use m/z ratio as weights since fragments with higher m/z ratios are more important.\n",
    "  mz = torch.arange(1, len(target)+1)\n",
    "\n",
    "  numerator = torch.sum(mz*output*target)\n",
    "  output_denom = torch.sqrt(torch.sum(mz*output**2))\n",
    "  target_denom = torch.sqrt(torch.sum(mz*target**2))\n",
    "  weighted_cosine_similarity = numerator/(output_denom*target_denom)\n",
    "  return(weighted_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(real_spectra, fake_spectra, chem_encodings, sorted_chem_names, plotting_chem='all', log_wandb=True, idx=[0,5]):\n",
    "  \"\"\"\n",
    "  Plot and compare real and synthetic spectra within a specified index range.\n",
    "\n",
    "  Takes in real and synthetic spectra and plots them for comparison.\n",
    "  Computes and prints the weighted cosine similarity between the real spectrum synthetic spectrum.\n",
    "\n",
    "  Args:\n",
    "      real_spectra (list of list of float): List of real spectra, where each spectrum is a list of intensity values.\n",
    "      fake_spectra (list of list of float): List of synthetic spectra generated by decoder.\n",
    "      labels (list of str): List of chemical name labels corresponding to each spectrum.\n",
    "      idx (list of int, optional): Range of indices to plot. Default is [0, 5].\n",
    "  \"\"\"\n",
    "  labels = [sorted_chem_names[list(encoding).index(1)] for encoding in chem_encodings]\n",
    "\n",
    "  start_num = idx[0]\n",
    "  stop_num = idx[1]\n",
    "  \n",
    "  for i, (real_spec, fake_spec) in enumerate(zip(real_spectra, fake_spectra)):\n",
    "    # only plot spectra within the specified index range\n",
    "    if i > stop_num:\n",
    "      break\n",
    "    if start_num <= i < stop_num:\n",
    "      # if a specific chemical has been specified only plot spectra for that chemical \n",
    "      if plotting_chem != 'all':\n",
    "        if labels[i] != plotting_chem:\n",
    "          stop_num +=1\n",
    "          continue\n",
    "          \n",
    "      # Define the x-axis range\n",
    "      numbers = range(0, len(real_spec)//2)\n",
    "\n",
    "      # Scale the real spectrum so highest peak is 100\n",
    "      true_spectra_copy = real_spec.copy()\n",
    "\n",
    "      # Scale the synthetic spectrum so highest peak is 100\n",
    "      synthetic_spectra_copy = fake_spec.copy()\n",
    "\n",
    "      weighted_cosine_similarity = get_weighted_cosine_similarity(synthetic_spectra_copy, true_spectra_copy)\n",
    "\n",
    "      # Create a plot with 2 subplots for real and synthetic spectra\n",
    "      _, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(20, 10))\n",
    "\n",
    "      ax[0].plot(numbers, true_spectra_copy[:len(numbers)], label='Positive')\n",
    "      ax[0].plot(numbers, true_spectra_copy[len(numbers):], label='Negative')\n",
    "      ax[0].set_title(f'True {labels[i]} Spectrum.', fontsize=18)\n",
    "      ax[0].set_xlabel('Mass to Charge Ratio', fontsize=16)\n",
    "      ax[0].set_ylabel('Intensity', fontsize=16)\n",
    "\n",
    "      ax[1].plot(numbers, synthetic_spectra_copy[:len(numbers)], label='Positive')\n",
    "      ax[1].plot(numbers, synthetic_spectra_copy[len(numbers):], label='Negative')\n",
    "      ax[1].set_title(f'Synthetic {labels[i]} Spectrum.', fontsize=18)\n",
    "      print('Weighted cosine similarity between true spectra and output spectra is: ', round(float(weighted_cosine_similarity), 2))\n",
    "      ax[1].set_xlabel('Drift Time', fontsize=16)\n",
    "      ax[1].set_ylabel('Ion Intensity', fontsize=16)\n",
    "\n",
    "      if log_wandb:\n",
    "        plt.savefig('tmp_plot.png', format='png', dpi=300)\n",
    "        wandb.log({'Comparison of Experimental and Synthetic Spectra': wandb.Image('tmp_plot.png')})\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cmdunham/ChemicalDataGeneration/models/wandb/run-20241031_155639-nsnlsw5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nsnlsw5w' target=\"_blank\">crawly-spirit-118</a></strong> to <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nsnlsw5w' target=\"_blank\">https://wandb.ai/catemerfeld/ims_encoder_decoder/runs/nsnlsw5w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--------------------------\n",
      "--------------------------\n",
      "New run with hyperparameters:\n",
      "batch_size  :  32\n",
      "epochs  :  30\n",
      "learning_rate  :  0.01\n",
      "--------------------------\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# set var deciding if results plot for this run is saved to wandb\n",
    "log_wandb = True\n",
    "# simulant to plot results for. It can be useful to see comparisons for the same simulant\n",
    "plotting_chem = 'TEPO'\n",
    "\n",
    "# Last 8 cols of the df are the chem names\n",
    "sorted_chem_names = list(train.columns[-8:])\n",
    "\n",
    "# model_config = {\n",
    "#   'batch_size':[128, 256],\n",
    "#   'epochs': [100],\n",
    "#   'learning_rate':[.01, .001]\n",
    "  # }\n",
    "\n",
    "# loss to compare for each model. Starting at something high so it will be replaced by first model's loss \n",
    "lowest_loss = 10000000\n",
    "\n",
    "model_config = {\n",
    "    'batch_size': [32],\n",
    "    'epochs': [30],\n",
    "    'learning_rate': [.01, .001]\n",
    "}\n",
    "\n",
    "keys = model_config.keys()\n",
    "values = model_config.values()\n",
    "\n",
    "# Generate all parameter combinations from model_config using itertools.product\n",
    "combinations = itertools.product(*values)\n",
    "\n",
    "# Iterate through each parameter combination and run model \n",
    "for combo in combinations:\n",
    "  combo = dict(zip(keys, combo))\n",
    "\n",
    "  train_dataset = DataLoader(TensorDataset(train_spectra_tensor, train_chem_encodings_tensor, train_embeddings_tensor), batch_size=combo['batch_size'], shuffle=True)\n",
    "  val_dataset = DataLoader(TensorDataset(val_spectra_tensor, val_chem_encodings_tensor, val_embeddings_tensor), batch_size=combo['batch_size'], shuffle=False)\n",
    "  generator = Generator().to(device)\n",
    "\n",
    "  generator_optimizer = torch.optim.AdamW(generator.parameters(), lr = combo['learning_rate'])\n",
    "  generator_criterion = nn.MSELoss()\n",
    "\n",
    "  wandb_kwargs = {\n",
    "    'learning_rate': combo['learning_rate'],\n",
    "    'epochs': combo['epochs'],\n",
    "    'batch_size': combo['batch_size'],\n",
    "    'model_architecture': 'generator',\n",
    "    'optimizer':'AdamW',\n",
    "    'loss': 'MSELoss'\n",
    "  }\n",
    "\n",
    "  run_with_wandb(config, **wandb_kwargs)\n",
    "\n",
    "  print('--------------------------')\n",
    "  print('--------------------------')\n",
    "  print('New run with hyperparameters:')\n",
    "  for key in combo:\n",
    "    print(key, ' : ', combo[key])\n",
    "  print('--------------------------')\n",
    "  print('--------------------------')\n",
    "\n",
    "  for epoch in range(combo['epochs']):\n",
    "    # Set model to training mode\n",
    "    generator.train(True)\n",
    "\n",
    "    # do a pass over the data\n",
    "    # at last epoch get predicted embeddings and chem names\n",
    "    if (epoch + 1) == combo['epochs']:\n",
    "      average_loss, predicted_spectra, output_name_encodings, original_spectra = train_one_epoch(\n",
    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
    "        )\n",
    "    else:\n",
    "      average_loss = train_one_epoch(\n",
    "        train_dataset, device, generator, generator_criterion, generator_optimizer, epoch, combo\n",
    "        )\n",
    "\n",
    "    epoch_val_loss = 0  \n",
    "    # evaluate model on validation data\n",
    "    generator.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "      for val_true_spectra, val_name_encodings, val_true_embeddings in val_dataset:\n",
    "        val_true_spectra = val_true_spectra.to(device)\n",
    "        val_name_encodings = val_name_encodings.to(device)\n",
    "        val_true_embeddings = val_true_embeddings.to(device)\n",
    "\n",
    "        val_batch_predicted_spectra = generator(val_true_embeddings)\n",
    "\n",
    "        val_loss = generator_criterion(val_batch_predicted_spectra, val_true_spectra)\n",
    "        # accumulate epoch validation loss\n",
    "        epoch_val_loss += val_loss.item()\n",
    "\n",
    "    # divide by number of batches to calculate average loss\n",
    "    val_average_loss = epoch_val_loss/len(val_dataset)\n",
    "\n",
    "    # log losses to wandb\n",
    "    wandb.log({\"Generator Training Loss\": average_loss, \"Generator Validation Loss\": val_average_loss})\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "      print('Epoch[{}/{}]:'.format(epoch+1, combo['epochs']))\n",
    "      print(f'   Training loss: {average_loss}')\n",
    "      print(f'   Validation loss: {val_average_loss}')\n",
    "      print('-------------------------------------------')\n",
    "\n",
    "  # saving comparison plots for first 5 spectra\n",
    "  plot_results(original_spectra, predicted_spectra, output_name_encodings, sorted_chem_names, plotting_chem, log_wandb, idx=[0,5])\n",
    "\n",
    "  if average_loss < lowest_loss:\n",
    "    lowest_loss = average_loss\n",
    "    best_hyperparams = combo\n",
    "\n",
    "  wandb.finish()\n",
    "\n",
    "print('Hyperparameters for best model: ')\n",
    "for key in best_hyperparams:\n",
    "  print('   ', key, ' : ', best_hyperparams[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem_data_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
