Using device: cuda
--------------------------
--------------------------
New run with hyperparameters:
batch_size  :  64
epochs  :  500
learning_rate  :  1e-07
Epoch[10/500]:
   Training loss: 0.0017098021966674734
   Validation loss: 0.0015102659073567464
-------------------------------------------
Epoch[20/500]:
   Training loss: 0.0004825270764807735
   Validation loss: 0.00046398344006618196
-------------------------------------------
Epoch[30/500]:
   Training loss: 0.00025377397167322284
   Validation loss: 0.0002518694551985516
-------------------------------------------
Epoch[40/500]:
   Training loss: 0.0001621033061385239
   Validation loss: 0.0001671414186501289
-------------------------------------------
Epoch[50/500]:
   Training loss: 0.00011455449861411
   Validation loss: 0.00012121268149054106
-------------------------------------------
Epoch[60/500]:
   Training loss: 8.593770515795788e-05
   Validation loss: 9.504272509667348e-05
-------------------------------------------
Epoch[70/500]:
   Training loss: 6.738190290128996e-05
   Validation loss: 7.503822810553299e-05
-------------------------------------------
Epoch[80/500]:
   Training loss: 5.460780224618088e-05
   Validation loss: 6.298896730366574e-05
-------------------------------------------
Epoch[90/500]:
   Training loss: 4.5298044021761224e-05
   Validation loss: 5.323200565303053e-05
-------------------------------------------
Epoch[100/500]:
   Training loss: 3.8381369295890984e-05
   Validation loss: 4.5965049419637265e-05
-------------------------------------------
Epoch[110/500]:
   Training loss: 3.3070651025085424e-05
   Validation loss: 4.1576549772892985e-05
-------------------------------------------
Epoch[120/500]:
   Training loss: 2.8833661378793952e-05
   Validation loss: 3.6516946571627444e-05
-------------------------------------------
Epoch[130/500]:
   Training loss: 2.5444698164341642e-05
   Validation loss: 3.297441847884141e-05
-------------------------------------------
Epoch[140/500]:
   Training loss: 2.2666865708509757e-05
   Validation loss: 3.1199496477099806e-05
-------------------------------------------
Epoch[150/500]:
   Training loss: 2.0373122945164828e-05
   Validation loss: 2.76691913629277e-05
-------------------------------------------
Epoch[160/500]:
   Training loss: 1.8417117192634777e-05
   Validation loss: 2.580795532970836e-05
-------------------------------------------
Epoch[170/500]:
   Training loss: 1.6772361998537487e-05
   Validation loss: 2.3997360606362517e-05
-------------------------------------------
Epoch[180/500]:
   Training loss: 1.540563617448244e-05
   Validation loss: 2.25818971626205e-05
-------------------------------------------
Epoch[190/500]:
   Training loss: 1.4157475512725184e-05
   Validation loss: 2.220354338176329e-05
-------------------------------------------
Epoch[200/500]:
   Training loss: 1.3120705736463955e-05
   Validation loss: 2.038608217237103e-05
-------------------------------------------
Epoch[210/500]:
   Training loss: 1.220196031095749e-05
   Validation loss: 1.9076302778891096e-05
-------------------------------------------
Epoch[220/500]:
   Training loss: 1.1352582546223127e-05
   Validation loss: 1.8487094175450944e-05
-------------------------------------------
Epoch[230/500]:
   Training loss: 1.0616111176248462e-05
   Validation loss: 1.775755611753272e-05
-------------------------------------------
Epoch[240/500]:
   Training loss: 9.99851894434343e-06
   Validation loss: 1.6863071641833373e-05
-------------------------------------------
Epoch[250/500]:
   Training loss: 9.39972861461357e-06
   Validation loss: 1.6422461904387e-05
-------------------------------------------
Epoch[260/500]:
   Training loss: 8.894728831013836e-06
   Validation loss: 1.5975666357803675e-05
-------------------------------------------
Epoch[270/500]:
   Training loss: 8.371642755287906e-06
   Validation loss: 1.5449180440504248e-05
-------------------------------------------
Epoch[280/500]:
   Training loss: 7.987082946310627e-06
   Validation loss: 1.5000631110289195e-05
-------------------------------------------
Epoch[290/500]:
   Training loss: 7.55229847597813e-06
   Validation loss: 1.443322919620751e-05
-------------------------------------------
Epoch[300/500]:
   Training loss: 7.236891657978355e-06
   Validation loss: 1.4014397868028686e-05
-------------------------------------------
Epoch[310/500]:
   Training loss: 6.8352576360943e-06
   Validation loss: 1.3751766587688508e-05
-------------------------------------------
Epoch[320/500]:
   Training loss: 6.559057600807976e-06
   Validation loss: 1.3463465690418177e-05
-------------------------------------------
Epoch[330/500]:
   Training loss: 6.277099569250696e-06
   Validation loss: 1.3178687165426755e-05
-------------------------------------------
Epoch[340/500]:
   Training loss: 5.990445576695501e-06
   Validation loss: 1.2934732503591195e-05
-------------------------------------------
Epoch[350/500]:
   Training loss: 5.765180041200319e-06
   Validation loss: 1.2716444400166829e-05
-------------------------------------------
Epoch[360/500]:
   Training loss: 5.5365647046537726e-06
   Validation loss: 1.2716344133521269e-05
-------------------------------------------
Epoch[370/500]:
   Training loss: 5.335192513651004e-06
   Validation loss: 1.2540010099189267e-05
-------------------------------------------
Epoch[380/500]:
   Training loss: 5.098288413002054e-06
   Validation loss: 1.2689567486843551e-05
-------------------------------------------
Epoch[390/500]:
   Training loss: 4.92238490003041e-06
   Validation loss: 1.1810554569339713e-05
-------------------------------------------
Epoch[400/500]:
   Training loss: 4.7939377787108804e-06
   Validation loss: 1.1640018454209925e-05
-------------------------------------------
Epoch[410/500]:
   Training loss: 4.641903825473876e-06
   Validation loss: 1.1418644291438795e-05
-------------------------------------------
Epoch[420/500]:
   Training loss: 4.433515837843612e-06
   Validation loss: 1.1244653553544909e-05
-------------------------------------------
Epoch[430/500]:
   Training loss: 4.29541853958416e-06
   Validation loss: 1.106776124946064e-05
-------------------------------------------
Epoch[440/500]:
   Training loss: 4.16642582152068e-06
   Validation loss: 1.0996736951722533e-05
-------------------------------------------
Epoch[450/500]:
   Training loss: 4.050392665716372e-06
   Validation loss: 1.0933029155902158e-05
-------------------------------------------
Epoch[460/500]:
   Training loss: 3.920040475434026e-06
   Validation loss: 1.0675738088615351e-05
-------------------------------------------
Epoch[470/500]:
   Training loss: 3.8115159016164467e-06
   Validation loss: 1.0562886777606685e-05
-------------------------------------------
Epoch[480/500]:
   Training loss: 3.702225507293873e-06
   Validation loss: 1.0515031505065053e-05
-------------------------------------------
Epoch[490/500]:
   Training loss: 3.6075928717899472e-06
   Validation loss: 1.033168509580711e-05
-------------------------------------------
Epoch[500/500]:
   Training loss: 3.505364350763958e-06
   Validation loss: 1.0277047585046163e-05
-------------------------------------------
-------------------------------------------
-------------------------------------------
Dataset:  carls
Target Embeddings:  ChemNet
-------------------------------------------
-------------------------------------------
Encoder(
  (encoder): Sequential(
    (0): Linear(in_features=1676, out_features=1548, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=1548, out_features=1420, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=1420, out_features=1292, bias=True)
    (5): LeakyReLU(negative_slope=0.01, inplace=True)
    (6): Linear(in_features=1292, out_features=1164, bias=True)
    (7): LeakyReLU(negative_slope=0.01, inplace=True)
    (8): Linear(in_features=1164, out_features=1036, bias=True)
    (9): LeakyReLU(negative_slope=0.01, inplace=True)
    (10): Linear(in_features=1036, out_features=908, bias=True)
    (11): LeakyReLU(negative_slope=0.01, inplace=True)
    (12): Linear(in_features=908, out_features=780, bias=True)
    (13): LeakyReLU(negative_slope=0.01, inplace=True)
    (14): Linear(in_features=780, out_features=652, bias=True)
    (15): LeakyReLU(negative_slope=0.01, inplace=True)
    (16): Linear(in_features=652, out_features=512, bias=True)
  )
)
-------------------------------------------
-------------------------------------------